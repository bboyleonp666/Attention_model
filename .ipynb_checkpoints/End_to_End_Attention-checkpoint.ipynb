{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav(PATH, LABELS=None, SAMPLE=None, default_rate=8000):\n",
    "    if LABELS is None:\n",
    "        LABELS = [f for f in os.listdir(os.getcwd()) if os.path.isdir(os.getcwd() + \"\\\\\" + f)]\n",
    "        \n",
    "    label_len = len(LABELS)\n",
    "\n",
    "    # initialize the output array\n",
    "    wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "    loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "    print(\"LABEL\\tTOTAL\\tREAD\\t<1s COUNT\")\n",
    "    print(\"-----\\t-----\\t----\\t---------\")\n",
    "    for i, label in enumerate(LABELS):\n",
    "        files = os.listdir(os.path.join(PATH, label))                                 # list all the files\n",
    "        waves = [f for f in files if f.endswith('.wav')]                              # get wave files\n",
    "        wave_len = len(waves)                                                         # get number of wave files\n",
    "        wave_count[i] = wave_len\n",
    "        \n",
    "        if SAMPLE is not None:\n",
    "            waves = [waves[sample] for sample in np.random.randint(wave_len, size=SAMPLE)]\n",
    "            wave_len = SAMPLE\n",
    "\n",
    "        # initialize the temp output array\n",
    "        tmp_wavData = np.zeros((wave_len, default_rate))\n",
    "        tmp_wavLabels = np.zeros(wave_len)\n",
    "\n",
    "        less_than_1s_count = 0\n",
    "        for j, wav in enumerate(waves):\n",
    "            path = os.path.join(PATH, label, wav)                                     # get path for each wave file\n",
    "            samples, sample_rate = librosa.load(path, sr=default_rate)                # read file by librosa\n",
    "\n",
    "            # padding the wave files\n",
    "            if len(samples) < sample_rate:\n",
    "                less_than_1s_count += 1                                               # count of files less than 1 second\n",
    "                white_noise = np.random.normal(0, 0.02, sample_rate-len(samples))     # generate white noise for padding\n",
    "                samples = np.concatenate((samples, white_noise), axis=None)           # padding the files that is less than 1s\n",
    "\n",
    "            # output as np.array\n",
    "            tmp_wavData[j, :] = samples                                               # temporary wave data\n",
    "            tmp_wavLabels[j] = i                                                      # temporary wave labels\n",
    "\n",
    "            # print the outcome every ten iterations\n",
    "            if j+1 == wave_len:\n",
    "                print(\"{}\\t{}\\t{}\\t{}\".format(label, wave_count[i], j+1, less_than_1s_count), end=\"\\n\")\n",
    "                loss_count[i] = less_than_1s_count\n",
    "            elif j % 10 == 9:\n",
    "                print(\"{}\\t{}\\t{}\\t{}\".format(label, wave_count[i], j+1, less_than_1s_count), end=\"\\r\")\n",
    "\n",
    "        if i == 0:\n",
    "            wavData = tmp_wavData\n",
    "            wavLabels = tmp_wavLabels\n",
    "        else:\n",
    "            wavData = np.concatenate((wavData, tmp_wavData), axis=0)                  # concatenate info of wave files\n",
    "            wavLabels = np.concatenate((wavLabels, tmp_wavLabels), axis=None)         # concatenate following labels\n",
    "\n",
    "    print()\n",
    "    print(\"MISSION COMPLETE!!!\")\n",
    "    \n",
    "    return wavData, wavLabels, wave_count.astype(np.int32), loss_count.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\t<1s COUNT\n",
      "-----\t-----\t----\t---------\n",
      "zero\t2376\t100\t8\n",
      "one\t2370\t100\t9\n",
      "two\t2373\t100\t4\n",
      "three\t2356\t100\t6\n",
      "four\t2372\t100\t6\n",
      "five\t2357\t100\t10\n",
      "six\t2369\t100\t3\n",
      "seven\t2377\t100\t8\n",
      "eight\t2352\t100\t10\n",
      "nine\t2364\t100\t8\n",
      "\n",
      "MISSION COMPLETE!!!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 8000\n",
    "\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "wav_array, label_array, total, loss = read_wav(PATH=train_audio_path, \n",
    "                                               LABELS=phoneme_dataframe.words, \n",
    "                                               SAMPLE=100,\n",
    "                                               default_rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if there is any NaN or Inf number exist so that we can avoid problems while training\n",
      "NaN Number: 0\n",
      "Inf Number: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Check if there is any NaN or Inf number exist so that we can avoid problems while training\")\n",
    "print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(waves, create_size, labels=None, phonemes=None, min_sz=6, max_sz=10, padding=True):\n",
    "    bind_size = np.random.randint(low=min_sz, high=max_sz+1, size=create_size)\n",
    "\n",
    "    wav_simu = np.zeros(create_size, dtype=np.object)\n",
    "    phone_simu = np.zeros(create_size, dtype=np.object)\n",
    "    label_simu = np.zeros(create_size, dtype=np.object)\n",
    "\n",
    "    for count, b_sz in enumerate(bind_size):\n",
    "        index = np.random.randint(len(waves), size=b_sz)\n",
    "        \n",
    "        wav_simu[count] = np.array([waves[i] for i in index]).flatten()\n",
    "        if padding:\n",
    "            # padding white noise\n",
    "            pad_sz = (max_sz - b_sz)*waves.shape[1]\n",
    "            white_noise = np.random.normal(0, 0.02, size=pad_sz)\n",
    "            wav_simu[count] = np.concatenate((wav_simu[count], white_noise), axis=None)\n",
    "        \n",
    "        if labels is not None:\n",
    "            label_simu[count] = [int(labels[i]) for i in index]\n",
    "            \n",
    "        if phonemes is not None:\n",
    "            phone_simu[count] = \"<start> \" + \" \".join([phonemes[i] for i in label_simu[count]]) + \" <end>\"\n",
    "        \n",
    "\n",
    "        if count % 10 == 9:\n",
    "            print(f\"Simulating {count+1}th wave \", end=\"\\r\")\n",
    "    print(\"\\n\\nSIMULATION COMPLETE!!!\")\n",
    "    \n",
    "    # the output simulated wave data(which is wav_simu) will be a object numpy array with shape=(create_size, )\n",
    "    # which is not a 2d array and cannot be put into tf.convert_to_tensor directly\n",
    "    # to do this in case one would like to apply spatial pyramid pooling instead of padding white noise\n",
    "    return wav_simu, label_simu, phone_simu\n",
    "\n",
    "\n",
    "def tokenize(phone):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(phone)\n",
    "    tensor = tokenizer.texts_to_sequences(phone)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "def convert(tokenizer, tensor):\n",
    "    print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "    print(\"-----------------------\")\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating 1000th wave \n",
      "\n",
      "SIMULATION COMPLETE!!!\n",
      "\n",
      "Example Phoneme Display: <start> F AO R Z IY R OW S IH K S F AY V Z IY R OW TH R IY F AO R <end>\n"
     ]
    }
   ],
   "source": [
    "CREATE_SIZE = 1000\n",
    "phonemes_array = phoneme_dataframe.phonemes.values\n",
    "simu_wave, simu_label, simu_phoneme = create_dataset(waves=wav_array, \n",
    "                                                     create_size=CREATE_SIZE, \n",
    "                                                     labels=label_array, \n",
    "                                                     phonemes=phonemes_array, \n",
    "                                                     min_sz=6, \n",
    "                                                     max_sz=8)\n",
    "simu_wave = np.vstack(simu_wave)\n",
    "\n",
    "print(f\"\\nExample Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter):\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        \n",
    "        return np.column_stack((energy, fbank))\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                   # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = MFCC(alpha=0.95, frame_size=0.025, frame_stride=0.01, n_fft=512, n_filter=12)\n",
    "sample_mfcc = mfcc.mfcc(samples=simu_wave[0], sample_rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000th iteration\n",
      "\n",
      "MISSION COMPELTE!!!\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.95\n",
    "FRAME_SIZE = 0.025\n",
    "FRAME_STRIDE = 0.01\n",
    "N_FFT = 512\n",
    "N_FILTER = 12\n",
    "\n",
    "mfcc = MFCC(alpha=ALPHA, frame_size=FRAME_SIZE, frame_stride=FRAME_STRIDE, n_fft=N_FFT, n_filter=N_FILTER)\n",
    "sample_mfcc = mfcc.mfcc(simu_wave[0], SAMPLE_RATE).flatten(order=\"C\")\n",
    "\n",
    "\n",
    "# adjust size so that it can fit in pBLSTM model\n",
    "divider = (N_FILTER + 1)*256\n",
    "new_size = int(((len(sample_mfcc)//divider + 1) * divider))\n",
    "simu_wave_mfcc = np.zeros((CREATE_SIZE, new_size))\n",
    "\n",
    "zero_padding = np.zeros(shape=(new_size - len(sample_mfcc)))\n",
    "for i in np.arange(CREATE_SIZE):\n",
    "    simu_wave_mfcc[i, :] = np.concatenate((mfcc.mfcc(simu_wave[i], \n",
    "                                                     SAMPLE_RATE).flatten(order=\"C\"), \n",
    "                                           zero_padding))\n",
    "    print(f\"{i+1}th iteration\", end=\"\\r\")\n",
    "\n",
    "print(\"\\n\\nMISSION COMPELTE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: (1000, 34)\n",
      "Input Shape: (1000, 13312)\n",
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "-----------------------\n",
      "10\t--->\t<start>\n",
      "5\t--->\tf\n",
      "19\t--->\tao\n",
      "3\t--->\tr\n",
      "15\t--->\tz\n",
      "6\t--->\tiy\n",
      "3\t--->\tr\n",
      "16\t--->\tow\n",
      "2\t--->\ts\n",
      "13\t--->\tih\n",
      "14\t--->\tk\n",
      "2\t--->\ts\n",
      "5\t--->\tf\n",
      "7\t--->\tay\n",
      "4\t--->\tv\n",
      "15\t--->\tz\n",
      "6\t--->\tiy\n",
      "3\t--->\tr\n",
      "16\t--->\tow\n",
      "17\t--->\tth\n",
      "3\t--->\tr\n",
      "6\t--->\tiy\n",
      "5\t--->\tf\n",
      "19\t--->\tao\n",
      "3\t--->\tr\n",
      "11\t--->\t<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = tokenize(simu_phoneme)\n",
    "wav_tensor = tf.convert_to_tensor(simu_wave_mfcc, dtype=tf.float32)\n",
    "\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "print(\"Input Shape: {}\".format(wav_tensor.shape))\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    convert(phoneme_tokenizer, tensor)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "LSTM_UNITS = 256\n",
    "FINAL_TIMESTEP = 64\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input Shape: (1, 13312)\n",
      "Original Output Shape: (1, 34)\n",
      "Reshaped Input Shape: (1, 13312, 1)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "print(f\"Original Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Original Output Shape: {example_target_batch.shape}\")\n",
    "\n",
    "example_input_batch = tf.expand_dims(example_input_batch, 2)\n",
    "print(f\"Reshaped Input Shape: {example_input_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, final_units, batch_sz, conv_filters, mfcc_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units\n",
    "        self.final_units = final_units\n",
    "        self.batch_sz = batch_sz\n",
    "        self.conv_filters = conv_filters\n",
    "        self.mfcc_dims = mfcc_dims\n",
    "        \n",
    "        # Convolution layer to extract feature after MFCC\n",
    "        self.conv_feat = tf.keras.layers.Conv1D(filters=self.conv_filters, \n",
    "                                                kernel_size=self.mfcc_dims, \n",
    "                                                padding='valid', \n",
    "                                                activation='relu', \n",
    "                                                strides=self.mfcc_dims)\n",
    "        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        build a pyramidal LSTM neural network encoder\n",
    "        '''\n",
    "        # Convolution Feature Extraction\n",
    "        x = self.conv_feat(x)\n",
    "        \n",
    "        # initialize states for forward and backward\n",
    "        initial_state_fw = None\n",
    "        initial_state_bw = None\n",
    "        \n",
    "        pyramid_layer_number = 0\n",
    "        while(x.shape[1] > self.final_units):\n",
    "            pyramid_layer_number += 1\n",
    "            # forward LSTM\n",
    "            fw_output, fw_state_h, fw_state_c = self.build_lstm(True)(x, initial_state=initial_state_fw)\n",
    "\n",
    "            # backward LSTM\n",
    "            bw_output, bw_state_h, bw_state_c = self.build_lstm(False)(x, initial_state=initial_state_bw)\n",
    "\n",
    "            x = tf.concat([fw_output, bw_output], -1)\n",
    "            x = self.reshape_pyramidal(x)\n",
    "\n",
    "            initial_state_fw = [fw_state_h, fw_state_c]\n",
    "            initial_state_bw = [bw_state_h, bw_state_c]\n",
    " \n",
    "        # print(f\"Encoder pyramid layer number: {pyramid_layer_number}\\n\")\n",
    "        return x, (fw_state_h, fw_state_c), (bw_state_h, bw_state_c)\n",
    "    \n",
    "    def build_lstm(self, back=True):\n",
    "        '''\n",
    "        build LSTM layer for forward and backward\n",
    "        '''\n",
    "        return tf.keras.layers.LSTM(units=self.lstm_units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    go_backwards=back)\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "    \n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (1, 64, 1024)\n",
      "Encoder forward state h shape: (batch size, units) (1, 256)\n",
      "Encoder forward state c shape: (batch size, units) (1, 256)\n",
      "Encoder backward state h shape: (batch size, units) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  final_units=FINAL_TIMESTEP, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  conv_filters=32, \n",
    "                  mfcc_dims=N_FILTER+1)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, (fw_sample_state_h, fw_sample_state_c), bw_sample_state = encoder(example_input_batch)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))\n",
    "print ('Encoder forward state c shape: (batch size, units) {}'.format(fw_sample_state_h.shape))\n",
    "print ('Encoder backward state h shape: (batch size, units) {}'.format(bw_sample_state[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (1, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (1, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, target_sz, embedding_dim, decoder_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        self.attention = BahdanauAttention(self.decoder_units)\n",
    "        self.lstm = tf.keras.layers.LSTM(units=self.decoder_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(target_sz)\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        build LSTM decoder\n",
    "        '''\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the LSTM\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[-1]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (1, 22)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  decoder_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, (enc_hidden_h, enc_hidden_c), bw_enc_hidden = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.0812\n"
     ]
    }
   ],
   "source": [
    "# the line below is a debugger which will make tensorflow run step by step\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # enc_hidden = encoder.initialize_hidden_state()\n",
    "    enc_hidden = None\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(STEP_PER_EPOCH)):\n",
    "        inp = tf.expand_dims(inp, 2)\n",
    "        batch_loss = train_step(inp, targ, phoneme_tokenizer, enc_hidden)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / STEP_PER_EPOCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(wave, max_input_len, max_output_len, tokenizer=None):\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    \n",
    "    inputs = tf.expand_dims(wave, 0)\n",
    "    inputs = tf.expand_dims(inputs, 2)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "\n",
    "    # hidden = [tf.zeros((1, units))]\n",
    "    hidden = None\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden=hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, wave, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, wave, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    # ax.set_xticklabels([''] + input_wav, fontdict=fontdict, rotation=90)\n",
    "#     ax.set_xticklabels(range(len(input_wav)))\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "\n",
    "    # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, _, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "\n",
    "    print(f'Original Input Length: {len(wave)}')\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing, testing_label, total, loss = read_wav(os.getcwd())\n",
    "test_wave, test_label, test_phoneme = create_dataset(wav_array, 1, label_array, phonemes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(test_wave, 8, sample_decoder_output.shape[1], phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
