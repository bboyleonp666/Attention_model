{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reference: \n",
    "    https://arxiv.org/abs/1508.01211\n",
    "    https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    https://github.com/jameslyons/python_speech_features\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "    \n",
    "data source: \n",
    "    https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveReader:\n",
    "    def __init__(self, path, sample_rate, padding_type, read_size):\n",
    "        '''\n",
    "        Args:\n",
    "            path: train path containing directory which one would like to load\n",
    "            sample_rate: sample rate for reading .wav file\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "            read_size: size that one would like to read\n",
    "        '''\n",
    "        \n",
    "        self.path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.padding_type = padding_type\n",
    "        self.read_size = read_size\n",
    "\n",
    "    def read(self, labels=None):\n",
    "        '''\n",
    "        read all the data under the labels(directories) one select\n",
    "        \n",
    "        Args:\n",
    "            labels: labels(directories) one would like to load\n",
    "                    None means read all the directories under that directory\n",
    "        '''\n",
    "        print(\"LABEL\\tTOTAL\\tREAD\\tSAVED\\t<1s COUNT\")\n",
    "        print(\"-----\\t-----\\t----\\t-----\\t---------\")\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f for f in os.listdir(path) if os.path.isdir(path + \"\\\\\" + f)]\n",
    "            \n",
    "        elif type(labels) == str:\n",
    "            samples, total_wave_count, total_wave_read, total_loss_count = self.read_dir(dir_name=labels)\n",
    "            sample_labels = np.repeat(labels, total_wave_read)\n",
    "            \n",
    "            print(\"\\n\\nMISSION COMPELTE!!!\")\n",
    "            return samples, sample_labels, total_wave_count, total_loss_count\n",
    "                    \n",
    "        label_len = len(labels)\n",
    "        total_wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "        total_wave_read = np.zeros(label_len, dtype=np.int32)\n",
    "        total_loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for i, lab in enumerate(labels):\n",
    "            samp, total_wave_count[i], total_wave_read[i], total_loss_count[i] = self.read_dir(dir_name=lab)\n",
    "            \n",
    "            if i == 0:\n",
    "                samples = samp\n",
    "                sample_labels = np.repeat(lab, total_wave_read[i])\n",
    "            else:\n",
    "                samples = np.concatenate((samples, samp), axis=0)\n",
    "                sample_labels = np.concatenate((sample_labels, np.repeat(lab, total_wave_read[i])), axis=None)\n",
    "        \n",
    "        print(\"\\n\\nMISSION COMPELTE!!!\")\n",
    "        return samples, sample_labels, total_wave_count, total_loss_count\n",
    "    \n",
    "    def read_dir(self, dir_name):\n",
    "        '''\n",
    "        read one directory of given directory name\n",
    "        \n",
    "        Args:\n",
    "            dir_name: directory name\n",
    "        '''\n",
    "        dir_path = os.path.join(self.path, dir_name)\n",
    "        wave_files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n",
    "        total_wave_files = len(wave_files)\n",
    "\n",
    "        if self.read_size is not None:\n",
    "            wave_files_read = self.read_size\n",
    "        else:\n",
    "            wave_files_read = total_wave_files\n",
    "\n",
    "        samples = np.zeros((wave_files_read, self.sample_rate))\n",
    "        less_than_1s_count = 0\n",
    "        num_of_file_read = 0\n",
    "        for i, wav_file in enumerate(wave_files):\n",
    "            wave_file_path = os.path.join(dir_path, wav_file)\n",
    "            samp, _ = librosa.load(wave_file_path, sr=self.sample_rate)\n",
    "\n",
    "            pad_size = self.sample_rate - len(samp)\n",
    "            if pad_size > 0:\n",
    "                less_than_1s_count += 1\n",
    "                if self.padding_type is None:\n",
    "                    # None: than skip this wave file\n",
    "                    continue\n",
    "\n",
    "                elif self.padding_type == \"white_noise\":\n",
    "                    # white_noise: pad white noise data behind\n",
    "                    padding = np.random.normal(0, 0.02, pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # zero: pad zeros behind\n",
    "                    padding = np.zeros(pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "            else:\n",
    "                samples[num_of_file_read, :] = samp\n",
    "                num_of_file_read += 1\n",
    "\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(dir_name, \n",
    "                                              total_wave_files, \n",
    "                                              i+1, \n",
    "                                              num_of_file_read, \n",
    "                                              less_than_1s_count), end=\"\\r\")\n",
    "            \n",
    "            if num_of_file_read == wave_files_read:\n",
    "                break\n",
    "                \n",
    "        print()\n",
    "\n",
    "        return samples, total_wave_files, wave_files_read, less_than_1s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "reader = WaveReader(path=train_audio_path, \n",
    "                    sample_rate=SAMPLE_RATE, \n",
    "                    padding_type=None, \n",
    "                    read_size=2000)\n",
    "\n",
    "wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words)\n",
    "# wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words[:1])\n",
    "\n",
    "# print(\"\\nCheck the existence of NaN and Inf\")\n",
    "# print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "# print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, create_size, min_sz=6, max_sz=8, padding_type=\"zero\"):\n",
    "        '''\n",
    "        Args:\n",
    "            create_size: size of binding wave one would like to create\n",
    "            min_sz: minimum size of wave data\n",
    "            max_sz: maximum size of wave data\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "        '''\n",
    "        self.create_size = create_size\n",
    "        self.min_sz = min_sz\n",
    "        self.max_sz = max_sz\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "    def simulate_wave(self, waves):\n",
    "        '''\n",
    "        method for simulating wave inputs\n",
    "        which will concatenate audio inputs for building longer audio dataset\n",
    "        \n",
    "        Args:\n",
    "            waves: input wave data\n",
    "        '''\n",
    "        # get picker for combining waves and labels(phonemes)\n",
    "        self.wave_shape = waves.shape\n",
    "        self.pickers = self.get_picker()\n",
    "        \n",
    "        print(\"Wave Data Simulation ... \", end=\"\")\n",
    "        \n",
    "        binded_length = self.wave_shape[1]*self.max_sz\n",
    "        simu_wave = np.zeros((self.create_size, binded_length))\n",
    "        \n",
    "        \n",
    "        for i, picker in enumerate(self.pickers):        \n",
    "            tmp_simu_wave = np.array([waves[p] for p in picker]).flatten()\n",
    "            \n",
    "            pad_size = binded_length - len(tmp_simu_wave)\n",
    "            if pad_size > 0:\n",
    "                if self.padding_type == \"white_noise\":\n",
    "                    # padding white noise\n",
    "                    padding = np.random.normal(0, 0.02, size=pad_size)\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # padding zeros\n",
    "                    padding = np.zeros(pad_size)\n",
    "\n",
    "                simu_wave[i] = np.concatenate((tmp_simu_wave, padding), axis=None)\n",
    "                \n",
    "            else:\n",
    "                simu_wave[i] = tmp_simu_wave\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_wave\n",
    "    \n",
    "    def get_picker(self):\n",
    "        '''\n",
    "        picker stands for index pick\n",
    "        this is for combining audio data with decided minimum and maximum size\n",
    "        '''\n",
    "        size = np.random.randint(low=self.min_sz, \n",
    "                                 high=self.max_sz+1, \n",
    "                                 size=self.create_size)\n",
    "\n",
    "        picker = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, s in enumerate(size):\n",
    "            picker[i] = np.random.choice(self.wave_shape[0]-1, size=self.max_sz, replace=False)[:s]\n",
    "            \n",
    "        return picker\n",
    "\n",
    "    def simulate_label(self, labels):\n",
    "        '''\n",
    "        method for simulating label inputs which will concatenate labels following simulated waves\n",
    "        \n",
    "        Args:\n",
    "            labels: input labels following with audio dataset\n",
    "        '''\n",
    "        print(\"Label Simulation ... \", end=\"\")\n",
    "        \n",
    "        simu_label = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, picker in enumerate(self.pickers):\n",
    "            simu_label[i] = np.array([labels[p] for p in picker])\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_label\n",
    "\n",
    "    def simulate_phoneme(self, labels, label_dict, phoneme_dict):\n",
    "        '''\n",
    "        method for sumulating phoneme inputs\n",
    "        which will concatenate audio phonemes with labels we concated by simulate_label()\n",
    "        \n",
    "        Args:\n",
    "            labels: labels that one would like to transfer\n",
    "            label_dict: label dictionary\n",
    "            phoneme_dict: phoneme dictionary\n",
    "        '''\n",
    "        print(\"Phoneme Simulation... \", end=\"\")\n",
    "        \n",
    "        self.label_dict = label_dict\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "\n",
    "        simu_phoneme = np.empty(self.create_size, dtype=np.object)\n",
    "        for i, label in enumerate(labels):\n",
    "            simu_phoneme[i] = \" \".join([self.phoneme_translator(lab) for lab in label])\n",
    "            simu_phoneme[i] = \"<start> \" + simu_phoneme[i] + \" <end>\"\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_phoneme\n",
    "\n",
    "    def phoneme_translator(self, input_label):\n",
    "        '''\n",
    "        translate labels to phoneme if simulate_phoneme is called\n",
    "        \n",
    "        Args:\n",
    "            input_label: label that one would like to transfer into phonemes\n",
    "        '''\n",
    "        for i, label in enumerate(self.label_dict):\n",
    "            if input_label == label:\n",
    "                return self.phoneme_dict[i]\n",
    "            \n",
    "    def tokenize(self, phoneme):\n",
    "        '''\n",
    "        with tensorflow we can simply apply Tokenizer for text(in our case, phoneme)\n",
    "        to generate phoneme outputs\n",
    "        \n",
    "        Args:\n",
    "            phoneme: phoneme string with '<start>' and '<end>'\n",
    "        '''\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(phoneme)\n",
    "        tensor = tokenizer.texts_to_sequences(phoneme)\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, tokenizer\n",
    "\n",
    "    def show_convert(self, tensor, tokenizer):\n",
    "        '''\n",
    "        showing case of tokenized word according to its index\n",
    "        \n",
    "        Args:\n",
    "            tensor: phoneme tensor\n",
    "            tokenizer: phoneme tokenizer\n",
    "        '''\n",
    "        print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "        print(\"=======================\")\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CREATE_SIZE = 20000\n",
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(create_size=CREATE_SIZE, \n",
    "                            min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "simu_wave = preprocesser.simulate_wave(wav_array)\n",
    "simu_label = preprocesser.simulate_label(label_array)\n",
    "simu_phoneme = preprocesser.simulate_phoneme(labels=simu_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {simu_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    '''\n",
    "    This is the Mel-Frequency Cepstral Coefficients, MFCCs Transformation\n",
    "    including\n",
    "        1. Pre-emphasis\n",
    "        2. Framing\n",
    "        3. Hamming window\n",
    "        4. Short-time Fourier Transform\n",
    "        5. Mel triangular bandpass filters\n",
    "        6. Log energy\n",
    "        \n",
    "    not including\n",
    "        7. Discrete cosine transform\n",
    "        8. Delta cepstrum\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter):\n",
    "        '''\n",
    "        Args:\n",
    "            alpha: coefficient applied when applying pre-emphasis, usually between (0.95, 0.98)\n",
    "            frame_size: duration of one frame in second\n",
    "            frame_stride: stride duration of one frame in second\n",
    "            n_fft: decided number while applying Fast Fourier Transformation\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate, apply_delta=False):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        mfcc_features = np.column_stack((energy, fbank))\n",
    "        \n",
    "        if apply_delta:\n",
    "            return np.concatenate(mfcc_features, self.delta(mfcc_features))\n",
    "        else:\n",
    "            return mfcc_features\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                              # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                           # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz\n",
    "    \n",
    "    def delta(self, mfcc_features, neighbor_len=2):\n",
    "        denominator = 2 * sum([i**2 for i in np.arange(1, neighbor_len+1)])\n",
    "        delta_feat = numpy.empty_like(mfcc_features)\n",
    "        padded = numpy.pad(mfcc_features, ((neighbor_len, neighbor_len), (0, 0)), mode='edge') # padded version of feat\n",
    "        for t in range(len(mfcc_features)):\n",
    "            delta_feat[t] = numpy.dot(numpy.arange(-neighbor_len, neighbor_len+1), \n",
    "                                      padded[t : t+2*neighbor_len+1]) / denominator # [t : t+2*N+1] == [(N+t)-N : (N+t)+N+1]\n",
    "        return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCApplier:\n",
    "    '''\n",
    "    This is the MFCC applier for applying MFCC \n",
    "    and pad zeros for fitting the data into Encoder-Decoder Model with Attention\n",
    "    which we will build later\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter):\n",
    "        '''\n",
    "        Arg:\n",
    "            mfcc: build by MFCC class for transform inputs\n",
    "            decide_size: a 2^k number which will make the inputs reshape into X*decide_size\n",
    "                         which will help us build the pyramidal RNN encoder\n",
    "        '''\n",
    "        self.mfcc = MFCC(alpha=alpha, \n",
    "                         frame_size=frame_size, \n",
    "                         frame_stride=frame_stride, \n",
    "                         n_fft=n_fft, \n",
    "                         n_filter=n_filter)\n",
    "        \n",
    "    def apply(self, inputs, sample_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: wave data that one would like to process\n",
    "        '''\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        sample = self.mfcc.mfcc(inputs[0, :], sample_rate)\n",
    "        sample_shape = sample.shape\n",
    "        outputs = np.zeros(((input_shape[0], sample_shape[0], sample_shape[1])))\n",
    "        for i in np.arange(input_shape[0]):\n",
    "            outputs[i, :, :] = self.mfcc.mfcc(inputs[i, :], sample_rate)\n",
    "            print(f\"Applying MFCC to {i+1}th case\", end=\"\\r\")\n",
    "            \n",
    "        print()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALPHA = 0.95\n",
    "# FRAME_SIZE = 0.025\n",
    "# FRAME_STRIDE = 0.01\n",
    "# N_FFT = 512\n",
    "# N_FILTER = 20\n",
    "\n",
    "# mfcc_applier = MFCCApplier(alpha=ALPHA, \n",
    "#                            frame_size=FRAME_SIZE, \n",
    "#                            frame_stride=FRAME_STRIDE, \n",
    "#                            n_fft=N_FFT, \n",
    "#                            n_filter=N_FILTER)\n",
    "\n",
    "# mfcced_simu_wave = mfcc_applier.apply(simu_wave, sample_rate=SAMPLE_RATE)\n",
    "# print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_simu_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "index = 1\n",
    "print(simu_label[index])\n",
    "print(simu_phoneme[index])\n",
    "ipd.Audio(simu_wave[index], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import logfbank\n",
    "\n",
    "ex_shape = logfbank(simu_wave[0], SAMPLE_RATE).shape\n",
    "mfcced_simu_wave = np.zeros(((len(simu_wave), ex_shape[0], ex_shape[1])))\n",
    "for i, wave in enumerate(simu_wave):\n",
    "    mfcc = logfbank(wave, SAMPLE_RATE)\n",
    "    \n",
    "#     mean = np.mean(mfcc, axis=0)\n",
    "#     std = np.std(mfcc, axis=0)\n",
    "#     mfcced_simu_wave[i, :, :] = (mfcc - mean) / std\n",
    "    mfcced_simu_wave[i, :, :] = mfcc\n",
    "    \n",
    "    print(f\"Transfering {i+1}th case\", end=\"\\r\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# shape = mfcced_simu_wave.shape\n",
    "# pad = np.array(mfcced_simu_wave[:, -1, :]).reshape(shape[0], 1, shape[2])\n",
    "# mfcced_simu_wave = np.concatenate((mfcced_simu_wave, pad), axis=1)\n",
    "phoneme_tensor, phoneme_tokenizer = preprocesser.tokenize(simu_phoneme)\n",
    "\n",
    "print(\"Input Shape: {}\".format(mfcced_simu_wave.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()\n",
    "    \n",
    "# Split the data into size (training set, testing set) (18000, 2000)\n",
    "TRAIN_SIZE = 0.9\n",
    "\n",
    "wav_tensor, wav_tensor_val, phoneme_tensor, phoneme_tensor_val = train_test_split(mfcced_simu_wave, \n",
    "                                                                                  phoneme_tensor, \n",
    "                                                                                  train_size=TRAIN_SIZE, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "VAL_WAV_SIZE = len(wav_tensor_val)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "wav_tensor = tf.convert_to_tensor(wav_tensor, dtype=tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(f\"Example Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Example Output Shape: {example_target_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters, stride=None):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        \n",
    "        if stride is None:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.filters1, self.stride, padding='valid', activation=\"relu\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.filters2, kernel_size, padding='same', activation=\"relu\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv1D(self.filters3, 1, padding='valid', activation=\"relu\")\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        if self.stride != 1:\n",
    "            short_cut = tf.keras.layers.Conv1D(self.filters3, \n",
    "                                               self.stride, \n",
    "                                               padding='valid', \n",
    "                                               activation=\"relu\")(input_tensor)\n",
    "            x += short_cut\n",
    "        else:\n",
    "            x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_block = ResnetIdentityBlock(32, [1, 2, example_input_batch.shape[-1]])\n",
    "resnet_output = resnet_block(example_input_batch)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(resnet_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_block.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder for MFCC transformed wave data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate, \n",
    "                 squeeze_time, \n",
    "                 rnn_initial_weight=None):\n",
    "        '''\n",
    "        Args:\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size            \n",
    "            dropout_rate: layer dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units    \n",
    "        self.squeeze_time = squeeze_time\n",
    "        self.rnn_initial_weight = rnn_initial_weight\n",
    "        \n",
    "        # normalization\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # conv1d\n",
    "        conv_units = 64\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=conv_units, \n",
    "                                           kernel_size=5, \n",
    "                                           strides=1, \n",
    "                                           padding='same', \n",
    "                                           activation=\"relu\", \n",
    "                                           kernel_initializer='he_normal')\n",
    "        \n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(64, [conv_units//4, conv_units//2, conv_units])\n",
    "        self.resnet2 = ResnetIdentityBlock(128, [conv_units//4, conv_units//2, conv_units])\n",
    "        self.resnet3 = ResnetIdentityBlock(256, [conv_units//4, conv_units//2, conv_units])\n",
    "        \n",
    "        # pBLSTM1\n",
    "        self.fw_lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"he_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='relu', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        self.bw_lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"he_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='relu', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate, \n",
    "                                             go_backwards=True)\n",
    "        \n",
    "        # pBLSTM2\n",
    "        self.fw_lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"he_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='relu', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        self.bw_lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"he_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='relu', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate, \n",
    "                                             go_backwards=True)\n",
    "        \n",
    "        # Encoder lstm\n",
    "        self.enc_lstm = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"he_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='relu', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call pyramidal LSTM neural network encoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: wave input\n",
    "        '''\n",
    "        self.layer_info[\"Input\"] = inputs.shape\n",
    "        inputs = self.bn(inputs)\n",
    "        \n",
    "        # ResNet\n",
    "        x = self.resnet1(x)\n",
    "        x = self.resnet2(x)\n",
    "        x = self.resnet3(x)\n",
    "        \n",
    "        # pBLSTM\n",
    "        (fw_outputs, fw_state_h, fw_state_c) = self.fw_lstm1(x)\n",
    "        (bw_outputs, bw_state_h, bw_state_c) = self.bw_lstm1(x)\n",
    "        x = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        (fw_outputs, fw_state_h, fw_state_c) = self.fw_lstm2(x)\n",
    "        (bw_outputs, bw_state_h, bw_state_c) = self.bw_lstm2(x)\n",
    "        x = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        # encoder output layer\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.enc_lstm(x)\n",
    "            \n",
    "        return fw_outputs, fw_state_h, fw_state_c\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        \n",
    "        Args:\n",
    "            outputs: outputs from LSTM\n",
    "            squeeze_time: time step one would like to squeeze in pyramidal LSTM\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "\n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * self.squeeze_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_DROPOUT_RATE = 0.0\n",
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, fw_sample_state_h, fw_sample_state_c = encoder(example_input_batch)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.V = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder for output phonemes\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 target_sz, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            target_sz: target size, total phoneme size in this case\n",
    "            embedding_dim: embedding dimension\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size\n",
    "            dropout_rate: dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.target_sz = target_sz\n",
    "        self.lstm_units = lstm_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        \n",
    "        # attention model\n",
    "        self.attention = LuongAttention(lstm_units)\n",
    "        \n",
    "        # decoder rnn            \n",
    "        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"he_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='relu', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "        \n",
    "        self.lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"he_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='relu', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "\n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(32, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.resnet2 = ResnetIdentityBlock(64, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet2_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.resnet3 = ResnetIdentityBlock(128, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet3_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "\n",
    "#         self.conv1 = tf.keras.layers.Conv1D(64, 13, padding=\"same\", activation=\"relu\")\n",
    "#         self.conv2 = tf.keras.layers.Conv1D(128, 11, padding=\"same\", activation=\"relu\")\n",
    "#         self.conv3 = tf.keras.layers.Conv1D(256, 9, padding=\"same\", activation=\"relu\")\n",
    "    \n",
    "        # Fully-connected\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(target_sz, activation=\"softmax\")\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        call LSTM decoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: target output, following phoneme for wave data input in this case\n",
    "            enc_hidden_h: encoder hidden state h\n",
    "            enc_hidden_c: encoder hidden state c\n",
    "            enc_output: encoder outputs\n",
    "        '''\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the 2-layer LSTM (Decoder)\n",
    "        outputs, state_h, state_c = self.lstm1(x, initial_state=None)\n",
    "        outputs, state_h, state_c = self.lstm2(outputs, initial_state=None)\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet1(outputs)\n",
    "        x = self.resnet1_dropout(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x = self.resnet2_dropout(x)\n",
    "        \n",
    "        x = self.resnet3(x)\n",
    "        x = self.resnet3_dropout(x)\n",
    "\n",
    "        # Convolution\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "    \n",
    "        # dense layer before final predict output dense layer\n",
    "        x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        # output shape == (batch_size, phoneme size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DECODER_DROPOUT_RATE = 0.0\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, phoneme size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate\n",
    "LEARING_RATE = 0.0001\n",
    "\n",
    "'''\n",
    "Candidate optimizer:\n",
    "    1. Adam\n",
    "    2. Nadam\n",
    "    \n",
    "    the preformence of other optimizers are not good\n",
    "    Adam is good but not good enough\n",
    "'''\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARING_RATE)\n",
    "# optimizer = tf.keras.optimizers.Nadam(learning_rate=LEARING_RATE)\n",
    "# optimizer = tf.keras.optimizers.Adadelta(learning_rate=LEARING_RATE)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "#     translate(test_wave, sample_output.shape[1], sample_decoder_output.shape[1], phoneme_tokenizer)\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run eagerly will make tensorflow run step by step or else it will raise\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "# similar to Pytorch, which is a dynamic graph for deep learning\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 20\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(\"{:>5}  {:>5}  {:>8}  {:>10}\".format(\"Epoch\", \"Batch\", \"Loss \", \" Time(s)\"))\n",
    "    \n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(STEP_PER_EPOCH)):\n",
    "        batch_loss = train_step(inp, targ, phoneme_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "            \n",
    "        if batch % 100 == 0:\n",
    "            print('{:<5}  {:>5}  {:>8}  {:>10}'.format(\"{}/{}\".format(epoch, EPOCHS), \n",
    "                                                       batch, \n",
    "                                                       \"{:.4f}\".format(batch_loss.numpy()), \n",
    "                                                       \"{:.4f}\".format(time.time() - epoch_start)))\n",
    "\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(\"\\n================================\")\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch, total_loss / STEP_PER_EPOCH))\n",
    "    print('Time taken for epoch {} -- {:.4f} min'.format(epoch, \n",
    "                                                         (time.time() - epoch_start)/60))\n",
    "    print('Total Time taken -- {:.4f} min'.format((time.time() - start)/60))\n",
    "    print(\"================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, max_input_len, max_output_len, tokenizer=None):\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    \n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "\n",
    "    # hidden = [tf.zeros((1, units))]\n",
    "    hidden = None\n",
    "    print(inputs.shape)\n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    \n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            inputs=dec_input, \n",
    "            enc_hidden_h=dec_hidden_h, \n",
    "            enc_hidden_c=dec_input, \n",
    "            enc_output=enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    # ax.set_xticklabels([''] + input_wav, fontdict=fontdict, rotation=90)\n",
    "#     ax.set_xticklabels(range(len(input_wav)))\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "\n",
    "    # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "\n",
    "    print(f'Original Input Length: {len(wave)}')\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with test wave data\n",
    "test_wave = tf.convert_to_tensor(wav_tensor_val[0], dtype=tf.float32)\n",
    "translate(test_wave, sample_output.shape[1], sample_decoder_output.shape[1], phoneme_tokenizer)\n",
    "\n",
    "test_phoneme = phoneme_tensor_val[0]\n",
    "preprocesser.show_convert(test_phoneme, phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
