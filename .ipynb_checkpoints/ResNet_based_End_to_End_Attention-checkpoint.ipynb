{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.0.0\n",
      "\n",
      "GPU available: True\n",
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reference: \n",
    "    https://arxiv.org/abs/1508.01211\n",
    "    https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    https://github.com/jameslyons/python_speech_features\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "    \n",
    "data source: \n",
    "    https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"CUDA enabled: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveReader:\n",
    "    def __init__(self, path, sample_rate, padding_type, read_size):\n",
    "        '''\n",
    "        Args:\n",
    "            path: train path containing directory which one would like to load\n",
    "            sample_rate: sample rate for reading .wav file\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "            read_size: size that one would like to read\n",
    "        '''\n",
    "        \n",
    "        self.path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.padding_type = padding_type\n",
    "        self.read_size = read_size\n",
    "\n",
    "    def read(self, labels=None):\n",
    "        '''\n",
    "        read all the data under the labels(directories) one select\n",
    "        \n",
    "        Args:\n",
    "            labels: labels(directories) one would like to load\n",
    "                    None means read all the directories under that directory\n",
    "        '''\n",
    "        print(\"LABEL\\tTOTAL\\tREAD\\tSAVED\\t<1s COUNT\")\n",
    "        print(\"-----\\t-----\\t----\\t-----\\t---------\")\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f for f in os.listdir(path) if os.path.isdir(path + \"\\\\\" + f)]\n",
    "            \n",
    "        elif type(labels) == str:\n",
    "            samples, total_wave_count, total_wave_read, total_loss_count = self.read_dir(dir_name=labels)\n",
    "            sample_labels = np.repeat(labels, total_wave_read)\n",
    "            \n",
    "            print(\"\\nMISSION COMPELTE!!!\")\n",
    "            return samples, sample_labels, total_wave_count, total_loss_count\n",
    "                    \n",
    "        label_len = len(labels)\n",
    "        total_wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "        total_wave_read = np.zeros(label_len, dtype=np.int32)\n",
    "        total_loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for i, lab in enumerate(labels):\n",
    "            samp, total_wave_count[i], total_wave_read[i], total_loss_count[i] = self.read_dir(dir_name=lab)\n",
    "            \n",
    "            if i == 0:\n",
    "                samples = samp\n",
    "                sample_labels = np.repeat(lab, total_wave_read[i])\n",
    "            else:\n",
    "                samples = np.concatenate((samples, samp), axis=0)\n",
    "                sample_labels = np.concatenate((sample_labels, np.repeat(lab, total_wave_read[i])), axis=None)\n",
    "        \n",
    "        print(\"\\nMISSION COMPELTE!!!\")\n",
    "        return samples, sample_labels, total_wave_count, total_loss_count\n",
    "    \n",
    "    def read_dir(self, dir_name):\n",
    "        '''\n",
    "        read one directory of given directory name\n",
    "        \n",
    "        Args:\n",
    "            dir_name: directory name\n",
    "        '''\n",
    "        dir_path = os.path.join(self.path, dir_name)\n",
    "        wave_files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n",
    "        total_wave_files = len(wave_files)\n",
    "\n",
    "        if self.read_size is not None:\n",
    "            wave_files_read = self.read_size\n",
    "        else:\n",
    "            wave_files_read = total_wave_files\n",
    "\n",
    "        samples = np.zeros((wave_files_read, self.sample_rate))\n",
    "        less_than_1s_count = 0\n",
    "        num_of_file_read = 0\n",
    "        for i, wav_file in enumerate(wave_files):\n",
    "            wave_file_path = os.path.join(dir_path, wav_file)\n",
    "            samp, _ = librosa.load(wave_file_path, sr=self.sample_rate)\n",
    "\n",
    "            pad_size = self.sample_rate - len(samp)\n",
    "            if pad_size > 0:\n",
    "                less_than_1s_count += 1\n",
    "                if self.padding_type is None:\n",
    "                    # None: than skip this wave file\n",
    "                    continue\n",
    "\n",
    "                elif self.padding_type == \"white_noise\":\n",
    "                    # white_noise: pad white noise data behind\n",
    "                    padding = np.random.normal(0, 0.02, pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # zero: pad zeros behind\n",
    "                    padding = np.zeros(pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "            else:\n",
    "                samples[num_of_file_read, :] = samp\n",
    "                num_of_file_read += 1\n",
    "\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(dir_name, \n",
    "                                              total_wave_files, \n",
    "                                              i+1, \n",
    "                                              num_of_file_read, \n",
    "                                              less_than_1s_count), end=\"\\r\")\n",
    "            \n",
    "            if num_of_file_read == wave_files_read:\n",
    "                break\n",
    "                \n",
    "        print()\n",
    "\n",
    "        return samples, total_wave_files, wave_files_read, less_than_1s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\tSAVED\t<1s COUNT\n",
      "-----\t-----\t----\t-----\t---------\n",
      "zero\t2376\t2160\t2000\t160\n",
      "one\t2370\t2262\t2000\t262\n",
      "two\t2373\t2222\t2000\t222\n",
      "three\t2356\t2207\t2000\t207\n",
      "four\t2372\t2201\t2000\t201\n",
      "five\t2357\t2185\t2000\t185\n",
      "six\t2369\t2164\t2000\t164\n",
      "seven\t2377\t2194\t2000\t194\n",
      "eight\t2352\t2232\t2000\t232\n",
      "nine\t2364\t2178\t2000\t178\n",
      "\n",
      "MISSION COMPELTE!!!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "reader = WaveReader(path=train_audio_path, \n",
    "                    sample_rate=SAMPLE_RATE, \n",
    "                    padding_type=None, \n",
    "                    read_size=2000)\n",
    "\n",
    "wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words)\n",
    "# wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words[0])\n",
    "\n",
    "# print(\"\\nCheck the existence of NaN and Inf\")\n",
    "# print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "# print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, create_size, min_sz=6, max_sz=8, padding_type=\"zero\"):\n",
    "        '''\n",
    "        Args:\n",
    "            create_size: size of binding wave one would like to create\n",
    "            min_sz: minimum size of wave data\n",
    "            max_sz: maximum size of wave data\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "        '''\n",
    "        self.create_size = create_size\n",
    "        self.min_sz = min_sz\n",
    "        self.max_sz = max_sz\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "    def simulate_wave(self, waves):\n",
    "        '''\n",
    "        method for simulating wave inputs\n",
    "        which will concatenate audio inputs for building longer audio dataset\n",
    "        \n",
    "        Args:\n",
    "            waves: input wave data\n",
    "        '''\n",
    "        # get picker for combining waves and labels(phonemes)\n",
    "        self.wave_shape = waves.shape\n",
    "        self.pickers = self.get_picker()\n",
    "        \n",
    "        print(\"Wave Data Simulation ... \", end=\"\")\n",
    "        \n",
    "        binded_length = self.wave_shape[1]*self.max_sz\n",
    "        simu_wave = np.zeros((self.create_size, binded_length))\n",
    "        \n",
    "        \n",
    "        for i, picker in enumerate(self.pickers):        \n",
    "            tmp_simu_wave = np.array([waves[p] for p in picker]).flatten()\n",
    "            \n",
    "            pad_size = binded_length - len(tmp_simu_wave)\n",
    "            if pad_size > 0:\n",
    "                if self.padding_type == \"white_noise\":\n",
    "                    # padding white noise\n",
    "                    padding = np.random.normal(0, 0.02, size=pad_size)\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # padding zeros\n",
    "                    padding = np.zeros(pad_size)\n",
    "\n",
    "                simu_wave[i] = np.concatenate((tmp_simu_wave, padding), axis=None)\n",
    "                \n",
    "            else:\n",
    "                simu_wave[i] = tmp_simu_wave\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_wave\n",
    "    \n",
    "    def get_picker(self):\n",
    "        '''\n",
    "        picker stands for index pick\n",
    "        this is for combining audio data with decided minimum and maximum size\n",
    "        '''\n",
    "        size = np.random.randint(low=self.min_sz, \n",
    "                                 high=self.max_sz+1, \n",
    "                                 size=self.create_size)\n",
    "\n",
    "        picker = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, s in enumerate(size):\n",
    "            picker[i] = np.random.choice(self.wave_shape[0]-1, size=self.max_sz, replace=False)[:s]\n",
    "            \n",
    "        return picker\n",
    "\n",
    "    def simulate_label(self, labels):\n",
    "        '''\n",
    "        method for simulating label inputs which will concatenate labels following simulated waves\n",
    "        \n",
    "        Args:\n",
    "            labels: input labels following with audio dataset\n",
    "        '''\n",
    "        print(\"Label Simulation ... \", end=\"\")\n",
    "        \n",
    "        simu_label = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, picker in enumerate(self.pickers):\n",
    "            simu_label[i] = np.array([labels[p] for p in picker])\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_label\n",
    "\n",
    "    def simulate_phoneme(self, labels, label_dict, phoneme_dict):\n",
    "        '''\n",
    "        method for sumulating phoneme inputs\n",
    "        which will concatenate audio phonemes with labels we concated by simulate_label()\n",
    "        \n",
    "        Args:\n",
    "            labels: labels that one would like to transfer\n",
    "            label_dict: label dictionary\n",
    "            phoneme_dict: phoneme dictionary\n",
    "        '''\n",
    "        print(\"Phoneme Simulation... \", end=\"\")\n",
    "        \n",
    "        self.label_dict = label_dict\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "\n",
    "        simu_phoneme = np.empty(self.create_size, dtype=np.object)\n",
    "        for i, label in enumerate(labels):\n",
    "            simu_phoneme[i] = \" \".join([self.phoneme_translator(lab) for lab in label])\n",
    "            simu_phoneme[i] = \"<start> \" + simu_phoneme[i] + \" <end>\"\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_phoneme\n",
    "\n",
    "    def phoneme_translator(self, input_label):\n",
    "        '''\n",
    "        translate labels to phoneme if simulate_phoneme is called\n",
    "        \n",
    "        Args:\n",
    "            input_label: label that one would like to transfer into phonemes\n",
    "        '''\n",
    "        for i, label in enumerate(self.label_dict):\n",
    "            if input_label == label:\n",
    "                return self.phoneme_dict[i]\n",
    "            \n",
    "    def tokenize(self, phoneme):\n",
    "        '''\n",
    "        with tensorflow we can simply apply Tokenizer for text(in our case, phoneme)\n",
    "        to generate phoneme outputs\n",
    "        \n",
    "        Args:\n",
    "            phoneme: phoneme string with '<start>' and '<end>'\n",
    "        '''\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(phoneme)\n",
    "        tensor = tokenizer.texts_to_sequences(phoneme)\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, tokenizer\n",
    "\n",
    "    def show_convert(self, tensor, tokenizer):\n",
    "        '''\n",
    "        showing case of tokenized word according to its index\n",
    "        \n",
    "        Args:\n",
    "            tensor: phoneme tensor\n",
    "            tokenizer: phoneme tokenizer\n",
    "        '''\n",
    "        print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "        print(\"=======================\")\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave Data Simulation ... Done\n",
      "Label Simulation ... Done\n",
      "Phoneme Simulation... Done\n",
      "\n",
      "Example Label Display: ['five']\n",
      "Example Phoneme Display: <start> F AY V <end>\n"
     ]
    }
   ],
   "source": [
    "CREATE_SIZE = 20000\n",
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(create_size=CREATE_SIZE, \n",
    "                            min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "simu_wave = preprocesser.simulate_wave(wav_array)\n",
    "simu_label = preprocesser.simulate_label(label_array)\n",
    "simu_phoneme = preprocesser.simulate_phoneme(labels=simu_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {simu_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [17787]\n",
      "[array(['two'], dtype='<U3')]\n",
      "['<start> T UW <end>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "index = np.random.randint(len(simu_wave), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "print(simu_label[index])\n",
    "print(simu_phoneme[index])\n",
    "ipd.Audio(simu_wave[index], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    '''\n",
    "    This is the Mel-Frequency Cepstral Coefficients, MFCCs Transformation\n",
    "    including\n",
    "        1. Pre-emphasis\n",
    "        2. Framing\n",
    "        3. Hamming window\n",
    "        4. Short-time Fourier Transform\n",
    "        5. Mel triangular bandpass filters\n",
    "        6. Log energy\n",
    "        \n",
    "    not including\n",
    "        7. Discrete cosine transform\n",
    "        8. Delta cepstrum\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Args:\n",
    "            alpha: coefficient applied when applying pre-emphasis, usually between (0.95, 0.98)\n",
    "            frame_size: duration of one frame in second\n",
    "            frame_stride: stride duration of one frame in second\n",
    "            n_fft: decided number while applying Fast Fourier Transformation\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        self.apply_delta = apply_delta\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        mfcc_features = np.column_stack((energy, fbank))\n",
    "        \n",
    "        if self.apply_delta:\n",
    "            mfcc_feat = mfcc_features.T\n",
    "            \n",
    "            d_mfcc_feat = self.delta(mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_feat.T, d_mfcc_feat.T), axis=1)\n",
    "\n",
    "            dd_mfcc_feat = self.delta(d_mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_features, dd_mfcc_feat.T), axis=1)\n",
    "\n",
    "        return mfcc_features\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                              # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                           # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz\n",
    "    \n",
    "    def delta(self, mfcc_features, neighbor_len=2):\n",
    "        denominator = 2 * sum([i**2 for i in np.arange(1, neighbor_len+1)])\n",
    "        delta_feat = np.empty_like(mfcc_features)\n",
    "        padded = np.pad(mfcc_features, ((neighbor_len, neighbor_len), (0, 0)), mode='edge') # padded version of feat\n",
    "        for t in range(len(mfcc_features)):\n",
    "            delta_feat[t] = np.dot(np.arange(-neighbor_len, neighbor_len+1), \n",
    "                                      padded[t : t+2*neighbor_len+1]) / denominator\n",
    "        return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCApplier:\n",
    "    '''\n",
    "    This is the MFCC applier for applying MFCC \n",
    "    and pad zeros for fitting the data into Encoder-Decoder Model with Attention\n",
    "    which we will build later\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Arg:\n",
    "            mfcc: build by MFCC class for transform inputs\n",
    "            decide_size: a 2^k number which will make the inputs reshape into X*decide_size\n",
    "                         which will help us build the pyramidal RNN encoder\n",
    "        '''\n",
    "        self.mfcc = MFCC(alpha=alpha, \n",
    "                         frame_size=frame_size, \n",
    "                         frame_stride=frame_stride, \n",
    "                         n_fft=n_fft, \n",
    "                         n_filter=n_filter, \n",
    "                         apply_delta=apply_delta)\n",
    "        \n",
    "        \n",
    "    def apply(self, inputs, sample_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: wave data that one would like to process\n",
    "        '''\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        sample = self.mfcc.mfcc(inputs[0, :], sample_rate)\n",
    "        sample_shape = sample.shape\n",
    "        \n",
    "        outputs = np.zeros(((input_shape[0], sample_shape[0], sample_shape[1])))\n",
    "        \n",
    "        for i in np.arange(input_shape[0]):\n",
    "            outputs[i, :, :] = self.mfcc.mfcc(inputs[i, :], sample_rate)\n",
    "            print(f\"Applying MFCC to {i+1}th case\", end=\"\\r\")\n",
    "            \n",
    "        print()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying MFCC to 20000th case\n",
      "Simulated MFCC wave: (input size, time steps, MFCC dimension) (20000, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.95\n",
    "FRAME_SIZE = 0.025\n",
    "FRAME_STRIDE = 0.01\n",
    "N_FFT = 512\n",
    "N_FILTER = 12\n",
    "\n",
    "mfcc_applier = MFCCApplier(alpha=ALPHA, \n",
    "                           frame_size=FRAME_SIZE, \n",
    "                           frame_stride=FRAME_STRIDE, \n",
    "                           n_fft=N_FFT, \n",
    "                           n_filter=N_FILTER, \n",
    "                           apply_delta=True)\n",
    "\n",
    "mfcced_simu_wave = mfcc_applier.apply(simu_wave, sample_rate=SAMPLE_RATE)\n",
    "print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_simu_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (20000, 99, 39)\n",
      "Output Shape: (20000, 7)\n",
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "6\t--->\tf\n",
      "8\t--->\tay\n",
      "7\t--->\tv\n",
      "2\t--->\t<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = preprocesser.tokenize(simu_phoneme)\n",
    "\n",
    "print(\"Input Shape: {}\".format(mfcced_simu_wave.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()\n",
    "    \n",
    "# Split the data into size (training set, testing set) (18000, 2000)\n",
    "TRAIN_SIZE = 0.95\n",
    "\n",
    "wav_tensor, wav_tensor_val, phoneme_tensor, phoneme_tensor_val = train_test_split(mfcced_simu_wave, \n",
    "                                                                                  phoneme_tensor, \n",
    "                                                                                  train_size=TRAIN_SIZE, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "VAL_WAV_SIZE = len(wav_tensor_val)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "wav_tensor = tf.convert_to_tensor(wav_tensor, dtype=tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "wav_tensor_val = tf.convert_to_tensor(wav_tensor_val, dtype=tf.float32)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((wav_tensor_val, phoneme_tensor_val)).shuffle(VAL_WAV_SIZE)\n",
    "\n",
    "# avoid running out of memory\n",
    "simu_wave = None\n",
    "mfcced_simu_wave = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input Shape: (4, 99, 39)\n",
      "Example Output Shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(f\"Example Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Example Output Shape: {example_target_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters, stride=None):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        \n",
    "        if stride is None:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.filters1, self.stride, padding='valid', activation=\"relu\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.filters2, kernel_size, padding='same', activation=\"relu\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv1D(self.filters3, 1, padding='valid', activation=\"relu\")\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        if self.stride != 1:\n",
    "            short_cut = tf.keras.layers.Conv1D(self.filters3, \n",
    "                                               self.stride, \n",
    "                                               padding='valid', \n",
    "                                               activation=\"relu\")(input_tensor)\n",
    "            x += short_cut\n",
    "        else:\n",
    "            x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "resnet_block = ResnetIdentityBlock(32, [1, 2, example_input_batch.shape[-1]])\n",
    "resnet_output = resnet_block(example_input_batch)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(resnet_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_identity_block\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              multiple                  40        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  4         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  66        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  8         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  117       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  156       \n",
      "=================================================================\n",
      "Total params: 391\n",
      "Trainable params: 307\n",
      "Non-trainable params: 84\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_block.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder for MFCC transformed wave data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate, \n",
    "                 units,\n",
    "                 squeeze_time, \n",
    "                 rnn_initial_weight=None):\n",
    "        '''\n",
    "        Args:\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size            \n",
    "            dropout_rate: layer dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units    \n",
    "        self.squeeze_time = squeeze_time\n",
    "        self.rnn_initial_weight = rnn_initial_weight\n",
    "        \n",
    "        # conv1d\n",
    "        self.feat_extract = tf.keras.layers.Dense(units=units, activation=\"relu\")\n",
    "        self.feat_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(32, filters=[units//4, units//2, units])\n",
    "        \n",
    "        units *= squeeze_time\n",
    "        self.resnet2 = ResnetIdentityBlock(64, [units//4, units//2, units])\n",
    "        \n",
    "        units *= squeeze_time\n",
    "        self.resnet3 = ResnetIdentityBlock(128, [units//4, units//2, units])\n",
    "        \n",
    "        # Encoder lstm\n",
    "        self.enc_lstm = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call pyramidal LSTM neural network encoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: wave input\n",
    "        '''\n",
    "        x = self.feat_extract(inputs)\n",
    "        x = self.feat_dropout(x)\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet1(x)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        x = self.resnet3(x)\n",
    "        \n",
    "        # encoder output layer\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.enc_lstm(x)\n",
    "            \n",
    "        return fw_outputs, fw_state_h, fw_state_c\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        \n",
    "        Args:\n",
    "            outputs: outputs from LSTM\n",
    "            squeeze_time: time step one would like to squeeze in pyramidal LSTM\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "\n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * self.squeeze_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 11, 256)\n",
      "Encoder forward state h shape: (batch size, units) (4, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_DROPOUT_RATE = 0.2\n",
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3, \n",
    "                  units=64)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, fw_sample_state_h, fw_sample_state_c = encoder(example_input_batch)\n",
    "# sample_output, sample_state = encoder(example_input_batch)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  2560      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_1 (Res multiple                  20016     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_2 (Res multiple                  324240    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_3 (Res multiple                  5562288   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  852992    \n",
      "=================================================================\n",
      "Total params: 6,762,096\n",
      "Trainable params: 6,759,184\n",
      "Non-trainable params: 2,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.V = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder for output phonemes\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 target_sz, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            target_sz: target size, total phoneme size in this case\n",
    "            embedding_dim: embedding dimension\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size\n",
    "            dropout_rate: dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.target_sz = target_sz\n",
    "        self.lstm_units = lstm_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        \n",
    "        # attention model\n",
    "        self.attention = LuongAttention(lstm_units)\n",
    "        \n",
    "        # decoder rnn            \n",
    "        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "\n",
    "        # ResNet\n",
    "#         self.resnet1 = ResnetIdentityBlock(32, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "#         self.resnet1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.resnet2 = ResnetIdentityBlock(64, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "#         self.resnet2_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.resnet3 = ResnetIdentityBlock(128, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "#         self.resnet3_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    \n",
    "        # Fully-connected\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(target_sz, activation=\"softmax\")\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        call LSTM decoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: target output, following phoneme for wave data input in this case\n",
    "            enc_hidden_h: encoder hidden state h\n",
    "            enc_hidden_c: encoder hidden state c\n",
    "            enc_output: encoder outputs\n",
    "        '''\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the 2-layer LSTM (Decoder)\n",
    "        x, state_h, state_c = self.lstm1(x)\n",
    "\n",
    "        # ResNet\n",
    "#         x = self.resnet1(outputs)\n",
    "#         x = self.resnet1_dropout(x)\n",
    "        \n",
    "#         x = self.resnet2(x)\n",
    "#         x = self.resnet2_dropout(x)\n",
    "        \n",
    "#         x = self.resnet3(x)\n",
    "#         x = self.resnet3_dropout(x)\n",
    "\n",
    "        # dense layer before final predict output dense layer\n",
    "        x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        # output shape == (batch_size, phoneme size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, phoneme size) (4, 22)\n"
     ]
    }
   ],
   "source": [
    "DECODER_DROPOUT_RATE = 0.2\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, phoneme size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2816      \n",
      "_________________________________________________________________\n",
      "luong_attention_1 (LuongAtte multiple                  131841    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  656384    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  1430      \n",
      "=================================================================\n",
      "Total params: 808,919\n",
      "Trainable params: 808,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "'''\n",
    "Candidate optimizer:\n",
    "    1. Adam\n",
    "    2. Nadam\n",
    "    \n",
    "    the preformence of other optimizers are not good\n",
    "'''\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.math.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    if tf.is_tensor(real):\n",
    "        real = real.numpy()\n",
    "        \n",
    "    if tf.is_tensor(pred):\n",
    "        pred = pred.numpy()\n",
    "        \n",
    "    if np.isscalar(real):\n",
    "        if real == pred:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    length = len(real)\n",
    "    count_ = 0\n",
    "    for i in range(length):\n",
    "        if real[i] == pred[i]:\n",
    "            count_ += 1\n",
    "\n",
    "    return tf.math.reduce_sum(count_) / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3, units=64)\n",
    "\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/ResNet_LSTM'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            \n",
    "            target_id = targ[:, t]\n",
    "            loss += loss_function(target_id, predictions)\n",
    "            \n",
    "            predicted_id = tf.math.argmax(predictions, axis=1)\n",
    "            acc += accuracy_function(target_id, predicted_id)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "#     clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)  # clipping for avoiding gradient explosion\n",
    "#     optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    batch_accuracy = (acc / int(targ.shape[1]))\n",
    "\n",
    "    return batch_loss, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    inp = tf.expand_dims(inp, 0)\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']], 1)\n",
    "\n",
    "    for t in range(1, targ.shape[0]):\n",
    "        predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "        \n",
    "        target_id = targ[t]\n",
    "        loss += loss_function(target_id, predictions)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        acc += accuracy_function(target_id, predicted_id)\n",
    "        \n",
    "        if targ_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return loss / int(targ.shape[0]), acc / int(targ.shape[0])\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return loss / int(targ.shape[0]), acc / int(targ.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10  Batch: 1000  Loss: 0.6216  Accuracy: 0.2500  Time: 184s\n",
      "Epoch: 1/10  Batch: 2000  Loss: 0.2330  Accuracy: 0.5357  Time: 371s\n",
      "Epoch: 1/10  Batch: 3000  Loss: 0.2577  Accuracy: 0.5714  Time: 558s\n",
      "Epoch: 1/10  Batch: 4000  Loss: 0.2453  Accuracy: 0.4643  Time: 750s\n",
      "Epoch: 1/10  Batch: 4750  Loss: 0.2474  Accuracy: 0.4286  Time: 893s\n",
      "\n",
      "================================\n",
      "Epoch 1/10\n",
      "Accuracy: 0.4445  Loss: 0.4693  val_acc: 0.4267  val_loss: 0.6110\n",
      "Time taken for epoch 1: 16.26 min\n",
      "Total Time taken: 16.26 min\n",
      "================================\n",
      "\n",
      "Epoch: 2/10  Batch: 1000  Loss: 0.1616  Accuracy: 0.4643  Time: 196s\n",
      "Epoch: 2/10  Batch: 2000  Loss: 0.2425  Accuracy: 0.5000  Time: 381s\n",
      "Epoch: 2/10  Batch: 3000  Loss: 0.4742  Accuracy: 0.3929  Time: 567s\n",
      "Epoch: 2/10  Batch: 4000  Loss: 0.1682  Accuracy: 0.4643  Time: 750s\n",
      "Epoch: 2/10  Batch: 4750  Loss: 0.0982  Accuracy: 0.6071  Time: 888s\n",
      "\n",
      "================================\n",
      "Epoch 2/10\n",
      "Accuracy: 0.5405  Loss: 0.1359  val_acc: 0.4603  val_loss: 0.4545\n",
      "Time taken for epoch 2: 16.08 min\n",
      "Total Time taken: 32.34 min\n",
      "================================\n",
      "\n",
      "Epoch: 3/10  Batch: 1000  Loss: 0.0616  Accuracy: 0.4643  Time: 184s\n",
      "Epoch: 3/10  Batch: 2000  Loss: 0.0031  Accuracy: 0.5357  Time: 369s\n",
      "Epoch: 3/10  Batch: 3000  Loss: 0.0741  Accuracy: 0.6429  Time: 556s\n",
      "Epoch: 3/10  Batch: 4000  Loss: 0.2041  Accuracy: 0.4286  Time: 742s\n",
      "Epoch: 3/10  Batch: 4750  Loss: 0.4247  Accuracy: 0.6071  Time: 881s\n",
      "\n",
      "================================\n",
      "Epoch 3/10\n",
      "Accuracy: 0.5517  Loss: 0.1016  val_acc: 0.4924  val_loss: 0.3861\n",
      "Time taken for epoch 3: 16.00 min\n",
      "Total Time taken: 48.34 min\n",
      "================================\n",
      "\n",
      "Epoch: 4/10  Batch: 1000  Loss: 0.0166  Accuracy: 0.6071  Time: 188s\n",
      "Epoch: 4/10  Batch: 2000  Loss: 0.2538  Accuracy: 0.4643  Time: 373s\n",
      "Epoch: 4/10  Batch: 3000  Loss: 0.1086  Accuracy: 0.6071  Time: 558s\n",
      "Epoch: 4/10  Batch: 4000  Loss: 0.0053  Accuracy: 0.6071  Time: 744s\n",
      "Epoch: 4/10  Batch: 4750  Loss: 0.1600  Accuracy: 0.5000  Time: 887s\n",
      "\n",
      "================================\n",
      "Epoch 4/10\n",
      "Accuracy: 0.5560  Loss: 0.0873  val_acc: 0.4999  val_loss: 0.3371\n",
      "Time taken for epoch 4: 16.14 min\n",
      "Total Time taken: 64.48 min\n",
      "================================\n",
      "\n",
      "Epoch: 5/10  Batch: 1000  Loss: 0.0532  Accuracy: 0.5714  Time: 188s\n",
      "Epoch: 5/10  Batch: 2000  Loss: 0.0560  Accuracy: 0.5714  Time: 375s\n",
      "Epoch: 5/10  Batch: 3000  Loss: 0.0007  Accuracy: 0.6786  Time: 562s\n",
      "Epoch: 5/10  Batch: 4000  Loss: 0.0799  Accuracy: 0.4643  Time: 748s\n",
      "Epoch: 5/10  Batch: 4750  Loss: 0.0027  Accuracy: 0.6429  Time: 887s\n",
      "\n",
      "================================\n",
      "Epoch 5/10\n",
      "Accuracy: 0.5584  Loss: 0.0790  val_acc: 0.5100  val_loss: 0.2398\n",
      "Time taken for epoch 5: 16.13 min\n",
      "Total Time taken: 80.60 min\n",
      "================================\n",
      "\n",
      "Epoch: 6/10  Batch: 1000  Loss: 0.1159  Accuracy: 0.5357  Time: 189s\n",
      "Epoch: 6/10  Batch: 2000  Loss: 0.0009  Accuracy: 0.4643  Time: 375s\n",
      "Epoch: 6/10  Batch: 3000  Loss: 0.0519  Accuracy: 0.5357  Time: 564s\n",
      "Epoch: 6/10  Batch: 4000  Loss: 0.0024  Accuracy: 0.6786  Time: 751s\n",
      "Epoch: 6/10  Batch: 4750  Loss: 0.0543  Accuracy: 0.6071  Time: 891s\n",
      "\n",
      "================================\n",
      "Epoch 6/10\n",
      "Accuracy: 0.5603  Loss: 0.0730  val_acc: 0.5071  val_loss: 0.2342\n",
      "Time taken for epoch 6: 16.17 min\n",
      "Total Time taken: 96.78 min\n",
      "================================\n",
      "\n",
      "Epoch: 7/10  Batch: 56  Loss: 0.0524  Accuracy: 0.5714  Time: 11s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6f1ea43c976c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoneme_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-d6c654939a9b>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, targ_tokenizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# forward algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdec_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden_c\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdec_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarg_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<start>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-3383fcf6ba2b>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# ResNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape_pyramidal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-3b0f1942d325>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_tensor, training)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    799\u001b[0m                                      \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m                                      \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m                                      self.epsilon)\n\u001b[0m\u001b[0;32m    802\u001b[0m     \u001b[1;31m# If some components of the shape got lost due to adjustments, fix that.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \u001b[1;31m# the precise order of ops that are generated by the expression below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n\u001b[1;32m-> 1442\u001b[1;33m         offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[1;34m(a, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_run_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mvalue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[0mvariable_accessed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     result = gen_resource_variable_ops.read_variable_op(self._handle,\n\u001b[1;32m--> 608\u001b[1;33m                                                         self._dtype)\n\u001b[0m\u001b[0;32m    609\u001b[0m     \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;34m\"ReadVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \"dtype\", dtype)\n\u001b[0m\u001b[0;32m    571\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run eagerly will make tensorflow run step by step or else it will raise\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "# similar to Pytorch, which is a dynamic graph for deep learning\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 10\n",
    "TOLERANCE = 0.08\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # train the encoder-decoder model\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for inp, targ in dataset.take(STEP_PER_EPOCH):\n",
    "        batch += 1\n",
    "        \n",
    "        batch_loss, batch_accuracy = train_step(inp, targ, phoneme_tokenizer)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "            \n",
    "        print(\"Epoch: {}/{}  Batch: {}  Loss: {:.4f}  Accuracy: {:.4f}  Time: {:.0f}s\".\n",
    "              format(epoch, EPOCHS, batch, batch_loss.numpy(), batch_accuracy.numpy(), time.time()-epoch_start), \n",
    "              end=\"\\r\")\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            print()\n",
    "    print()\n",
    "    # saving (checkpoint) the model when total loss is less than 0.9\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    # validation process\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    for val_inp, val_targ in dataset_val.take(VAL_WAV_SIZE):\n",
    "        val_loss, val_acc = validate_step(val_inp, val_targ, phoneme_tokenizer)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "\n",
    "    # print out the epoch results\n",
    "    mean_total_acc = total_accuracy / STEP_PER_EPOCH\n",
    "    mean_total_loss = total_loss / STEP_PER_EPOCH\n",
    "    \n",
    "    mean_val_acc = total_val_acc / VAL_WAV_SIZE\n",
    "    mean_val_loss = total_val_loss / VAL_WAV_SIZE\n",
    "    \n",
    "    print(\"\\n================================\")\n",
    "    print(\"Epoch {}/{}\".format(epoch, EPOCHS))\n",
    "    print('Accuracy: {:.4f}  Loss: {:.4f}  val_acc: {:.4f}  val_loss: {:.4f}'.format(\n",
    "        mean_total_acc, \n",
    "        mean_total_loss, \n",
    "        mean_val_acc,\n",
    "        mean_val_loss))\n",
    "    print('Time taken for epoch {}: {:.2f} min'.format(epoch, (time.time() - epoch_start)/60))\n",
    "    print('Total Time taken: {:.2f} min'.format((time.time() - start)/60))\n",
    "    print(\"================================\\n\")\n",
    "    \n",
    "    if mean_total_loss < TOLERANCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, max_input_len, max_output_len, tokenizer=None):\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    \n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    \n",
    "    \n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    \n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            inputs=dec_input, \n",
    "            enc_hidden_h=dec_hidden_h, \n",
    "            enc_hidden_c=dec_hidden_c, \n",
    "            enc_output=enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [309]\n",
      "Predicted translation: w ah n <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "19\t--->\tw\n",
      "11\t--->\tah\n",
      "3\t--->\tn\n",
      "2\t--->\t<end>\n"
     ]
    }
   ],
   "source": [
    "# testing with test wave data\n",
    "index = [i for i in np.random.randint(len(wav_tensor_val), size=1)]\n",
    "print(f\"Index: {index}\")\n",
    "test_wave = wav_tensor_val[index]\n",
    "test_phoneme = phoneme_tensor_val[index, :]\n",
    "\n",
    "translate(test_wave, sample_output.shape[1], test_phoneme.shape[1], phoneme_tokenizer)\n",
    "for tensor in test_phoneme:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
