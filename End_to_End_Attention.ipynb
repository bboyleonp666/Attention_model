{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reference: \n",
    "    https://arxiv.org/abs/1508.01211\n",
    "    https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    https://github.com/jameslyons/python_speech_features\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "    \n",
    "data source: \n",
    "    https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveReader:\n",
    "    def __init__(self, path, sample_rate, padding_type, read_size):\n",
    "        '''\n",
    "        Args:\n",
    "            path: train path containing directory which one would like to load\n",
    "            sample_rate: sample rate for reading .wav file\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "            read_size: size that one would like to read\n",
    "        '''\n",
    "        \n",
    "        self.path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.padding_type = padding_type\n",
    "        self.read_size = read_size\n",
    "\n",
    "    def read(self, labels=None):\n",
    "        '''\n",
    "        read all the data under the labels(directories) one select\n",
    "        \n",
    "        Args:\n",
    "            labels: labels(directories) one would like to load\n",
    "                    None means read all the directories under that directory\n",
    "        '''\n",
    "        print(\"LABEL\\tTOTAL\\tREAD\\tSAVED\\t<1s COUNT\")\n",
    "        print(\"-----\\t-----\\t----\\t-----\\t---------\")\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f for f in os.listdir(path) if os.path.isdir(path + \"\\\\\" + f)]\n",
    "            \n",
    "        elif type(labels) == str:\n",
    "            samples, total_wave_count, total_wave_read, total_loss_count = self.read_dir(dir_name=labels)\n",
    "            sample_labels = np.repeat(labels, total_wave_read)\n",
    "            \n",
    "            print(\"\\n\\nMISSION COMPELTE!!!\")\n",
    "            return samples, sample_labels, total_wave_count, total_loss_count\n",
    "                    \n",
    "        label_len = len(labels)\n",
    "        total_wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "        total_wave_read = np.zeros(label_len, dtype=np.int32)\n",
    "        total_loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for i, lab in enumerate(labels):\n",
    "            samp, total_wave_count[i], total_wave_read[i], total_loss_count[i] = self.read_dir(dir_name=lab)\n",
    "            \n",
    "            if i == 0:\n",
    "                samples = samp\n",
    "                sample_labels = np.repeat(lab, total_wave_read[i])\n",
    "            else:\n",
    "                samples = np.concatenate((samples, samp), axis=0)\n",
    "                sample_labels = np.concatenate((sample_labels, np.repeat(lab, total_wave_read[i])), axis=None)\n",
    "        \n",
    "        print(\"\\n\\nMISSION COMPELTE!!!\")\n",
    "        return samples, sample_labels, total_wave_count, total_loss_count\n",
    "    \n",
    "    def read_dir(self, dir_name):\n",
    "        '''\n",
    "        read one directory of given directory name\n",
    "        \n",
    "        Args:\n",
    "            dir_name: directory name\n",
    "        '''\n",
    "        dir_path = os.path.join(self.path, dir_name)\n",
    "        wave_files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n",
    "        total_wave_files = len(wave_files)\n",
    "\n",
    "        if self.read_size is not None:\n",
    "            wave_files_read = self.read_size\n",
    "        else:\n",
    "            wave_files_read = total_wave_files\n",
    "\n",
    "        samples = np.zeros((wave_files_read, self.sample_rate))\n",
    "        less_than_1s_count = 0\n",
    "        num_of_file_read = 0\n",
    "        for i, wav_file in enumerate(wave_files):\n",
    "            wave_file_path = os.path.join(dir_path, wav_file)\n",
    "            samp, _ = librosa.load(wave_file_path, sr=self.sample_rate)\n",
    "\n",
    "            pad_size = self.sample_rate - len(samp)\n",
    "            if pad_size > 0:\n",
    "                less_than_1s_count += 1\n",
    "                if self.padding_type is None:\n",
    "                    # None: than skip this wave file\n",
    "                    continue\n",
    "\n",
    "                elif self.padding_type == \"white_noise\":\n",
    "                    # white_noise: pad white noise data behind\n",
    "                    padding = np.random.normal(0, 0.02, pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # zero: pad zeros behind\n",
    "                    padding = np.zeros(pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "            else:\n",
    "                samples[num_of_file_read, :] = samp\n",
    "                num_of_file_read += 1\n",
    "\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(dir_name, \n",
    "                                              total_wave_files, \n",
    "                                              i+1, \n",
    "                                              num_of_file_read, \n",
    "                                              less_than_1s_count), end=\"\\r\")\n",
    "            \n",
    "            if num_of_file_read == wave_files_read:\n",
    "                break\n",
    "                \n",
    "        print()\n",
    "\n",
    "        return samples, total_wave_files, wave_files_read, less_than_1s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\tSAVED\t<1s COUNT\n",
      "-----\t-----\t----\t-----\t---------\n",
      "zero\t2376\t2160\t2000\t160\n",
      "one\t2370\t2262\t2000\t262\n",
      "two\t2373\t2222\t2000\t222\n",
      "three\t2356\t2207\t2000\t207\n",
      "four\t2372\t2201\t2000\t201\n",
      "five\t2357\t2185\t2000\t185\n",
      "six\t2369\t2164\t2000\t164\n",
      "seven\t2377\t2194\t2000\t194\n",
      "eight\t2352\t2232\t2000\t232\n",
      "nine\t2364\t2178\t2000\t178\n",
      "\n",
      "\n",
      "MISSION COMPELTE!!!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "reader = WaveReader(path=train_audio_path, \n",
    "                    sample_rate=SAMPLE_RATE, \n",
    "                    padding_type=None, \n",
    "                    read_size=2000)\n",
    "\n",
    "wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words)\n",
    "\n",
    "# print(\"\\nCheck the existence of NaN and Inf\")\n",
    "# print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "# print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, create_size, min_sz=6, max_sz=8, padding_type=\"zero\"):\n",
    "        '''\n",
    "        Args:\n",
    "            create_size: size of binding wave one would like to create\n",
    "            min_sz: minimum size of wave data\n",
    "            max_sz: maximum size of wave data\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "        '''\n",
    "        self.create_size = create_size\n",
    "        self.min_sz = min_sz\n",
    "        self.max_sz = max_sz\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "    def simulate_wave(self, waves):\n",
    "        '''\n",
    "        method for simulating wave inputs\n",
    "        which will concatenate audio inputs for building longer audio dataset\n",
    "        \n",
    "        Args:\n",
    "            waves: input wave data\n",
    "        '''\n",
    "        # get picker for combining waves and labels(phonemes)\n",
    "        self.wave_shape = waves.shape\n",
    "        self.pickers = self.get_picker()\n",
    "        \n",
    "        print(\"Wave Data Simulation ... \", end=\"\")\n",
    "        \n",
    "        binded_length = self.wave_shape[1]*self.max_sz\n",
    "        simu_wave = np.zeros((self.create_size, binded_length))\n",
    "        \n",
    "        \n",
    "        for i, picker in enumerate(self.pickers):        \n",
    "            tmp_simu_wave = np.array([waves[p] for p in picker]).flatten()\n",
    "            \n",
    "            pad_size = binded_length - len(tmp_simu_wave)\n",
    "            if pad_size > 0:\n",
    "                if self.padding_type == \"white_noise\":\n",
    "                    # padding white noise\n",
    "                    padding = np.random.normal(0, 0.02, size=pad_size)\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # padding zeros\n",
    "                    padding = np.zeros(pad_size)\n",
    "\n",
    "                simu_wave[i] = np.concatenate((tmp_simu_wave, padding), axis=None)\n",
    "                \n",
    "            else:\n",
    "                simu_wave[i] = tmp_simu_wave\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_wave\n",
    "    \n",
    "    def get_picker(self):\n",
    "        '''\n",
    "        picker stands for index pick\n",
    "        this is for combining audio data with decided minimum and maximum size\n",
    "        '''\n",
    "        size = np.random.randint(low=self.min_sz, \n",
    "                                 high=self.max_sz+1, \n",
    "                                 size=self.create_size)\n",
    "\n",
    "        picker = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, s in enumerate(size):\n",
    "            picker[i] = np.random.choice(self.wave_shape[0]-1, size=self.max_sz, replace=False)[:s]\n",
    "            \n",
    "        return picker\n",
    "\n",
    "    def simulate_label(self, labels):\n",
    "        '''\n",
    "        method for simulating label inputs which will concatenate labels following simulated waves\n",
    "        \n",
    "        Args:\n",
    "            labels: input labels following with audio dataset\n",
    "        '''\n",
    "        print(\"Label Simulation ... \", end=\"\")\n",
    "        \n",
    "        simu_label = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, picker in enumerate(self.pickers):\n",
    "            simu_label[i] = np.array([labels[p] for p in picker])\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_label\n",
    "\n",
    "    def simulate_phoneme(self, labels, label_dict, phoneme_dict):\n",
    "        '''\n",
    "        method for sumulating phoneme inputs\n",
    "        which will concatenate audio phonemes with labels we concated by simulate_label()\n",
    "        \n",
    "        Args:\n",
    "            labels: labels that one would like to transfer\n",
    "            label_dict: label dictionary\n",
    "            phoneme_dict: phoneme dictionary\n",
    "        '''\n",
    "        print(\"Phoneme Simulation... \", end=\"\")\n",
    "        \n",
    "        self.label_dict = label_dict\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "\n",
    "        simu_phoneme = np.empty(self.create_size, dtype=np.object)\n",
    "        for i, label in enumerate(labels):\n",
    "            simu_phoneme[i] = \" \".join([self.phoneme_translator(lab) for lab in label])\n",
    "            simu_phoneme[i] = \"<start> \" + simu_phoneme[i] + \" <end>\"\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_phoneme\n",
    "\n",
    "    def phoneme_translator(self, input_label):\n",
    "        '''\n",
    "        translate labels to phoneme if simulate_phoneme is called\n",
    "        \n",
    "        Args:\n",
    "            input_label: label that one would like to transfer into phonemes\n",
    "        '''\n",
    "        for i, label in enumerate(self.label_dict):\n",
    "            if input_label == label:\n",
    "                return self.phoneme_dict[i]\n",
    "            \n",
    "    def tokenize(self, phoneme):\n",
    "        '''\n",
    "        with tensorflow we can simply apply Tokenizer for text(in our case, phoneme)\n",
    "        to generate phoneme outputs\n",
    "        \n",
    "        Args:\n",
    "            phoneme: phoneme string with '<start>' and '<end>'\n",
    "        '''\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(phoneme)\n",
    "        tensor = tokenizer.texts_to_sequences(phoneme)\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, tokenizer\n",
    "\n",
    "    def show_convert(self, tensor, tokenizer):\n",
    "        '''\n",
    "        showing case of tokenized word according to its index\n",
    "        \n",
    "        Args:\n",
    "            tensor: phoneme tensor\n",
    "            tokenizer: phoneme tokenizer\n",
    "        '''\n",
    "        print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "        print(\"=======================\")\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave Data Simulation ... Done\n",
      "Label Simulation ... Done\n",
      "Phoneme Simulation... Done\n",
      "\n",
      "Example Label Display: ['seven']\n",
      "Example Phoneme Display: <start> S EH V AH N <end>\n"
     ]
    }
   ],
   "source": [
    "CREATE_SIZE = 2000\n",
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(create_size=CREATE_SIZE, \n",
    "                            min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "simu_wave = preprocesser.simulate_wave(wav_array)\n",
    "simu_label = preprocesser.simulate_label(label_array)\n",
    "simu_phoneme = preprocesser.simulate_phoneme(labels=simu_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {simu_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    '''\n",
    "    This is the Mel-Frequency Cepstral Coefficients, MFCCs Transformation\n",
    "    including\n",
    "        1. Pre-emphasis\n",
    "        2. Framing\n",
    "        3. Hamming window\n",
    "        4. Short-time Fourier Transform\n",
    "        5. Mel triangular bandpass filters\n",
    "        6. Log energy\n",
    "        \n",
    "    not including\n",
    "        7. Discrete cosine transform\n",
    "        8. Delta cepstrum\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter):\n",
    "        '''\n",
    "        Args:\n",
    "            alpha: coefficient applied when applying pre-emphasis, usually between (0.95, 0.98)\n",
    "            frame_size: duration of one frame in second\n",
    "            frame_stride: stride duration of one frame in second\n",
    "            n_fft: decided number while applying Fast Fourier Transformation\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate, apply_delta=False):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        mfcc_features = np.column_stack((energy, fbank))\n",
    "        \n",
    "        if apply_delta:\n",
    "            return np.concatenate(mfcc_features, self.delta(mfcc_features))\n",
    "        else:\n",
    "            return mfcc_features\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                              # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                           # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz\n",
    "    \n",
    "    def delta(self, mfcc_features, neighbor_len=2):\n",
    "        denominator = 2 * sum([i**2 for i in np.arange(1, neighbor_len+1)])\n",
    "        delta_feat = numpy.empty_like(mfcc_features)\n",
    "        padded = numpy.pad(mfcc_features, ((neighbor_len, neighbor_len), (0, 0)), mode='edge') # padded version of feat\n",
    "        for t in range(len(mfcc_features)):\n",
    "            delta_feat[t] = numpy.dot(numpy.arange(-neighbor_len, neighbor_len+1), \n",
    "                                      padded[t : t+2*neighbor_len+1]) / denominator # [t : t+2*N+1] == [(N+t)-N : (N+t)+N+1]\n",
    "        return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCApplier:\n",
    "    '''\n",
    "    This is the MFCC applier for applying MFCC \n",
    "    and pad zeros for fitting the data into Encoder-Decoder Model with Attention\n",
    "    which we will build later\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter):\n",
    "        '''\n",
    "        Arg:\n",
    "            mfcc: build by MFCC class for transform inputs\n",
    "            decide_size: a 2^k number which will make the inputs reshape into X*decide_size\n",
    "                         which will help us build the pyramidal RNN encoder\n",
    "        '''\n",
    "        self.mfcc = MFCC(alpha=alpha, \n",
    "                         frame_size=frame_size, \n",
    "                         frame_stride=frame_stride, \n",
    "                         n_fft=n_fft, \n",
    "                         n_filter=n_filter)\n",
    "        \n",
    "    def apply(self, inputs, sample_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: wave data that one would like to process\n",
    "        '''\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        sample = self.mfcc.mfcc(inputs[0, :], sample_rate)\n",
    "        sample_shape = sample.shape\n",
    "        outputs = np.zeros(((input_shape[0], sample_shape[0], sample_shape[1])))\n",
    "        for i in np.arange(input_shape[0]):\n",
    "            outputs[i, :, :] = self.mfcc.mfcc(inputs[i, :], sample_rate)\n",
    "            print(f\"Applying MFCC to {i+1}th case\", end=\"\\r\")\n",
    "            \n",
    "        print()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALPHA = 0.95\n",
    "# FRAME_SIZE = 0.025\n",
    "# FRAME_STRIDE = 0.01\n",
    "# N_FFT = 512\n",
    "# N_FILTER = 20\n",
    "\n",
    "# mfcc_applier = MFCCApplier(alpha=ALPHA, \n",
    "#                            frame_size=FRAME_SIZE, \n",
    "#                            frame_stride=FRAME_STRIDE, \n",
    "#                            n_fft=N_FFT, \n",
    "#                            n_filter=N_FILTER)\n",
    "\n",
    "# mfcced_simu_wave = mfcc_applier.apply(simu_wave, sample_rate=SAMPLE_RATE)\n",
    "# print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_simu_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [755]\n",
      "[array(['nine'], dtype='<U4')]\n",
      "['<start> N AY N <end>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,UklGRiR9AABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACABAAZGF0YQB9AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/+z/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/+z/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAFAAUAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/+z/AAAAAAAA7P/s/+z/7P/X/9f/AAAAAOz/1//X/9f/7P/s/+z/AADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACkAKQAUABQAAAAUACkAKQAUABQAAAAUABQAAAAUABQAFAAUABQAFAAAABQAFAAAAAAAAAAAAAAAAAAAAOz/7P8AAAAA7P/s/9f/1//s/9f/1//s/8P/w//X/9f/1//D/8P/w//X/8P/w//D/8P/w//X/+z/AAAAAAAAAAAAAAAAAAAAAAAAAAApAD0APQA9AD0AUgBnAHsAewCQAHsAZwBnAGcAZwBnAGcAZwA9AD0AKQA9ACkAKQAUAAAAAAAUAAAAAADs/+z/7P/s/+z/1//D/8P/1//X/9f/1//X/67/rv+u/5n/hf+F/0f/Mv9H/0f/R/8e//T+Cf8e/x7/Hv8J/x7/R/9H/1v/cP+F/5n/w//X/+z/AAAUAD0AZwClAPcADAEgAUoBiAGxAe8BAwIYAi0CQQJrAmsCawJ/AkECQQJBAlYCQQIYAu8B2gHFAXMBXgEgAc4ApQBnABQAw/9w/x7/tv54/hH+qv1D/bP8S/zk+2j72Ppc+vX5jvkS+av4GvjI95/3ivdh92H3Yfd197P3GviW+BL5zPmF+n37ifxs/aL+7P81Ab0CRQS5BVUHBwmPCu0LTA2WDswPwxCmEWASxxIFEwUT8RKeEjcSkhGFEFAP3Q1VDLgKBwkXBxMF5gKlAKL+YPwz+kT4F/YT9E3yh/DV7nftGOz36hTqRum16E7oJegl6E7ooegx6f/pDOtB7GLtmO4L8NHxgvNd9WH3O/k/+1j9R/9eAaADIAafCPYK+gwSD2gRgROFFV8X0xgdGlIbIRzGHC0dVh0tHcYcNRxSGx0aqhgNF0cVbBM/EcAOKwyCCcUGBwRKAcv+N/x5+dD2evRN8jTwWu6o7CDrwem16NLnGeed5krmDOY25ojmxuYZ577noejB6c7qA+wP7UXupO9A8fLyevTt9WH36fjD+rP8T/7D/3MBtQMLBk0IEwrEC7MNzA8OEhEUmRXkFhkYTxlGGusaZxuQG3wbZxsVG0YaERnHF1MWohTxEhYR1A5VDNUJLAeDBNoBhf8F/Zr6gvhU9jz0TfKb8D3vB+5N7ZTs7+te6wzr4uri6iDrnOsY7KjsYu3y7YPuZu+b8Kjxi/Ju8zz0H/UX9g738fer+GX5R/oq+zf8WP1k/oX/NQEQA5gECwaoB5cJrwvdDcwPKxGzEmQUrhXPFgQY5xhjGd8ZRhoyGsoZOhlXGDYXFRaiFMcSrxAvDpsLGwmHBvMDNQG2/jf8ovlh9131RPN+8fbvg+537dLsGOxz6wzrueq56jXrsest7L3sd+0w7v/u9u8D8Q/yG/PV8470H/XE9bz2Yffc92346fh5+R76AfvQ+9z8ZP7s/4gBJAPWBJsGyQg0C2ENJw8WERoT4BR8FgQYJRkIGusa4xtKHDUczhvrGrYZqhghF0cVGhOaEPEN9goPCBMFGAIy/3X84PlM98z0n/Kb8MHuTe1B7HPrzupS6uvpwenB6SnqkOog6+/r0uy17W7uUe9J8EDxYfJZ89XzUfTM9F31AvZ+9rz2+fY395/3lvh5+TP6pvsu/cv+ewAtAhwEXgYHCYYLsw31DyISTxRTFi4YoRkAG0ocgB05Hk4eJR6pHcYczhtvGpUYfBYRFFQRbQ40C+YHmARKATv+P/uC+Jv18vKb8IPuvexK6ynqMel36DnoEOgQ6Dnooegx6f/pIOtB7E3tMO49713wfvG08sDzZfTh9F312fVA9rz2Dvc393X3yPdt+Cf5M/q7+2z9Cf+5ANICKAXRB6MKTA3gD4kSHhVfF7YZuhtWHd4eZiCcISwiQSLuITUhKSDeHhgdrRoEGAkV0BFYDqMKsAaoAgn/u/tt+F31OPJ77/vs4upa6SXoGedz5jbmIeYh5ojmQuf85wjpPeqx6w/tRe5m74fwvPEG81H0NPWv9Rf2afa89jf3iveK94r3yPcv+L/4jvnt+on8T/4UAEECgwQXBxMK+gzMD4kSXBXwF0YaXxw5HtYfXiGoIncjiyNNI70i2iGQILUecxyhGZEWQxPMD+0L0QfJA+z/+ftY+Ar10fHV7i3s/+kl6Nvm+OVT5RXl6+Q+5ePlneaU57Xo1ulK6+bsbu7N7xfxTfKC88z0Avb59mH3YfeK98j3Bvga+Nz3iveK95/3Gvj9+Ef65PuB/Uf/iAEHBMUGrAmnDI4PYBJcFS4YmRrGHLUeZiADIncjWSRuJEUkySPRIkkhWh/vHAgazxZYE3kPSAvZBqgCeP5x+n723fKP73/s6+m+587lhOSN4xHj6OIl47bjcOQ+5V/mlOfz6KTqf+ww7uLvVfGL8v7zcfWS9or38ffx9xr4L/gv+C/48fd19zf3TPez9234zPlU+8f8jf65AGIDcgaCCWkMJw83EoUVgBgVG0IdHB+5IGoi8iOsJKwkbiTJI70iSSFvH9scthk/FrMS1A6jCjQGxQFD/dT4CvV+8RzuIOtj6CHmcORP45XiGeLH4QXiquJ445nkzuUZ53foKeot7DDuze9q8d3yZfTt9WH3WPi/+L/4q/ir+NT41PiC+Ab4iveK9/H3q/jg+X37Gv0J/5wBRQRBB3oKyA0BESYUSxdGGu8cRh81Ib0iWSS4JbAmxSZJJjwlHCSUIlIgqR1bGpEWdRJEDsAJ6gQpAJL7Dvfd8j3vA+zz6CHmy+MZ4g3h5ODk4Lrg5OCy4fziW+Tj5YDnMekM6w/tPe9V8QbzUfSv9Q73WPiO+R769fll+RL56fj9+Kv4BvhM99D2I/dE+Mz5VPvw/PT+XgFFBNEHhgvADrsR9BRXGKUbjB7OIJQiHCS4JWondihhKH4nXSb/JGIjSSFOHlsaFRa7EXUNyQjJA8v+t/nM9NnwTe096i3nW+Qu4rrgU+B94JHgkeD44EPi3+PO5ZTnCOmQ6lbsg+7Z8PLyjvSv9bz2yPcS+Qr6R/rg+dT4Bvjc97P3TPd+9pv1SPXt9bP39fk3/E/+uQDJA2oHXQs7D2ASMhVsGOMbMR/FIaAjlyRmJdkmYSjyKA8oHybJI14h3h4MHEIYBROKDd0IwQTOAAX9/fjM9H7xuO8L8BfxcvBa7jnt++zm7Mnt3u2H67XoVucE57Hmc+aR5d/jeOMp5ann8+i16NLnQufS58roCOmU51vkdOE/4OTgleJG5HPm6+n273n5wQT1DzIaySP5LCo2HD8LRllJ6UmCSbNI2UZFRBRA8zkFM2Et8igwJBAeaBbxDY8FT/6z97DwCOmd4eTblthM1/rWONdE2MTaR98p5ffqpO8b88T1Bvhc+g385PsK+mH3zPSL8iDwYu2Y6T7l8OEq4IXfHt/g3svecN9L4cvjpeVz5nPmuuV85e/mwek57ZPxBvjiAKcMwhqXKXQ3D0NUTIBTxldrWFtV/U5yRsY8VzOjKmoiwhpkFHkPfgzhCgcJIAZWAmz9YfdV8e/r2+Yu4o3eBd3U3T/g9OPz6DDuMPOK9wH7Gv1Y/ab7bfgn9LjvLeyE6WvnuuVb5KHjOuP84hHjoeOZ5M7liOYt56Hoe+pz6wzrZ+qY6RDoiOal5QDlKeXS50XugvhJBqYW7ibXNWpCjko7T7tRaFEOTR9GJD6iNI4qQSJ8Gz8WOxS2FAEW+BbXFX0RZQoDAnn5avGH67HmLuLg3sfcN9x53njjueqf8gr6w//vAc4AWP2K9xfxXusE567keOMR49PiGeIu4njjuuXK6Errf+yx6/PoSubC5HDk6+Rn5Yjmqedr553m4+Wu5OvkHOkP8qL+Gg7WH8MwjD7dSKJPBVMuU6JPREkgQfg2XSuQIBkYYBL1D0sSdBdSG4gc6xqZFQ4NMARg/I70ye1O6KHjkeBc333gcOS56rTyw/riAMkDawIu/cT1oO3b5vDhed5g3JLbDtxs3ezftuN36H/sPe9d8Cjv7+sl6KXlFeWR5bHm5+d36NLnNuaE5KriNuFX4mvnavGQAJYTvCdOOfZFdU3sUM9R7FCKTQJHaz3YMOYiuhaCDnILRA6NFJAbZiA9ID4bsxLRBxH+n/fJ8tXunOsQ6HDkbOK240bpI/Jg/GYFEwoHCVYCyPcD7NvhaduO2a/as9wm3prfsuGZ5Ajp6u6O9Nz3+fZN8iDr3+Nw343efeAy5GPoxesP7QPsHOkV5eTgy94F4hjsYPyFEI8lQjiXRGVK3E31T8tP3E1ZSXtApjFWIr8T/gmfCO0LIRcDIiMoZSqkJcYc1A5zAb/42fAP7XPr1ukl6KXlNubr6dnwt/leAeIFyQPQ+3LwtuPh2erT3dLt1eHZt96N4xDoOe0P8pL2jvm/+I70i+2R5a7fgd153onhKeUl6HvqNetS6iXocOTs3xrd9d6p55b4jg+8J04+O08hV2tYPlZ9UXlKckFfN68r+x1LEu0L4QqjD2MZ3iNxK2Et+ifvHNQO7P/y8hTqU+W+4rLhleJb5O/mA+yX8+T7EAPFBsEE3Pyo8T7lHtpZ0+bRetSj2cPfiOYP7cnyL/h1/Oj9IvzQ9sHu1+TQ24rXL9gi3HThvucD7Bjsoei248veSNr+2HDfJO30/jIV1C7hRfRUGF17YDVcU1E8RYw53C1WIlcYFhG8DEgL/Q5CGKgiJyojLaspCB+rDjv+A/EM5kffEt4q4HjjqecP7fLyq/hk/mIDpAWLA578VfEd5N3Xj88QzeLPDtcN4RjsxPVg/FIA9wDo/Zb4m/AZ59TdYdd61I/U3dfU3XDkwekt7LHrjOjo4mzdntzH4U3tPQAyGrY0n0i6VhxfQV3oU2VKnEE2N1QsGCJfF7MNYQjyCJYO0xjVJNAs3C3lJ1Ib9gqv+vvs3+MB4H3gtuP85y3sh/DM9FD5gf3OAJwBW//1+f/uDeHl1rDQFM9d1dff8+gj8qL5dfzg/pwBW/9Y+DTwneYi3MTVNNWK10zc9OPO6lrug+5z69/jO9ks0erOzdQt524EqCJBPTpUzWByYWpdZ1aaS5g/QzP/JBUWJwqIAf39tQOvEFIgrzA1PAw8rzD7HX4Hn/LC5Lfe6N3P4HzlCOnv68Xw8ffD/34HNw28DJgE7fXL46zTCMk2xu/Lp9aN4yDwCvqQAHcD/wTWBCABR/og8BHjxNWUzGfK6s472e/mE/T5+3X8I/fv64Hdk9GUzNrQXN/x91wVrzB2SBxam2HuYUVfLVjYS7E8xCvnGNkGffvI97v7DwglGVAqdDdnO7Y0CyYuE4X/mO784tzcaduB3bLhseaU7Ar1/f1RBWUKuArWBCf5/+lU2zTQIctWzIvS3Nwc6Qr1Cf9eBukJGwkcBAH7bu5o4NXTXsuMyAPMAtb042rxw/om/lz6Pe8B4N3SQczJzX3bNPW/E7sxFkzzXvZln2P3W2RPNUGVMzwlDRebC00DO/6lAO0LOhnZJnA1WzrHMnolDhLD+jnoWN2K13bXJt7j5S3s1fPt+tf/jwUbCQsGmf9A9hDoSNr60YPOINAb2LbjuO/D+hwE/glIC58I0gKO+cntieHQ1o/PB84H03Xcd+gT9ND7Bf0O917rTNzzzVPFusUX0anncgaKKH5HJF5Ia0Nup2ccWkRJkTbmIlQREAMn+bz2dfy0CGMZCitrODk+zjuiL5kadwPq7qrdydIg0KPUZN736kz3sQGPCukOig12CFv/RPNf5graf9EkzTnNrNN53trrR/rZBpoQ6BO3D8UGjvmE6bfZjM2Ax1fHTs3+2O/mZfR4/msCcP/Z9UrmwNN0xgXC48Xd1w73OhnKOZFWjmp8cWBtk2IFU6RAuyzwF+4GN/x69Hr0AwIyFdkmBDjmQv8/8DJvHyQIMPOq4pPW0dH+09TYrt9K6/34YgPZC9ARzA9eBhL5wenE2s3PnMsczrDVpuCU7ET4+wJlCjcNQAw4CEf/D/Jw5N3Xd821yDXL6tN04cXws/x/AtoBgvgt5zTVvsf5wEfEQNaO9DsUGjOaUApmv24rbDBkkVZqQkwt0xibBpr6CvXl9koBiRJuJGQ0FED/P400iyPADo75SuZE2CTSf9GG1ardyui89k0DpwxgEj8R8ggq+z3qFts00KXKTs1+1qbg7+uW+EECJAgCDPoMGwnOAMz02+b12T3PMclzy4/UYOED8YX/IAbeA/n75uyC2FfHO76Ovs7KtuPBBHoqYE0GZOxwW3UnarZUF0I7L3wW5gLc/KL5QPZSAJYTqCIWMQc/akIAO5IsFRZD/THp/thvzo/PyNcV4JTs3PxVBxcMcRC3D3sF3Pd76qfb98/vy2LN9tQu4l3wx/z2BcQLnw32CvYFjf7+86Hoqt3+04zNf8xF0xXgwe6e/JsGyQjFAQbzCd+Iy/W+VLsFwrDVG/PLFNs3x1J2Y/RvU3F2Y/VPQT0fJgIMu/s89LzxHvp6CiEc3C1nO8E/xjwuMxwfAwdd9TbmONdB0c3UFttj41Hv3PxhCLcPXBCjCpAAfvF94ILT0szay6jRlt256uX2DAF+BzQLxAvmB4gB4Pmk79PiDtf3zyTNpM8K2t/ogvh7BRMKpAV9+4TpJNJgwae7yLyVx8vjqAeHJo9FD2OdcmN06G7qX0BHpyy/E4H9h/D/7m7zFADXFRMqQjiLQxdHSjxEKa4VcP9W59DW4s/SzOLP5NuQ6r/4kwciEk8UFhG8Bw73AOUX1rnKDcYMy6fWT+O88eIAEwqzDTsPTA1VBxH+3fKA56fbD9K9zO/LTdIy30XudfybBjAJoANU9r7iJM0GvZ+3I7y1zbXtCRUkPklhc3f/f0V6m2aOSi8uMhXs/6vzBvOz9/cAQxN2KCE390AXR15B7DD4GwMC3+j61nvKNsZazvnbhOm3+ZcJYBK2FBoTegrM+QTnftatyfTDSsZe0FPg2fDL/icK7RDQEfENcgZD/bTyuuX+2EHR3s2MzbjUCOTV86UAUQqSDD0FI/eh46nMr7qgskm1/MeH68sUFDtqXYR1jHlkbzRhu0yeMrobowqz/DT15fY7/hcMRh+FMDk+m0a0Q/Q0FCAkCPvsQNYDzJjJTs3h2dbplviHBgER4BSzEq8Ljf5B7AraGMzCxLrF1s5k3vbvCf+0CCMNIw1ZCbUDgf2G9WfqT9481GLNXss00LPcg+7g/t0IxAtyBlD5peUUz6K+uLmBvcbLb+kBEU80HlCvaxh92neSZ1NW8z4UII8Fn/dA8aTvYfesCc4g8DIHPy9JBkmAOEYfLAdh8qrdHM41y8XQitci4dnwzgBIC8cShRW3D00DYfIJ3z3PGcd8xcbLFtvv60T4QQLpCXUNDg3JCGsCmvo97/jgctX3z+rOdtIv3WLtpvtuBCQIgwRE+D7lQdFkw7O88bz3yoTpKwz5LMtPu2xjeSl2lmkmVMY3JR4bCdT4G/Nx9VT7nwhSG7gqdDd2Q49FYzk0JjMQTPdc30XOGcc+ypvVRuR69CgFaBGuFdMTNw25AJvwAeAX0ZXHkcUMyyPXfOVR9L0CfgxxEMMQdQ3qBNT45uzP4GXUoM3zzebR2NrO6g38VQddC00IWP096kHRYLybtcS688gV5Q4NlTMmTwZkDXIycLxiBVOkQI4qOxR/Aqv4hvXQ9tf/eRRQKs82MT+0Q9Y6KCWnDCv2bOKs02vMwc6r2Lbjm/AgAfENQxPLFHEQ8wOf8j/gA9FOyMbGXstM13fovPalAG4JYQ0XDIIJgwTc/Hr0FOrU3f7T6s7zzVHU6OKC8yABuAoCDLEBZu8G2PnAH7ULtX3As9xiA/Io1EkbZLpxfHGraW9aWUSWLvQZwQQX9tnwavHt+o4PhyaAONVEWUk1QbMtOxQK+t/jPNQMywDKD9IJ3w/tnvztCzIVxxdHFUgL4Plf5jTVEMh0wU/D+8zx3FHvMv8rDJkVgBiNFEAMZwAs8XTh1dOtyYjGIcu41ADl8fc0BhoOTA05A5Px9dlkwzi3tLffwybeoAMTKtFHsVwBbKpu+mJsUxtEKjHrGtEHIvxU9kz3pQArEXImTjnRQv5E8z5EKYYLJ/Q24cXQkMpRz5bY9ONV8Y3+ZQqJEqoTLw4oBcT1S+Fq0fzHhcSYyWnWBOeS9hgCegplD/0OPAqgA578BvPb5tDbPNRe0CDQ2dU64xvzcP/uBmEIZwCD7nbX8MECtiO3R8Ro4H4Hsy1UTLxi6G4ubpdkSlf2RWgxUhv2Bdn1d+3J7S/4kgyPJa06fkcfS+ZCJi87FLf5W+RR1IjLGMxu0zve2utx+g8IxxKdF0cVkgyq/QjpuNTwxiLB9MPqznDfA/GZ/xMKzA9oEeAPEwo1AdD2PerH3GLSd813zUXTuuDd8uIAqAe0CBgCqPEr29vG3LwvvSHG1N1WAsUmbkSUXVhueW9RZSFXrEQGLnwWyQPt+n72CvXX//QUvCcJNblAUUUhPNUpqhN1/PznI9d3zZjO7dWu34PuuQDIDSYUKhZoEcEEgvN04d3SZ8rTx73Mq9gt52X0Mv9+B9AMBg4KC9YE8Pzd8s7l6dja0IPOxdDx11/mlviDBGoH/wTt+iHmd803vCO3xLpSyrnqNxJXM6dM7mFsbkhr0l1PT+o/XSv9E9ICM/on9P7zxQG6FrQovjgLRmlHWzq4JUwNH/Wd4cnSvcza0HnZCOT68UoBvAzcEk8U/Q5/AtHxMt89zyHGCcQAyrPXOeiK9zAENw3QESISyA00BtP91fPn5+Tbn9KPz7TSO9lG5KP0oAOsCe4GO/7y7RfWhb9RtHq0jr7+2Nf/zSX2RW5fhXDnc1hp31nISGwzCBpiA6/11e7J7VD5wxCKKDE6I0hDTgdEDi2qE8P66OIL0L7HSstl1MPf9u+xAUQO9BTbFzIVGwnQ9sLkl9MqxTO/y8Ny0Kbgi/JFBHEQcBUBFrsRDwgN/IfwfOVl2TTQ1s7d0rfZ1+Rx9dYEUQpBB67/NPAO18u+wbP2tLe+p9b5+6AjRUQpWydqT2/EZutVSEb4NqgiFwyz/JL2n/LA84MEoB7cMqRAJ0rdSEs3HRp4/hDowNP8xynKzdRL4cntIvysCQoQSBD9DlEKGv1e6w7ce8/4xY3DA8wi3A/tqv1ADKIUFRazEvYKewAK9VLqkeB219HR5tGT1jLfA+xU+5sGJAg1Af7zKuAZx6S0brNtvSDQ4u/HF3M8T1RNY7tsbWmxXFhO+z2fKJIRmf/y8vvsVfEAAAQYVDGwQf5JUEpSOykgwQSU7FnY883WzpvVCd8g6wb4QQInCpoQQxNcEAsG2fX049HRJsMmvlvEbtNW5/n7Iw0/FksXZBTIDfsCRPjF8EbpEt4H01HPTdKf14nhG/PWBOEKHATx9+/m5swftaSvbbjnxyXjcgvfNDJQ3l7taz5x4WWyUgdE1zWMHs0F3Pfd8iPyJ/l1DSwnazgsQpNHXkFdKy8OK/aA4g/SNcvNzwHbxuZ+8UP9JAjdDaMPjg9uCSL85uxD3azOkcX4xZzQkeCf8v8EsxJCGKYWSBCwBtj6cvCM6AHgn9fq0yvWadtL4ZTsLv0XB8kD6fgM64bVSLqIq+6ww7/I19z8CyaTR/db5Wcmb+1rc1zUSa06myYTChf2+vHR8aP0age4JZk6yUOWSXpFATHoEyf5cOSC04zIZ8pV1i7iJO0B+4sIow+SEYkSgg73AFHv4N4ozybDKsCpx7zW6+lw/1QRWxrjG9sXnw0pAJfz3+ho4KPZ/tOX0yfZIuGH66L5UQVJBuj9ze/h2abAkK+UrM20Kco08NsXtjlTVh9mIm2BbhtkJlQ0RtwyfBZb/6vzzuqk6uj90xiWLklBs03MSk456x+YBPvsVNu5z6DNUdSn29PiRe5o+0kGUA8NFxkYLw4m/hjsyNd0xjO//ML/zmjgE/SHBtMTJRnbF4ETQAwMAXH1h+t94EDWA9FB0dDWBeLu8GcAJwr2BTv5WunR0em4LaycsB6/Dtfg+RgdRjrkUbxiRGlYaRtkNlejRdQuvxMB+wzrX+ZF7jAEMR+uNWlH8U29QrMtfBa//QTnt9l61L3RrNOa2o3jPe91/IsIERT4G40ZFwyO+R3k/84ZwoW/ssaw1RDoAfsKCwkVvhghFzcSPArL/kTztej93eHUnNDA0xrdteib9fsCUQrBBO31eONvzgK7NbCDswXCcdoB+0ocFDsuU2pi8Wg3aHtgslIkQ4EurhVeAU3yxesG82YFqR2iNItDVUctPdEnRA759nPmN9xt2Njat95s4vjl7+sX9sUBNw22FGQU7Qum+53mi9LXxIHCc8sr227uUgDIDQkVZBT9DrQIGAJ9+8z00uwR4/XZE9RZ02XZ1+Tm8f39uQXFAZ/yrt/mzNi6ua/htOzEntyz/DUcBDiKTSlbLGLAZIJkIFy7TAQ4IRycAajxwe7Q9lkJqCL4NvdA1j/TMzUhNw3k+8HuzuUB4OTbEtkG2HHaleLF8AMCFhElGV8XHwuW+CHmDtfWzsHOC9VH3ynq4fRw/2EIEg+SEY4PzAp/Aor3auyq4jfchtr52z/g5+c08Lz2kvsK+nLwEeOP1CHGL7hqtkPCbtMT78wPkizqRD5WUWCGZmln+13cTc47JCN+B5/3cfXg+WoHiBwjLXA1DTczMKgi3BJWApfzY+h94MTap9Zp1ivbHeRA8aUAyA0yFWQU9grY+inqlt3t1XrUG9hP3qXlpO+S+1EF8Q3oE9wSvAx3A7/46u5C52zicN953oXf8OHn52bvXfWC+B/1Neue3NLMcb/dtw68SssR41v/fBv4NthLV1iDX2FjyWPFXNhLHTWIHG4Eq/gB+70CwxDuIacsLy7AKUkhVxijD+oEUPkP7eTg+taT0VHUGt3f6L/45gdlDwoQSAtWAoL41e4h5irgItzE2vHcOuPy7bf5BwRpDB4QNw1VBzUBHvp28ofrIeZX4irgIuE25l7rwe5N8p/yYu3L4xLZFM8Rwze8fcCgzZHlvQJaH045fUwhVy1dIGG1XplVdkhwNSUerwtaBIMExAuAGOIggyQ8JfcgpRuZFScKS/xy8HzlFtvZ1dDWkttP48ntlvhKAbwHrAmHBmcAGvhR7xDoY+N94FzfsuGp55vwpvu5BdkLKwzmB0ECu/ub9SPyuO+U7GPoRuRX4uvk4uos8dD2RPgX8fTjONdOzTrDO770w17QuuVSAOsaADYrR8dNGVNCWPNZzlZ5TzE/FycFEw8IAwfUDksXEB5FJKgirRp8Fi4TEwoF/TTwmeSe3GDcU+D84vjl8+is7pr66gQTCrgKRQTM+eLvhOmU5/znMel76n/sIPCn9nD/NAbRBz0FewCF+gL2/vP68Y/vaux36IjmOehB7PbvZfSS9sXwzuWf11LKv8LcwQjJKNT84uX2zAqQIEs3yEihVDFasVemUcBJTj7cMp8oqR2qEwYOJw8eFXwb/x+DH2MZjg8HBKv4e+8I6TLkGeIZ4oDiHeTS53ftCvUa/agCKAWDBMP//fhZ81Hv++xW7CTtwe6H8B/1kvs9AL0CiAEi/Pn2wPPR8d3yCvXp82bv9+oI6QzrcvAf9Q73PPRe63DfPNTSzHjI08eDzsTa7+sUAEMTNCbjNrlAKEWvS9NTulbHUs1FMzDnHfQUvxPbFykbPhuAGBEUjg+jCuIF9wBH+iPyhOm24+jicORz5jbmruQt5xzulviLA8AJMAm9AoX6UfTR8Qbzm/XI9/H3evQD8X7xr/UW++j9gf2F+gL2tPJ+8ajxtPIG84vyOPIX8XvvPe9d8DTw2uuN41zaRdOxyy3H88192/vsawKBEykgqyloMUU/XFCtWvdbQ1OcQXUtRh9nGzkeziC1HjYX3Q3RBwMHBwkTCigFCvqs7kLnFeVC5xDoDOZj47rgjePq7kP9FwfpCbAGUgBU+8z55PsR/rP8lvhu883vKO+T8VT2ovkv+Cf0D/Kf8rj0I/fc93H1A/He7fLt4u848mX0r/Vh8pjpHt8G2MTVj9Rd1bfZMt/r6dj6mwvXGrQoMjVJQT9MdFI2Ui9OG0QFMx8mUiDjG4QaZxvnGH0RJwpeBgcE5gJeAbv74fRa7oDnQ+Ku3xLeWN104Xvq1fN9+14BrARuBC0CpQClAIgBewBo+x/1j+9/7MntvPH29B/1bvP+8yP3t/mv+o75fvZN8tXu3u1J8Hr0kvY09UDxNevj5aHjleIB4P7YE9RZ2LrgqOyi+agCeQ8UICYvlD1xS9tSZE9hSDE/zzETKv4pnyjFIdsXtw+XCcUG5gdBB9YEDAG3+Z/yJO0t5y7ineEy5D7lBOe97ILz9flH/+YCHASgA7UDBwQHBIgBffuv9XLwnOsU6nftJ/SW+Nz34fS88VXxE/Sz9/X5yPfV8yPyD/IG8wr13PfU+MT1Pe8I6QTna+el5cPfHtpx2pHgseu3+agHeRSkIGkskTZmQPFIaUx2SDk+uzEbKXImSSb7IvAX4QocBLUD2QZhCBwEIvzp87Xt/+lj6PzngOdW53Pm4+Uc6e7w4PmQAKADdwNBAhADewV7BdoBaPv29NHxfvGL8qP0ivf9+Mj3H/Xd8hP06fga/U/+aPt199n1cfX59o75M/qO+Tf30fFK61bniObO5QXiddwe2p7cDOZd9fMDzA+2GQ8jIy0NN9Y/WUR2QwQ9BTObKxcniyOQIJkaVBFhCG4EKAXiBZgEW//Q9gvwoO1N7ajsGOyk6srob+kD7IfwyPe//fcAQQJzAVIAw//X/43+KvtY+A73kvav9dn1s/dM98T1AvYj99T4t/lY+Gn2AvZM9475ffsW+wr6R/qa+mX5NPXN7xjsCOnj5fDhxNqb1W3YieF77yr7JAP9DvgbvCeeMkI4oTlGOvw4hTWrLtEnRSQUIAgaohTMD7wMbQ7MD1EKzgAG+LTy0fF28kDxveyA53Pmyugk7YLz/fhL/Oj9nvwq+4n8KQBiA9ICGv3Q9gL22PrD/1IAv/0B++D5cfo/+2j7FvtH+mX58ff59v34qv01AXD/4Pkj9xL5Dfxo+wr1GOyE5H3g/d2a2sTaGeKo7Jv1u/vg/kkGfBYCJ0cwNzIBMRYx3DIuM7cvTSicIfcgaiKMHiEXSBCPCmEIcgZSAGX5XfU89Onzk/FB7MbmQueo7AvwavE48jjyPPRM9wb4L/hc+qr9pQA1AYX/9P4AAFIArv+2/gX9fftH+r/4BviW+B76Kvtc+sz5cfo3/OD+4P59+6v4Bvj59kTzJO3n57HmOehj6ITkU+DH4QjptPJQ+dD7iAHxDZAbeiV2KEAn3SiBLokyrzCGKx8m+yL3IM4beRQGDh8LVQzACbEBFvtt+Jb4WPgf9RPvPeoM62Lt5uwD7IfrlOyk793ym/Wr+GD8W/9KAX8CAwIDAk0DJAOUAvsCGAJw/7P8VPtg/HD/ewA7/iL8kvuV/aUA1/99+3X3F/aK9y/49vTi70Hszurr6QTnIuGJ3Mve7+Y97xP0afZ9+xcHMhUIH24kdiivK+ku5DEmL7wnySMkI3Mhax2mFnkPDg3xDcwK3gNs/Qr6w/p9+3X39u/B6SXob+k167HrXuuo7Ljv8vIf9XX3aPtb/7EBAwLaARADPQVBB2oHPQXSApwBsQG9AvsCAwIpANP9+fs/+9D7ifxU+6v42fUK9Q73yPcT9PLtyuhW56Ho0ueA4i/dt9585b3sRPOf94n8fgfsFRQgoCOPJScqgS5QL6cs0Se4JZMn9iX7HegTig0aDrsRUA89BSL8Zfl9+/n7vPYw7hDoY+hq7Jju5uzW6evpi+3J8tD2gviF+oH9UgDmAoME9gWTBzgIVQf2BVEFKAUoBTAE9wDo/UP9Ef5P/hH+Gv2S++D5GvjQ9u317fUf9dnwpOql5ZHlyujK6Drjp9vh2Q3hlOzM9Nn1fvYR/gIMJRmtH/cgTSPdKDctgS7tK5smiyNiI+IgrRrLFGgRwA7hCj0F2gGlAIn88feS9rj04u8t7P/pgOfb5kXus/du7jLfVufH/MUGHARKAR7/+fvSAiYUTxndCDf80gITCu4GZgVZCdkGT/4N/NP9tv41AfYFfwLu8DHp+vEK+kP9xPWd5kPiLedR77j06+l52UzXLuLJ7avz+vHv68HukABcFTkeKRuqGNIddihUMUcwfifWH/sdNSHmIgQdLhMjDdkLcgs8CpsGKQCv+kT4F/aL8v/u++yH6+Lqset/7GLtIPCX83729flY/R7/DAEwBMUGDwgkCPIIjwq4CqMKJwqfCH4H2QY9BZQCw//T/Vj9Bf1x+lT2bvMw88z0K/bh9JPxye0D7PvsJO1S6hnnHeTP4JHg1+QD7LzxD/Ly8j/7+gfTExUbfBtjGbobDyP+KaspAyJSGykbLR3jG7oW2BDQDOkJ0QcoBWcAffvg+av4E/Ti78Hue++P727u8u0979HxCvUa+Dv5ZfkN/IgBmwasCTwKMAnyCOEKGg4KEEwNVQfzAwcEmAQHBKUApvur+Nz3WPhl+dT4r/Uj8u7wn/L+8/rxye3B6U7o6+m56tvmaOAe34jm9u8w8zTwXfD1+d0ITxQhF2QU4BQtHacnqymgI+cdxhwxH+sfPht5FB4Qgg6KDeEKewXOAGcAAADk+1T2n/IP8lnzG/Mg8A/t++xd8DT1Gviz90z3FvsgAaQFqAcXB9kGtAiGC9AMXQswCVUHSQZJBv8E7wEUAFv/O/4i/Mz53PeS9u31CvVl9I70l/PZ8AfulOwk7Qfu2ush5n3gpuCp5//uKO8Y7Afu0PZFBFwQbBOOD8MQTxnaIaQldyNOHj4bXxwQHs4bXxe2FOQR8Q1ICycK5gduBAwB5PtA9kj1DveG9X7xOe3O6pjuUfQr9h/1ZfQT9Mz5mwaPBar9dwOsBH8C/Q43ErQI0Qd+ByAG6QmCCfMD7wFw/yr7s/xk/oX6RPjI9zz0i/Kv9fn2gvPV7qTqrelq7MXr7+ZD4rfe8OEM6w/tMelz6xP0T/6LCMgNDg39Ds8W/x//JO4hnRw1HBwfXiGkIAAbERR9EYkSnhLpDvIIWgSxAVv/x/we+rz2MPOT8Unwbu6s7pvw+vF28pfzafYe+uj94gC9AkUESQafCI8KXQsfC8wKUQoHCSwHCwbqBAcETQO5AC797frt+mj7M/qn9snyvPEb85fz7vAD7HfojOiY6TnoCORH3wHg4+Xr6YTphOn/7mX5bgRIC34M5QxDE2MezSX/JBQglB1SINUkeiXWH3QXQxOuFQQYohS8DFEF0gIcBE0Dlf0X9lXxsPAj8rzxte1e6+bs4u/p8/n2+fYa+PD87wE9BSAG4gVVB8AJ9gqjCoIJnwgPCFUHIAb/BKADGALD/zf8R/oz+uD5n/eX84fwm/CL8iPy8u2t6Tnob+nr6Z3mz+AS3kPiY+hn6pjpIOvy8nj++gcXDJIMtw9XGMUhZiWxIRAegx9NI6wkSSFSGyEX5BayF5kVHhDpCXIGjwU5A8v+4PnZ9ZfzdvJy8MHumO7/7qTvxfCf8sT16fjY+rP8R/8YApgEsAbmB/oHtAjhCtkLPApNCCwHkwe8B48FGAK2/vD8x/zQ+6v4E/TZ8BfxdvIj8v/ukOrn57XoIOuk6lPlAeDX39fkFOqc62fq9+r68U/+8ghVDB8LdQ0VFrkgciZiI6kdnRziIHImxSZGH+wVGhOmFvwYHhUrDKADSgG1A4sDJv7t9T3vNPDE9SP3I/Ko7OLq1e5o+/D8kvaf99XzAvYoBY8K8ghlCj0FQQLmB58Now8nDwcJuQCF/4gBfwK5ALv7AvZu81H0ZfTR8VHvi+0D7Err1ukl6OfnBOdG5AXiKuDP4CnlOeg56OvpC/CW+PcAAwd6Cp8NTxR3HsEk8iPuIdohJCObJnYodyM1HBkYuhbbF18XMxDRB/MDOQO9AqUA2PqC8/bvavHy8kDxHO7v61bsXfCO9K/1NPXl9gH7Hv+UAoME1gTFBtUJNAsfCzwKbgmCCUQJageYBIgBW/8R/on89fnl9v7zI/IP8g/yh/Bu7u/rhOnK6ITp8+gM5mziKuBg4ZHlremQ6nPrcvBY+AMCEwp1DRIPjRQtHRwkLCf/JCAhAyL2JX4nZiUcH0IYARamFq4VuxGGC3sFLQL3ACb+9flU9qvzfvEL8M3v1e5u7knw0fF28lH0+fbM+dz8y/4pADkD2QZECXoKPArACaMKHwvpCWoHHARrAl4BeP4W+4L4N/d+9lH0avG47+LvQPFy8BjsJehr52PoHOlC54DiXN+d4annLexq7Grs0fH5+z0FowrlDDsPKhZaH6wkySPOIGYgiyNAJ/YlWh86GV8XnRc/FuQRhgs0BkUEYgN7AIn8RPjh9Abz5vGw8LjvpO9R7z3vcvC08ob1RPhc+tD7Jv4tAgsGvAfmBzgI6QmGC68L/gnuBigFEwXBBJQC4P7t+lD5zPkn+Rf25vHN73LwavFd8APsa+eI5sro6+mU59Pi19/84v/prO6Y7qDtVfHQ+6gHig3xDasOqhO9HV0mAicYIrUeuSCkJacn0SL0GU8UOxQqFgkVgg40BgwBUgCIAZn/efkG8/bv2fCL8jjyIPDJ7TDufvG49ND2lvj1+Tf8mf85A14GDwi0CFkJ/gn2Cq8LuAq8B/8EBwTJA6gCcP8W+5b41PgS+bz23fKw8LDwavGH8CTtFOpG6VLq4up36FvkJePv5qjsKO+D7hPvuPRk/moHFwySDAYOjRSAHWIjoCMpIJQdCB9/Ik0jHB8ZGFgTYBLHEisRAgyPBQwBrv8e/8f8q/j+8yzx7vAD8Zvw4u9R76TvQPHp85L2Evl9+6r91/9/Ao8FYQjpCWUKZQplCqMKJwq0CLAG6gRNAwwB4P4F/Wj7ovnc9zT1BvOL8p/y5vFm7xjsFOpn6ofrNet36ADlmeQ56A/tze/27+7wAva2/hcHxAt1DVAPZBS6G14h0SK5IKAetR65ILEhoB5PGQkVYBLYECcPKwyoB/sC1/+q/bv7jvmS9unzk/H27wvwA/Go8bzxvPHy8q/1efnH/B7/FACIAYME5gcTCjwKlwkHCd0IGwlNCDQGyQPaAaUA9P4u/ZL7Zflh98T1UfTA8zDzk/Fm7yTt7+tz65DqCOkt55Hl+OWM6Ifrye24793yGvjg/nsFzAoGDsMQRxXCGncemB9vH7UejB7zHhAePhvHF3AVQxOaELMNEwpeBskDXgGV/bf55fb29G7zvPEL8G7u8u2k71XxYfLV86/1RPi7+wn/9wDSAigF0QduCcAJ6QnVCcAJrAl2CIcG/wS1Ay0CpQBb/4H9aPui+cj3K/bM9KvzYfJd8DDuf+xe6+Lq/+mh6BDooegU6i3sg+4X8cz0Cvr0/hADLAcfC3kP0xM2FzoZRhopG3McgB3bHBUbOhk2F8MVERTtEMgN9goPCHsFlAKF/4n84Pmz95v1wPO08nbyD/Ko8frx8vK49Lz2lvhH+tD7v/17ACQDwQSkBUkGAwf6B8kI3QgkCEEHAwdyBlEFHAS9AnMBPQAJ/1j90Psz+tT4TPcf9VnzI/LZ8D3vJO1e65Dqe+qk6mfqPepK66Dtm/CC8+31q/hL/D0AbgTRBycKfgxlD2ASthTXFT8Wpha6FvgWDRdTFgkVWBOmEQoQRA7tC4IJFwduBNoBhf+B/bv7HvqW+A73K/YX9mn2vPYj9wb4jvmm+4H9Mv/OAMUBJAOYBOIF7gZBB1UHfgfuBgsGewWsBBADcwHX/+j9S/ya+pb4VPbp8+bxSfCs7tLsDOsx6RDo/Of85xDooeg96g/tSfAw89n1J/ls/UECxQaPCp8NXBBsEyEXoRnCGmcbfBt8G7obkBsIGscXwxXTE5IRJw8rDPIICwb7Auz/Q/1x+hr42fXA83byk/Es8Szxk/F28pfz9vQO91D5Kvsu/Qn/pQAYAncD6gTNBXIG7gbZBjQGjwXqBMkDQQKlAB7/WP2S+zv5vPZ69J/yA/Eo72LtXuvW6VrpMenf6Mrowena627uVfEn9A73XPqi/iQDLAd6CrMN7RD9E/gWERkyGsIaKRtnGz4bRhqAGLoWHhVYE+0Q8Q0KCzgIZgV/AnD/YPyi+Z/3F/aO9J/yLPHu8Czx5vEb82X0hvWz9x76Ivxk/mcA7wF3Az0FxQa8B3YItAh2CCQIvAfZBrkFmATSArkACf8u/e36gvhU9unz5vH2797tVuzO6oTpCOkI6cromOn36sntSfBZ8yv2RPgUAEoBEwU8CjwK9Q9HFaYWBBh4GVcYMhohHC0dRhqAGHwW0xM7FIkSeQ+PCmoHmASxASkAv/0K+kz34fRE87TyD/L68SzxQPF28hP02fUv+Fz6u/u//df/sQFNAxMFzQWbBqgHDwgkCKgH2QYgBj0F8wOUAuIA4P6z/IX68fft9f7zvPH27/LtA+yQ6q3pMekc6Rzpwelz67XtNPDd8pv1v/ie/KUAmAT6Bx8Lgg6mESYUUxYEGBEZthkdGt8ZERnwF5EW9BTxEq8QBg40C58ICwbSAuz/gf0/+zv5YffE9RP0RPPy8gbzgvM89B/1fvaW+IX6dfzo/cP/iAG9AgcEUQXFBmoHqAeoBywHhwbiBVEF8wMtAlIAov7w/AH7WPjZ9Sf0yfKb8JjuJO2H6wzr9+oU6v/pe+rv627u2fBZ88T16fie/HsAWgTmBzQLRA4/EegTKhbwFzoZyhkIGvQZ/BguGCEXRxVYEwERbQ4rDJcJmwaLA3sAT/5g/DP6RPhp9sz0E/SX8xvzgvMT9OH0F/Z191D5Fvvw/I3+1/8gAZQCHARmBUkGmwZJBvYFUQU9BeoEoANrArkAMv+B/ZL7zPmK90j1WfO88SDwg+777LHrDOuk6s7qSutB7KDtpO/68Xr0TPeF+uj9cwETBWEIrwurDpIRERSZFSEXVxj8GBEZvhjHF3wWcBXTE7sRow/6DDwKqAcTBVYCw/9s/T/7O/l199n1jvTV84LzWfPp8/b0F/Zh9+n4w/qz/Hj+FABKAagCHARmBUkG7gZVB0EHAwebBiAGPQXeA1YCuQAJ/y797fq/+Lz2uPTy8hfxe+/J7Wrsh+v36s7qDOux6/vsE+8D8W7zQPY7+XX8rv85A9kGUQp1DR4QYBJkFCoWiRdXGJUYQhiyFyEX7BVPFEsS4A/dDTQLtAg0BiQDewBP/tD7CvpE+JL2cfWj9Onzq/M89OH0xPXl9m349fnQ+5X9W/+lALEBJAODBI8FcgbuBsUGXgb2BT0FmASLAwMCpQAJ/y79P/sS+Q73CvVE82rxuO9F7ubsxetK6zXrSuux66jsRe6b8AbzhvVt+Lv7Cf9WAiAGlwlpDCcP5BF5FJEW2xdsGJUYqhhsGNsXzxb0FMcS2BCCDgIMbgmbBqADuQBk/iL8CvpE+H72zPSr8zDzBvNu8xP0SPWS9tz3jvl9+2z99P57ANoBTQOsBAsGFweTB9EHqAcsB8UGCwYTBfMDawK5AB7/Bf3D+un45faO9PLyfvH271ruD+0t7JzrXuuc61bsoO1m737x/vN+9v34IvyZ/yQD2Qb+CX4M/Q67Ef0TcBW6FokXshedF3QXzxaFFaoTaBFlDzcNjwr6B1EFlAIAAKr9ffui+fH3VPY09Y70E/Tp8yf0CvUC9vn2bfge+vn7gf0J/1IAnAFNA0UEUQVyBsUGFwfuBkkGzQX/BN4DvQJKAdf/Jv5L/Ef6L/hU9mX0i/JA8c3vWu5i7X/sLewY7Fbs0uze7fbvOPKj9Df3t/mz/AAAYgPZBhMK0Aw7D7sRJhSuFc8WXxeJF4kXNhd8FjIVgRNUERIP+gyPCiQIZgVrAhQA0/27+8z5Gvh+9l319vRR9Hr09vSb9af2yPc7+e36ifw7/sP/9wBBArUD6gS5BYcG7gYDBxcHmwbNBeoEyQN/AjUB1/8m/g389fnc9yv2uPQw86jxNPAT71ru3u2g7aDtte1a7o/vQPFu85v1Bvhx+kP9ZwCLA9kGwAnZC/ENcRCeEhEU9BRHFTIVyxR5FBEUnhLYEP0O+gzhCp8ISQa1A5wBmf+//fn7R/rp+Nz3I/dp9hf2fvYO97P3v/jg+dj6IvzT/TL/UgCIAagCyQPqBHsFpAW5BaQFjwXqBMkD5gLaAZAAhf8R/nX8mvqr+OX2m/Wj9JfzdvIs8XLwj+/q7lHvZu/N75vw5vHV8yv2v/g/+7/9FAD7AvYF8ghlCnoKkgzgD1QRNxLTEwUTXBAzEBYRcRCjD2UPTA3ACVUHKAVBAuz/9P4R/rP8ffsz+m345fZU9iP3bfjg+Wj7S/wa/Vj9v/2u/9oB3gPiBSwHFwdyBocGhwY0BiAGNAa5BRwEGALX/5X9DfwB+/X5gvi89o705vHN78HuHO7e7XftvexB7APs7+t/7KDtpO/y8qf2HvpD/bkAwQQPCNkLcRDoEyoWBBg6GREZqhglGU8ZQhg/FnkUiRInD5sLbglBBzAEsQHX//D8jvlh9yv24fRl9KP0zPQf9Yb1K/az93n5ffuV/R7/KQAgAUECOQPzA4ME1gSsBDAEoAPSAgMCxQGxAfcA1//0/r/9nvwi/CL80Pua+o75RPi89tn1NPXM9FH08vJ+8V3w/+6s7qzuWu5u7vbvtPLE9en4ifw9ALUDkwfEC3kPSxKiFCoWphaRFqYWIRcNFxUWZBQiElAPpwzMCrQIewWUAq7/3Pyv+gb4F/Zx9cz0uPTh9K/1vPaK91D5Afue/GT+FAAkAzkDcwHzA7AGCwb/BFEFpAUTBXcDvQIQA1YCZwCF/9f/ZP4N/Lv7nvym+3H6Hvo7+YL4yPcj9/n2QPZI9WX0n/KH8OLvh/BJ8GbvIPCf8vb0Dvcz+mz9ZwBFBGEIFwwvDrcPNxKBE2wT6BNkFBEUvxPHEsMQEg9hDV0LbgnuBkUEGAIUAL/9P/uO+cj3vPYO95/3yPdt+KL5hfq7+1j9Hv/OABgCLQKUArUDMARuBDAEHAS1A70CfwItAkoBuQDs/0f/tv6B/XX8aPvY+q/6mvoK+lD51PhY+PH3yPfI97P35faG9f7zyfL68VXxLPG88ZPxTfJI9Zb4pvvg/koBbgQPCOEKig15D3EQ7RArEaYRaBFoEeQRwxD9DkwNrwsnCvIIFwdFBAMCUgDT/bv77fq3+RL5ovkK+lz6FvtL/JX9O/4y/84AQQK9AtIC+wI5A2IDiwN3AzkDqALFAUoBDAE9AMP/hf94/oH9dfx9+yr7AfsB+8P6cfr1+Y75jvll+Tv5eflQ+Vj4I/eG9Sf0WfPd8p/yn/Ib827z4fRM96L5dfwpAGIDewUXB9UJVQxhDdQO4A8eEDMQjg+jDzMQZQ+CDt0N7QtECUEHZgVNA0oBW/9s/eT7XPpl+f34/fjg+Sr7dfwu/dP9Cf+F/wAAIAFWAk0DyQPJA3cD5gLmAjkDOQPSAkECnAHOAHD/O/5D/Uv80Pt9+xb7w/oK+sz54Pl5+aL5M/qF+lz6UPmW+Mj3p/Zp9pL2fvZA9hf27fXE9QL2yPdo+3j+KQBWAloEPQX2BSQI9gqnDLMNBg7dDXUNAgyGCysMkgxVDPYKMAlyBqADqALFAZAA1//g/oH9+fua+kf67foi/L/99P6u/9f/hf9w/ykAcwF/AqAD3gPSAtoB9wC5AEoB7wGoAn8C4gAJ/4H9YPzk+w38x/zc/ND7HvrU+Jb4EvkK+lT7YPxL/O36ovlt+Mj38fcS+R76Hvo7+UT4YfeK99T4P/u2/nMBvQKUApwB9wAYAjQGwAmjCsQLrwtZCX4HAwe0CEgLxAvtC7gKmwagAyQD5gJrAr0CoAO9AhQAgf27+z/7ifyN/hQA9wAUAE/+WP0F/f39kACoAmID5gKcAQAAMv8y/1IA2gHvAV4BZwBP/on8+fs3/In88PyV/Vj9DfyF+vX5XPoW+w388Pyz/Kb7mvrM+Sf5ZfkK+pr6w/qa+oX6r/rD+pL7WP22/gAAiAHFAdoBvQJNA5gEXgZBB7wHDwhBB0EHqAfFBgMHfgdBB4cGgwTeAwcElAKcAV4BzgCQABQA1//s/wn/ov4y/9f/KQBnAJAAIAFzAUoBNQFzAV4BxQHSAi0CIAH3AFIAMv/0/uD+T/5k/uj98Px1/JL7aPvk++T7+ftg/Df8+fuS+2j70PsN/Ev8s/ye/CL80Pt9+yr7P/um+zf8x/wa/Wz9lf2//Xj+cP97AIgBvQLeA1oEbgTBBNYEKAU0Bl4G7gbRB4cGewVRBYMEbgSsBJgEgwTJA/sC7wGlAGcAPQAUACkA7P8UAGcAAABb/3D/7P9SAPcAXgFzATUBuQApAEf/ov6i/gn/R//g/hH+WP2e/ND7kvv5+578Bf3w/GD80PvQ++T7N/zc/EP9gf0R/jv+0/0a/Yn8Gv2q/ZX9Ef62/qL+6P1D/Vj9T/5b/2cAXgGIAZwBsQGcAQMC0gKgA/8EzQW5BVEFgwQHBBwErAR7Bc0FjwWYBHcDvQJBAhgCAwJBAhgCDAE9AMP/Cf/L/uD+Hv+F/8P/7P/D//T+Ef7o/Xj+9P4e/zL/y/5P/mz9x/yz/Br9v/3o/ej9v/2//Sb+Jv6//ej9jf70/gn/y/6i/k/+v/3o/Xj+y/70/h7/W/8y/8v+y/4J/0f/hf+u/5n/rv/X/+z/AABSAPcAnAEDAlYCQQKoAiQDJAOLA/MDtQOLA6ADiwOgA3cDTQNNA/sCawLvAbEBDAFnACkAFADX/4X/cP/0/o3+ZP6N/gn/Cf+2/qL+T/79/dP96P0R/v39/f0R/v39qv2//U/+4P5b/5n/7P/s/4X/R/9H/4X/1/8AAOz/rv8y/zL/1//s/8P/FAB7ACkA7P/D/1v/Hv8e/1v/rv+F/zL/9P7L/h7/mf97AIgBnAGxAQMCxQHFAQMCawK9Ar0ClAJrAmsC2gGcAQMCQQJWAn8CVgLvATUBZwA9AD0AKQApANf/Mv+i/nj+jf62/h7/Mv8e/wn/4P5k/uj90/0m/rb+tv5P/jv+eP54/mT+9P5b/4X/1//D/1v/9P7g/gn/4P70/oX/AAAUAOz/mf+F/wAAUgApABQAFAAUAOz/hf8y/5n/AAAAAOz/7P9SAPcASgFeATUBXgFzAV4BSgFzAdoBGAIYApwBDAHiAM4A9wBKAYgB2gHvAXMBkAA9AD0AKQA9AFIAKQDs/3D/ov4R/o3+tv7g/lv/hf+Z/3D/tv4m/ib+O/5k/gn/hf9w/wn/tv62/gn/W//X/3sApQApAMP/mf8J/+D+W//X/+z/FAA9ABQAAADX/8P/7P/s/wAAAAAUANf/hf+F/67/w//s/2cA9wBKAUoBIAHiAKUAuQDiAPcA9wC5AKUApQClAM4AzgDOAPcAzgAgASABzgD3AKUAZwA9ABQAAAAAAAAAAAAAAOz/7P/s/+z/1/+Z/1v/W/+u/4X/Hv/0/sv+jf62/h7/cP9b/4X/w//s/9f/hf+F/4X/cP9w/8P/1/+u/67/1/8AACkAewC5AHsAKQApAAAA1//X/67/cP+Z/8P/1/8AAOz/7P9SAHsAZwB7AHsAKQAUAD0AKQApAD0AKQApAAAAFABSAFIAZwDOAOIAZwApABQAFAAAABQAKQApAFIAUgAAANf/1/8pAFIAPQBSAGcAKQCF/0f/hf+u/5n/rv/s/wAAAADs/+z/w/+u/+z/AADs/5n/cP+F/1v/W//X/xQAFAAAABQAZwB7AGcAZwBSABQAAAAAAAAA7P/D/9f/w//s/wAAFAAUABQAFABnAGcAZwA9AOz/1//s/8P/w/8UAFIAPQApACkAPQA9ACkAUgB7AFIAPQAUAOz/w/+u/8P/AAAUABQAPQA9AAAA7P/X/67/rv/X/wAAw/9w/3D/hf+Z/5n/w//s/wAAw/+u/8P/w/+Z/67/rv+Z/67/1//s/+z/FAA9AFIAUgA9AD0APQA9ABQAAAAAAAAAAAAAABQA7P8AABQAKQA9ABQAAAAAAAAAAAAAABQAKQApAAAAFAAUAAAA7P8UAFIAKQApAFIAKQAAAAAAPQBSAD0AUgApAAAAAADs/+z/AAApACkAPQA9AAAA7P8AABQAAAAAAAAAAADs/8P/hf+F/9f/AADX/+z/AADs/8P/rv/X/9f/1//s/67/w//X/9f/AADs/9f/AAAAAAAAAAAAAAAAAAAAAAAA7P/X/+z/AAAUABQAKQApAAAAAAAAAAAAAAApACkAAAAUABQAAADs/wAAKQBSAHsApQCQAD0AFAAAAAAAKQBSAFIAUgBSABQAAAAUABQAAAApACkAAAAAANf/rv+F/4X/rv/X/wAAw/+F/5n/mf+u/9f/7P8AAAAA1//X/67/w/8AAAAAAAAUACkAAAAUAFIAPQAUAD0APQAUACkAKQAAAOz/AAAUAD0AKQAUACkAFAAAABQAPQA9ABQAFAAAAOz/w/+u/8P/AAApAAAAAAAAAAAAAAAUABQAKQA9ABQAAADs/wAAAAAAABQAAAApAD0APQAUANf/1//X/+z/FADD/8P/AADs/8P/7P8AAMP/1//s/8P/rv/D/67/mf/X/8P/w//X/8P/1/8AAAAAFAAAAAAAAAAAAAAAAAAUAAAAAAAAAK7/1/8AAAAAAADs/wAAKQAUAAAAAAAAAAAAFAAAABQAFAAAAOz/7P8AABQAPQA9AD0APQA9AD0AKQApAD0AKQAUACkAKQApACkAFAApACkAPQAUAAAAAADs/wAAAAAAAAAA7P8AAAAA7P/s/wAA7P/s/+z/1//X/9f/1//s/9f/7P8AAAAA7P/s/+z/7P/s/wAAAADs/wAAAAAAAAAAAAAAAAAAFAAAAAAAFAAAAAAAAAAAABQAFAApABQAAADs/9f/7P8UABQAAAAAABQAFAAAABQAPQApABQAKQApABQAAAAAAAAAFAAUAAAAFAApABQAAAAAAAAA7P8AAAAA7P8AAOz/w//D/wAA7P/D/9f/7P/s/+z/1//X/+z/7P/s/9f/1/8AAAAA7P/s/wAAAAAUABQAAAApABQAAAAAAAAAAAAAAAAAAAAUABQAFAApACkAFAAUAAAAAAAAAOz/7P8UAAAA7P8AAOz/7P8AAOz/AAApACkAAADs/wAAAAAAABQAFAAUABQAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAOz/7P/s/+z/AADs/9f/1//s/wAAAAAAANf/1//s/+z/AAAAAAAA7P8AAAAAAADs/wAAFAAUACkAFAAUACkAPQApAOz/AAAAABQAAAAAAAAA7P/s/xQAKQAUABQAKQAUACkAFAAAABQAAAAAABQAFAAAAAAAAAAUAD0AUgApAAAAAAApABQA7P/s/xQAFAAAAAAAAAAAAOz/KQA9ABQAAAAAAAAAFAAAAOz/AAAAAOz/7P/s/67/rv/s/+z/7P8AAAAA1/+u/67/rv/D/8P/7P/D/9f/7P/X/wAAAADs/wAA7P8AACkAFAAUAAAAFAApAAAAAAAAACkAKQAAABQAPQAUAAAAFAAAAAAAAADs/+z/AAAAAAAAAAAAAAAAAAAUACkAKQAAAAAAAADX/+z/FAAAAOz/7P8AAAAA7P8AAAAAAAAAAAAAAAAUAAAA7P/s/9f/1//s/wAAAADs/+z/AAAAAAAAAAAAAOz/FAAUAAAAAAAAAOz/AAAAAAAAFAAUAAAAAAAUAD0AFAAAABQAAAAAACkAFAA9AFIAFAApAD0AKQAUAAAAAAAUAD0AFAAAABQAAAAAAAAAAAAAABQAKQAUAAAAAADX/67/1//s/9f/1//D/67/7P/X/9f/AADX/+z/AAAAAAAAAAAAAAAAAADs/+z/7P8AAAAAAAAAAAAAAAAAAOz/AAAAAOz/7P8AAAAA7P8AAAAAAAAAAAAAFAApAOz/FAA9ACkAAAAAABQAFAAUACkAAAAAAAAAKQAUAAAAAAAAABQAFAAUAAAAAAAAAAAA7P/s/wAAAADs/wAAAAAAAAAAAAAAAAAAAAAAABQAAADs/9f/AAAAANf/AAAAAAAAAAAUABQAFAAAAAAAAAAAAAAAAADX/67/7P8AAAAAAAAAAAAAAADs/9f/1//s/9f/7P8AAAAAAAAAAOz/AADs/xQAFAAUACkAKQApAAAA7P8AAAAAFAA9ACkAFAAAAAAAAAAAABQAAAAAAAAAAADs/wAAAAAAAAAA7P8AAAAAAAAUAAAAAAAAAOz/7P8AAAAAAAAUAAAA7P8AAAAAAAAAAAAAAAAAAAAA7P/s/wAAAAAAAAAA7P8AABQAKQAAABQAFAAAAOz/AAAUAAAAAAAAAAAAFADs/+z/AAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAFAAUABQAKQApAAAAFAApABQAFAApABQAAAAAAAAAFAAUAAAAAAAAABQAAAAAAAAAAAAAANf/1//s/+z/1/8AAOz/w//s/+z/7P8AAAAA7P/s/wAAAAAAAOz/1//D/+z/AAAAAOz/1/8AAAAAAADs/9f/AAAAAAAA7P/s/wAAAAAAAOz/AAAAAAAA1//s/wAAAAAAABQAFAAAAAAAKQApABQAFAAAABQAFAAUACkAKQAUACkAPQAUABQAKQApABQAPQApAAAAAAAAABQAFAAAAAAAAADs/wAAAAAAAAAAAAAAAOz/7P8AAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQA7P/X/wAAAAAAAAAA7P/s/+z/AAAAAOz/7P8AAAAAAADs/wAAAADs/wAAAAAAAOz/7P8AAAAAAAAAAAAA7P8AABQAAADs/wAA7P/s/wAA7P/s/+z/AADs/+z/AAAAAAAA7P/s/wAAAADs/+z/7P/s/wAAAAAAAOz/FAApAAAAFAAUAAAAAAAAABQAFAAAAAAAAAAAAAAAAAAUABQAFAAAAAAAAAAAABQAFAAAAOz/7P8UAAAAAAAUACkAAAAUACkAFAAAAAAAFAAUAAAAFAAUABQAFAApABQAFAApABQAFAAUABQAFAAUAAAAFAAAAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAOz/7P8AABQAAADs/+z/AAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAUABQA7P8AAAAA7P8AAAAA7P8AAOz/7P8AAAAAAAAAAOz/7P/s/wAAAAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/7P8AAAAA7P8AAAAA7P8AAAAA7P8AAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAOz/AAAAABQAFAAUAAAAAAAAAAAAAAAAAOz/AAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAADs/wAAAADs/wAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAUACkAFADs/wAAFAAAAAAA7P8AABQAKQApABQAAAApAD0AFAAAAAAAAAAAABQAAAAAABQAAAAAABQAFAAAAAAAAAAAAOz/7P8AAAAAAADs/9f/7P8AAAAAAADs/9f/7P8AAOz/7P8AAOz/AAAAAAAAAAAAAOz/7P8AAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANf/7P8AAAAAAAAAAAAAAAAUAAAAAAAAAAAA7P8AAAAA1//s/+z/7P/s/wAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAUABQAAAAUAAAAFAAUAOz/AAAUABQAFAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAFAAUAAAAFAAUABQAAADs/wAAAAAAAAAAAAAAABQAKQAUAAAAFAAUABQAAAAAABQAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAFADs/9f/AAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAFAAAAAAAAAAUABQAAAAAAAAAAAAAAOz/FAApAAAA7P/s/wAAAAAUABQAAAAAAAAAAAAAAAAAAADs/9f/7P8AAOz/7P8AABQAAADs/+z/AAApABQAAADs/wAAFAAAACkAFAAAAAAAFAAUAAAAAAAUAAAAAAAUAAAA7P8AAAAAAAAAAAAAAAAAABQAAAAAAAAA1//X/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAOz/AAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAFADs/wAAAAAAAAAA7P/X/+z/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAADs/wAAFAAAAAAAAAAAABQAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAABQAFAAAABQAAAAAABQAAAAAAAAAAAAAAAAAAAAUABQAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAA7P/s/xQAFAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAADs/wAAAADs/wAAAAAAAAAA7P/s/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQA7P/X/wAAAAAAAAAA7P8AAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAUAAAAAAAAAAAAAADs/+z/AAAAAOz/7P8AAAAAAAAAAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAA7P8AAAAA7P/s/+z/7P8AAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAABQAFAAAAAAAAAAUAAAAAAAAAAAAFAAAAAAAFAAAAAAAAAAUABQAAAAUABQAFAAAABQAFAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQA7P/s/wAA7P8AAAAA7P/X/8P/7P8AANf/7P/s/+z/AADs/wAAAADs/+z/AADs/9f/AADs/+z/AADs/+z/AAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAUAAAAAAAUAAAAAAAUAAAAKQApAAAAAAAAAAAAAAAUABQAAAAUACkAKQA9ABQAAAAAABQAFAAAAAAAAAAAABQAAADs/wAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAFAAAABQAKQDs/wAAFAAAAAAAAADs/wAAFAAUAAAAAAAAAAAAAAAAAAAA7P/s/wAA7P8AAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/9f/7P8AAAAAAADs/+z/AAAAAOz/7P8AAAAAAAAAAAAA1//X/wAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAA7P8AABQAAADs/xQAKQAUAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P/s/wAAAAAAAAAAAADs/wAAAAAAAOz/7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAUABQAAAAAABQAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAUAOz/7P8AAAAAFAAAAOz/7P/s/wAAFAAAAOz/AAAAAAAA7P/s/xQAAAAAAAAA7P8AAAAAAAAUAAAAAAAAAAAAAAAAAAAA7P8AABQAAAAAAAAAAAAUABQAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAA7P/s/wAAFAAAAAAAAAAAAAAA7P/s/wAAAAAAAAAA7P8AAAAAFAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P/s/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P/s/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P/s/wAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P/s/+z/AAAAAAAAAAAAAOz/AAAUAAAA7P8AAAAAAAAAANf/7P8AAAAAAADs/wAAAAAAAAAA7P8AAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAUABQAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAUAAAAAAAAABQAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAFAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAAAADs/+z/AAAAAAAAAAAAAAAAAAAAAOz/7P8AAAAA7P/s/+z/AAAAAOz/AAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAADs/+z/AAAAAAAAAAAAAAAA7P/s/wAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAOz/AAAAABQAAAAAAAAAAAAAAAAAAAAAAAAA7P8UAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAUABQAAAAAABQAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAFAAUAOz/AAAUABQAAAAAABQAAAAAABQAFAAUAAAAAAAAAAAAAAAAAAAAFAAAABQAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADX/+z/AAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAAAAAAAAAFAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAOz/7P/s/wAAAAAAAOz/7P8AAAAAAADs/9f/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAFAAUABQAAAAUABQAFAAAAAAAAAAAABQAFAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAADs/wAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/7P8AAAAAAAAAAOz/AAAAAOz/AAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAAAAAABQAAAAAABQAAAAAAAAAAAAAAAAAFAAUAAAAAAAUABQAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAA7P/s/wAAAAAAAAAAAAAAAAAAAAAAAOz/7P8AAAAAAAAAAAAAAAAAAAAAAAAAAOz/7P8AAAAA7P8AAAAA7P8UABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAFAAUAAAAAAAUABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAA7P8AAOz/7P8AAAAAAAAAAAAAAAAUAOz/1/8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAAADs/wAAAAAUAAAAAAAAAAAAAAAAAAAAAADs/+z/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAADs/wAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAOz/AAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAADs/wAAAAAAAAAAAAAUAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAOz/7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUABQAAAAAAAAAAAAAABQAAAAAAAAAAAAAABQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQAFAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs/wAAAAAAAAAAAAAAAOz/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7P8AAA==\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "index = np.random.randint(len(simu_wave), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "print(simu_label[index])\n",
    "print(simu_phoneme[index])\n",
    "ipd.Audio(simu_wave[index], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering 2000th case\n"
     ]
    }
   ],
   "source": [
    "from python_speech_features import logfbank\n",
    "\n",
    "ex_shape = logfbank(simu_wave[0], SAMPLE_RATE).shape\n",
    "mfcced_simu_wave = np.zeros(((len(simu_wave), ex_shape[0], ex_shape[1])))\n",
    "for i, wave in enumerate(simu_wave):\n",
    "    mfcc = logfbank(wave, SAMPLE_RATE)\n",
    "    \n",
    "#     mean = np.mean(mfcc, axis=0)\n",
    "#     std = np.std(mfcc, axis=0)\n",
    "#     mfcced_simu_wave[i, :, :] = (mfcc - mean) / std\n",
    "    mfcced_simu_wave[i, :, :] = mfcc\n",
    "    \n",
    "    print(f\"Transfering {i+1}th case\", end=\"\\r\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (2000, 99, 26)\n",
      "Output Shape: (2000, 7)\n",
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "5\t--->\ts\n",
      "18\t--->\teh\n",
      "11\t--->\tv\n",
      "9\t--->\tah\n",
      "3\t--->\tn\n",
      "2\t--->\t<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = preprocesser.tokenize(simu_phoneme)\n",
    "\n",
    "print(\"Input Shape: {}\".format(mfcced_simu_wave.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()\n",
    "    \n",
    "# Split the data into size (training set, testing set) (18000, 2000)\n",
    "TRAIN_SIZE = 0.9\n",
    "\n",
    "wav_tensor, wav_tensor_val, phoneme_tensor, phoneme_tensor_val = train_test_split(mfcced_simu_wave, \n",
    "                                                                                  phoneme_tensor, \n",
    "                                                                                  train_size=TRAIN_SIZE, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "VAL_WAV_SIZE = len(wav_tensor_val)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "wav_tensor = tf.convert_to_tensor(wav_tensor, dtype=tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input Shape: (4, 99, 26)\n",
      "Example Output Shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(f\"Example Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Example Output Shape: {example_target_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters, stride=None):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        \n",
    "        if stride is None:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.filters1, self.stride, padding='valid', activation=\"relu\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.filters2, kernel_size, padding='same', activation=\"relu\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv1D(self.filters3, 1, padding='valid', activation=\"relu\")\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        if self.stride != 1:\n",
    "            short_cut = tf.keras.layers.Conv1D(self.filters3, \n",
    "                                               self.stride, \n",
    "                                               padding='valid', \n",
    "                                               activation=\"relu\")(input_tensor)\n",
    "            x += short_cut\n",
    "        else:\n",
    "            x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 99, 26)\n"
     ]
    }
   ],
   "source": [
    "resnet_block = ResnetIdentityBlock(32, [1, 2, example_input_batch.shape[-1]])\n",
    "resnet_output = resnet_block(example_input_batch)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(resnet_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_identity_block\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              multiple                  27        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  4         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  66        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  8         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  78        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  104       \n",
      "=================================================================\n",
      "Total params: 287\n",
      "Trainable params: 229\n",
      "Non-trainable params: 58\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_block.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder for MFCC transformed wave data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate, \n",
    "                 squeeze_time, \n",
    "                 rnn_initial_weight=None):\n",
    "        '''\n",
    "        Args:\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size            \n",
    "            dropout_rate: layer dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units    \n",
    "        self.squeeze_time = squeeze_time\n",
    "        self.rnn_initial_weight = rnn_initial_weight\n",
    "        \n",
    "        # normalization\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # conv1d\n",
    "        conv_units = 64\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=conv_units, \n",
    "                                           kernel_size=5, \n",
    "                                           strides=1, \n",
    "                                           padding='same', \n",
    "                                           activation=\"relu\", \n",
    "                                           kernel_initializer='glorot_uniform')\n",
    "        \n",
    "        # ResNet\n",
    "#         self.resnet1 = ResnetIdentityBlock(32, filters=[conv_units//4, conv_units//2, conv_units])\n",
    "#         self.resnet2 = ResnetIdentityBlock(64, [conv_units//4, conv_units//2, conv_units])\n",
    "#         self.resnet3 = ResnetIdentityBlock(128, [conv_units//4, conv_units//2, conv_units])\n",
    "        \n",
    "        # pBLSTM1\n",
    "        self.fw_lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        self.bw_lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate, \n",
    "                                             go_backwards=True)\n",
    "        self.bn_pblstm1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # pBLSTM2\n",
    "#         self.fw_lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "#                                              return_sequences=True, \n",
    "#                                              return_state=True, \n",
    "#                                              kernel_initializer=\"lecun_normal\",\n",
    "#                                              activation='tanh', \n",
    "#                                              recurrent_activation='sigmoid', \n",
    "#                                              recurrent_initializer='orthogonal', \n",
    "#                                              dropout=dropout_rate)\n",
    "        \n",
    "#         self.bw_lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "#                                              return_sequences=True, \n",
    "#                                              return_state=True, \n",
    "#                                              kernel_initializer=\"lecun_normal\",\n",
    "#                                              activation='tanh', \n",
    "#                                              recurrent_activation='sigmoid', \n",
    "#                                              recurrent_initializer='orthogonal', \n",
    "#                                              dropout=dropout_rate, \n",
    "#                                              go_backwards=True)\n",
    "#         self.bn_pblstm2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Encoder lstm\n",
    "        self.enc_lstm = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call pyramidal LSTM neural network encoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: wave input\n",
    "        '''\n",
    "        self.layer_info[\"Input\"] = inputs.shape\n",
    "        x = self.bn(inputs)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # ResNet\n",
    "#         x = self.resnet1(x)\n",
    "#         x = self.resnet2(x)\n",
    "#         x = self.resnet3(x)\n",
    "        \n",
    "        # pBLSTM 1\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.fw_lstm1(x)\n",
    "        bw_outputs, bw_state_h, bw_state_c = self.bw_lstm1(x)\n",
    "        x = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        x = self.bn_pblstm1(x)\n",
    "        \n",
    "        # pBLSTM 2\n",
    "#         fw_outputs, fw_state_h, fw_state_c = self.fw_lstm2(x)\n",
    "#         bw_outputs, bw_state_h, bw_state_c = self.bw_lstm2(x)\n",
    "#         x = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "#         x = self.reshape_pyramidal(x)\n",
    "#         x = self.bn_pblstm2(x)\n",
    "        \n",
    "        # encoder output layer\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.enc_lstm(x)\n",
    "            \n",
    "        return fw_outputs, fw_state_h, fw_state_c\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        \n",
    "        Args:\n",
    "            outputs: outputs from LSTM\n",
    "            squeeze_time: time step one would like to squeeze in pyramidal LSTM\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "\n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * self.squeeze_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 33, 256)\n",
      "Encoder forward state h shape: (batch size, units) (4, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_DROPOUT_RATE = 0.0\n",
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, fw_sample_state_h, fw_sample_state_c = encoder(example_input_batch)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_57 (Batc multiple                  104       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           multiple                  8384      \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               multiple                  328704    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               multiple                  328704    \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc multiple                  6144      \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               multiple                  1836032   \n",
      "=================================================================\n",
      "Total params: 2,508,072\n",
      "Trainable params: 2,504,948\n",
      "Non-trainable params: 3,124\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.V = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder for output phonemes\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 target_sz, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            target_sz: target size, total phoneme size in this case\n",
    "            embedding_dim: embedding dimension\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size\n",
    "            dropout_rate: dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.target_sz = target_sz\n",
    "        self.lstm_units = lstm_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        \n",
    "        # attention model\n",
    "        self.attention = LuongAttention(lstm_units)\n",
    "        \n",
    "        # decoder rnn            \n",
    "        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "        \n",
    "        self.lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "\n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(32, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.resnet2 = ResnetIdentityBlock(64, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet2_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.resnet3 = ResnetIdentityBlock(128, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet3_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "\n",
    "#         self.conv1 = tf.keras.layers.Conv1D(64, 13, padding=\"same\", activation=\"relu\")\n",
    "#         self.conv2 = tf.keras.layers.Conv1D(128, 11, padding=\"same\", activation=\"relu\")\n",
    "#         self.conv3 = tf.keras.layers.Conv1D(256, 9, padding=\"same\", activation=\"relu\")\n",
    "    \n",
    "        # Fully-connected\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(target_sz, activation=\"softmax\")\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        call LSTM decoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: target output, following phoneme for wave data input in this case\n",
    "            enc_hidden_h: encoder hidden state h\n",
    "            enc_hidden_c: encoder hidden state c\n",
    "            enc_output: encoder outputs\n",
    "        '''\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the 2-layer LSTM (Decoder)\n",
    "        outputs, state_h, state_c = self.lstm1(x)\n",
    "        outputs, state_h, state_c = self.lstm2(outputs)\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet1(outputs)\n",
    "        x = self.resnet1_dropout(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x = self.resnet2_dropout(x)\n",
    "        \n",
    "        x = self.resnet3(x)\n",
    "        x = self.resnet3_dropout(x)\n",
    "\n",
    "        # Convolution\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "    \n",
    "        # dense layer before final predict output dense layer\n",
    "        x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        # output shape == (batch_size, phoneme size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, phoneme size) (4, 22)\n"
     ]
    }
   ],
   "source": [
    "DECODER_DROPOUT_RATE = 0.0\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, phoneme size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  2816      \n",
      "_________________________________________________________________\n",
      "luong_attention_4 (LuongAtte multiple                  131841    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               multiple                  656384    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               multiple                  525312    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_16 (Re multiple                  313536    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_17 (Re multiple                  575680    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_18 (Re multiple                  1099968   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             multiple                  1430      \n",
      "=================================================================\n",
      "Total params: 3,323,415\n",
      "Trainable params: 3,320,727\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "'''\n",
    "Candidate optimizer:\n",
    "    1. Adam\n",
    "    2. Nadam\n",
    "    \n",
    "    the preformence of other optimizers are not good\n",
    "'''\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "# optimizer = tf.keras.optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "# optimizer = tf.keras.optimizers.Adadelta(learning_rate=LEARNING_RATE)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3)\n",
    "\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)  # clipping for avoiding gradient explosion\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "\n",
    "#     translate(test_wave, sample_output.shape[1], sample_decoder_output.shape[1], phoneme_tokenizer)\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  Batch     Loss      Time(s)\n",
      "1/100      0    1.4178      0.4410\n",
      "1/100    100    1.5079     42.0200\n",
      "1/100    200    1.5246     87.5831\n",
      "1/100    300    1.5250    130.6851\n",
      "1/100    400    1.5445    172.5231\n",
      "\n",
      "================================\n",
      "Epoch 1 Loss 1.6307\n",
      "Time taken for epoch 1 -- 3.2359 min\n",
      "Total Time taken -- 3.2359 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "2/100      0    1.6472      0.4260\n",
      "2/100    100    1.4120     39.6440\n",
      "2/100    200    1.5608     79.5824\n",
      "2/100    300    1.3746    118.7664\n",
      "2/100    400    1.5784    158.9347\n",
      "\n",
      "================================\n",
      "Epoch 2 Loss 1.6284\n",
      "Time taken for epoch 2 -- 2.9902 min\n",
      "Total Time taken -- 6.2261 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "3/100      0    1.3333      0.3970\n",
      "3/100    100    1.4480     39.2590\n",
      "3/100    200    1.2893     81.4795\n",
      "3/100    300    1.6457    121.5725\n",
      "3/100    400    1.4895    162.5238\n",
      "\n",
      "================================\n",
      "Epoch 3 Loss 1.6258\n",
      "Time taken for epoch 3 -- 3.0782 min\n",
      "Total Time taken -- 9.3043 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "4/100      0    1.6368      0.3990\n",
      "4/100    100    1.9757     41.1820\n",
      "4/100    200    1.8180     80.1890\n",
      "4/100    300    1.4915    119.3510\n",
      "4/100    400    1.7827    160.8805\n",
      "\n",
      "================================\n",
      "Epoch 4 Loss 1.6237\n",
      "Time taken for epoch 4 -- 3.0401 min\n",
      "Total Time taken -- 12.3444 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "5/100      0    1.5939      0.4160\n",
      "5/100    100    1.6240     43.4785\n",
      "5/100    200    1.8967     85.5623\n",
      "5/100    300    1.9886    125.4544\n",
      "5/100    400    1.7466    168.1190\n",
      "\n",
      "================================\n",
      "Epoch 5 Loss 1.6230\n",
      "Time taken for epoch 5 -- 3.1762 min\n",
      "Total Time taken -- 15.5206 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "6/100      0    1.8282      0.4390\n",
      "6/100    100    1.5685     42.2925\n",
      "6/100    200    1.6548     82.3015\n",
      "6/100    300    1.7491    121.9845\n",
      "6/100    400    1.5259    161.7685\n",
      "\n",
      "================================\n",
      "Epoch 6 Loss 1.6200\n",
      "Time taken for epoch 6 -- 3.0439 min\n",
      "Total Time taken -- 18.5645 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "7/100      0    1.5653      0.5130\n",
      "7/100    100    1.5578     46.0106\n",
      "7/100    200    1.9191     90.3214\n",
      "7/100    300    1.4898    131.9101\n",
      "7/100    400    1.3316    173.7429\n",
      "\n",
      "================================\n",
      "Epoch 7 Loss 1.6201\n",
      "Time taken for epoch 7 -- 3.2669 min\n",
      "Total Time taken -- 21.8314 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "8/100      0    1.9433      0.4500\n",
      "8/100    100    1.6436     43.7914\n",
      "8/100    200    1.5319     89.5342\n",
      "8/100    300    1.6304    132.8363\n",
      "8/100    400    1.5224    176.9470\n",
      "\n",
      "================================\n",
      "Epoch 8 Loss 1.6165\n",
      "Time taken for epoch 8 -- 3.3084 min\n",
      "Total Time taken -- 25.1398 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "9/100      0    1.8755      0.4120\n",
      "9/100    100    2.0196     42.5245\n",
      "9/100    200    1.7846     81.3345\n",
      "9/100    300    1.4456    119.2415\n",
      "9/100    400    1.6416    158.4715\n",
      "\n",
      "================================\n",
      "Epoch 9 Loss 1.6144\n",
      "Time taken for epoch 9 -- 2.9696 min\n",
      "Total Time taken -- 28.1095 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "10/100      0    1.8536      0.4330\n",
      "10/100    100    1.4504     43.4313\n",
      "10/100    200    1.3557     84.6473\n",
      "10/100    300    1.5580    125.5332\n",
      "10/100    400    1.6007    166.9162\n",
      "\n",
      "================================\n",
      "Epoch 10 Loss 1.6147\n",
      "Time taken for epoch 10 -- 3.1841 min\n",
      "Total Time taken -- 31.2936 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "11/100      0    1.7477      0.4370\n",
      "11/100    100    1.4442     41.4020\n",
      "11/100    200    1.5585     81.7870\n",
      "11/100    300    1.8260    124.6360\n",
      "11/100    400    1.5585    168.9424\n",
      "\n",
      "================================\n",
      "Epoch 11 Loss 1.6147\n",
      "Time taken for epoch 11 -- 3.1874 min\n",
      "Total Time taken -- 34.4810 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "12/100      0    1.4099      0.4340\n",
      "12/100    100    1.7895     44.6070\n",
      "12/100    200    1.3804     87.3205\n",
      "12/100    300    1.3006    128.6931\n",
      "12/100    400    1.7479    167.7786\n",
      "\n",
      "================================\n",
      "Epoch 12 Loss 1.6125\n",
      "Time taken for epoch 12 -- 3.1239 min\n",
      "Total Time taken -- 37.6049 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "13/100      0    1.6354      0.4100\n",
      "13/100    100    1.7160     40.3300\n",
      "13/100    200    1.6356     82.7046\n",
      "13/100    300    1.3309    126.9176\n",
      "13/100    400    1.7481    167.4806\n",
      "\n",
      "================================\n",
      "Epoch 13 Loss 1.6125\n",
      "Time taken for epoch 13 -- 3.1569 min\n",
      "Total Time taken -- 40.7618 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "14/100      0    1.8139      0.4090\n",
      "14/100    100    1.7205     38.6750\n",
      "14/100    200    1.2901     78.2590\n",
      "14/100    300    1.5214    116.5440\n",
      "14/100    400    1.5216    156.2090\n",
      "\n",
      "================================\n",
      "Epoch 14 Loss 1.6114\n",
      "Time taken for epoch 14 -- 2.9516 min\n",
      "Total Time taken -- 43.7134 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "15/100      0    1.5224      0.4100\n",
      "15/100    100    1.7883     43.3953\n",
      "15/100    200    1.4107     85.6453\n",
      "15/100    300    1.5577    126.0513\n",
      "15/100    400    1.4865    166.7733\n",
      "\n",
      "================================\n",
      "Epoch 15 Loss 1.6107\n",
      "Time taken for epoch 15 -- 3.1268 min\n",
      "Total Time taken -- 46.8402 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "16/100      0    1.4980      0.4410\n",
      "16/100    100    1.8473     41.1871\n",
      "16/100    200    1.2898     81.8936\n",
      "16/100    300    1.5731    121.2866\n",
      "16/100    400    1.7041    161.9606\n",
      "\n",
      "================================\n",
      "Epoch 16 Loss 1.6114\n",
      "Time taken for epoch 16 -- 3.0416 min\n",
      "Total Time taken -- 49.8818 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "17/100      0    1.7154      0.4310\n",
      "17/100    100    1.4800     41.0521\n",
      "17/100    200    1.8642     82.9297\n",
      "17/100    300    1.5577    125.8907\n",
      "17/100    400    1.5306    167.8762\n",
      "\n",
      "================================\n",
      "Epoch 17 Loss 1.6099\n",
      "Time taken for epoch 17 -- 3.1397 min\n",
      "Total Time taken -- 53.0215 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "18/100      0    1.6701      0.4350\n",
      "18/100    100    1.4451     43.2335\n",
      "18/100    200    1.8605     83.0621\n",
      "18/100    300    1.6690    124.8782\n",
      "18/100    400    2.0494    166.4273\n",
      "\n",
      "================================\n",
      "Epoch 18 Loss 1.6099\n",
      "Time taken for epoch 18 -- 3.1343 min\n",
      "Total Time taken -- 56.1558 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "19/100      0    1.7536      0.4330\n",
      "19/100    100    1.4496     40.9630\n",
      "19/100    200    1.5574     81.0350\n",
      "19/100    300    2.0237    122.3190\n",
      "19/100    400    1.7501    163.5450\n",
      "\n",
      "================================\n",
      "Epoch 19 Loss 1.6101\n",
      "Time taken for epoch 19 -- 3.1037 min\n",
      "Total Time taken -- 59.2595 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "20/100      0    1.5501      0.5280\n",
      "20/100    100    1.4795     43.2140\n",
      "20/100    200    1.7473     86.6420\n",
      "20/100    300    1.3680    127.9400\n",
      "20/100    400    1.6358    170.1805\n",
      "\n",
      "================================\n",
      "Epoch 20 Loss 1.6098\n",
      "Time taken for epoch 20 -- 3.2030 min\n",
      "Total Time taken -- 62.4625 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "21/100      0    1.6348      0.4240\n",
      "21/100    100    1.5583     41.8310\n",
      "21/100    200    1.4458     83.2239\n",
      "21/100    300    1.7466    125.5288\n",
      "21/100    400    1.3672    169.1238\n",
      "\n",
      "================================\n",
      "Epoch 21 Loss 1.6097\n",
      "Time taken for epoch 21 -- 3.1824 min\n",
      "Total Time taken -- 65.6450 min\n",
      "================================\n",
      "\n",
      "Epoch  Batch     Loss      Time(s)\n",
      "22/100      0    1.6349      0.4590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-f298a4b678ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEP_PER_EPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoneme_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-a4b9d05864b0>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, targ_tokenizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m# passing enc_output to the decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdec_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-4150bed17331>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, enc_hidden_h, enc_hidden_c, enc_output)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet1_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet2_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-3b0f1942d325>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_tensor, training)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(features, name)\u001b[0m\n\u001b[0;32m  10928\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m  10929\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10930\u001b[1;33m         name, _ctx._post_execution_callbacks, features)\n\u001b[0m\u001b[0;32m  10931\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10932\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run eagerly will make tensorflow run step by step or else it will raise\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "# similar to Pytorch, which is a dynamic graph for deep learning\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 100\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(\"{:>5}  {:>5}  {:>8}  {:>10}\".format(\"Epoch\", \" Batch\", \"Loss \", \"  Time(s)\"))\n",
    "    \n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(STEP_PER_EPOCH)):\n",
    "        batch_loss = train_step(inp, targ, phoneme_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "            \n",
    "        if batch % 100 == 0:\n",
    "            print('{:<5}  {:>5}  {:>8}  {:>10}'.format(\"{}/{}\".format(epoch, EPOCHS), \n",
    "                                                       batch, \n",
    "                                                       \"{:.4f}\".format(batch_loss.numpy()), \n",
    "                                                       \"{:.4f}\".format(time.time() - epoch_start)))\n",
    "\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(\"\\n================================\")\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch, total_loss / STEP_PER_EPOCH))\n",
    "    print('Time taken for epoch {} -- {:.4f} min'.format(epoch, \n",
    "                                                         (time.time() - epoch_start)/60))\n",
    "    print('Total Time taken -- {:.4f} min'.format((time.time() - start)/60))\n",
    "    print(\"================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, max_input_len, max_output_len, tokenizer=None):\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    \n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    \n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    \n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            inputs=dec_input, \n",
    "            enc_hidden_h=dec_hidden_h, \n",
    "            enc_hidden_c=dec_input, \n",
    "            enc_output=enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    # ax.set_xticklabels([''] + input_wav, fontdict=fontdict, rotation=90)\n",
    "#     ax.set_xticklabels(range(len(input_wav)))\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "\n",
    "    # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "\n",
    "    print(f'Original Input Length: {len(wave)}')\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1cba6ff8b00>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [25]\n",
      "Original Input Length: 99\n",
      "Predicted translation: f s f s f s f s f s f s f s f s f s f s f s \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "3\t--->\tn\n",
      "6\t--->\tay\n",
      "3\t--->\tn\n",
      "2\t--->\t<end>\n"
     ]
    }
   ],
   "source": [
    "# testing with test wave data\n",
    "index = np.random.randint(len(wav_tensor_val), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "test_wave = tf.convert_to_tensor(wav_tensor_val[0], dtype=tf.float32)\n",
    "translate(test_wave, sample_output.shape[1], sample_decoder_output.shape[1], phoneme_tokenizer)\n",
    "\n",
    "test_phoneme = phoneme_tensor_val[0]\n",
    "preprocesser.show_convert(test_phoneme, phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
