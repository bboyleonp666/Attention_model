{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav(PATH, LABELS=None, SAMPLE=None, default_rate=8000):\n",
    "    if LABELS is None:\n",
    "        LABELS = [f for f in os.listdir(os.getcwd()) if os.path.isdir(os.getcwd() + \"\\\\\" + f)]\n",
    "        \n",
    "    label_len = len(LABELS)\n",
    "\n",
    "    # initialize the output array\n",
    "    wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "    loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "    print(\"LABEL\\tTOTAL\\tREAD\\t<1s COUNT\")\n",
    "    print(\"-----\\t-----\\t----\\t---------\")\n",
    "    for i, label in enumerate(LABELS):\n",
    "        files = os.listdir(os.path.join(PATH, label))                                 # list all the files\n",
    "        waves = [f for f in files if f.endswith('.wav')]                              # get wave files\n",
    "        wave_len = len(waves)                                                         # get number of wave files\n",
    "        wave_count[i] = wave_len\n",
    "        \n",
    "        if SAMPLE is not None:\n",
    "            waves = [waves[sample] for sample in np.random.randint(wave_len, size=SAMPLE)]\n",
    "            wave_len = SAMPLE\n",
    "\n",
    "        # initialize the temp output array\n",
    "        tmp_wavData = np.zeros((wave_len, default_rate))\n",
    "        tmp_wavLabels = np.zeros(wave_len)\n",
    "\n",
    "        less_than_1s_count = 0\n",
    "        for j, wav in enumerate(waves):\n",
    "            path = os.path.join(PATH, label, wav)                                     # get path for each wave file\n",
    "            samples, sample_rate = librosa.load(path, sr=default_rate)                # read file by librosa\n",
    "\n",
    "            # padding the wave files\n",
    "            if len(samples) < sample_rate:\n",
    "                less_than_1s_count += 1                                               # count of files less than 1 second\n",
    "                white_noise = np.random.normal(0, 0.02, sample_rate-len(samples))     # generate white noise for padding\n",
    "                samples = np.concatenate((samples, white_noise), axis=None)           # padding the files that is less than 1s\n",
    "\n",
    "            # output as np.array\n",
    "            tmp_wavData[j, :] = samples                                               # temporary wave data\n",
    "            tmp_wavLabels[j] = i                                                      # temporary wave labels\n",
    "\n",
    "            # print the outcome every ten iterations\n",
    "            if j+1 == wave_len:\n",
    "                print(\"{}\\t{}\\t{}\\t{}\".format(label, wave_count[i], j+1, less_than_1s_count), end=\"\\n\")\n",
    "                loss_count[i] = less_than_1s_count\n",
    "            elif j % 10 == 9:\n",
    "                print(\"{}\\t{}\\t{}\\t{}\".format(label, wave_count[i], j+1, less_than_1s_count), end=\"\\r\")\n",
    "\n",
    "        if i == 0:\n",
    "            wavData = tmp_wavData\n",
    "            wavLabels = tmp_wavLabels\n",
    "        else:\n",
    "            wavData = np.concatenate((wavData, tmp_wavData), axis=0)                  # concatenate info of wave files\n",
    "            wavLabels = np.concatenate((wavLabels, tmp_wavLabels), axis=None)         # concatenate following labels\n",
    "\n",
    "    print()\n",
    "    print(\"MISSION COMPLETE!!!\")\n",
    "    \n",
    "    return wavData, wavLabels, wave_count.astype(np.int32), loss_count.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phoneme_path = os.path.join(os.path.dirname(os.getcwd()), \"Phonemes\")\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "wav_array, label_array, total, loss = read_wav(train_audio_path, phoneme_dataframe.words, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check if there is any NaN or Inf number exist so that we can avoid problems while training\")\n",
    "print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(waves, labels, phonemes, create_size, min_sz=6, max_sz=10, padding=True):\n",
    "    bind_size = np.random.randint(low=min_sz, high=max_sz+1, size=create_size)\n",
    "\n",
    "    wav_simu = np.zeros(create_size, dtype=np.object)\n",
    "    phone_simu = np.zeros(create_size, dtype=np.object)\n",
    "    label_simu = np.zeros(create_size, dtype=np.object)\n",
    "\n",
    "    for count, b_sz in enumerate(bind_size):\n",
    "        index = np.random.randint(len(waves), size=b_sz)\n",
    "        \n",
    "        wav_simu[count] = np.array([waves[i] for i in index]).flatten()\n",
    "        if padding:\n",
    "            # padding white noise\n",
    "            pad_sz = (max_sz - b_sz)*waves.shape[1]\n",
    "            white_noise = np.random.normal(0, 0.02, size=pad_sz)\n",
    "            wav_simu[count] = np.concatenate((wav_simu[count], white_noise), axis=None)\n",
    "        \n",
    "        label_simu[count] = [int(labels[i]) for i in index]\n",
    "        phone_simu[count] = \"<start> \" + \" \".join([phonemes[i] for i in label_simu[count]]) + \" <end>\"\n",
    "        \n",
    "\n",
    "        if count % 10 == 9:\n",
    "            print(f\"Simulating {count+1}th wave \", end=\"\\r\")\n",
    "    print(\"\\n\\nSIMULATION COMPLETE!!!\")\n",
    "    \n",
    "    # the output simulated wave data(which is wav_simu) will be a object numpy array with shape=(create_size, )\n",
    "    # which is not a 2d array and cannot be put into tf.convert_to_tensor directly\n",
    "    # to do this in case one would like to apply spatial pyramid pooling instead of padding white noise\n",
    "    return wav_simu, label_simu, phone_simu\n",
    "\n",
    "\n",
    "def tokenize(phone):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(phone)\n",
    "    tensor = tokenizer.texts_to_sequences(phone)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "def convert(tokenizer, tensor):\n",
    "    print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "    print(\"-----------------------\")\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CREATE_SIZE = 5000\n",
    "phonemes_array = phoneme_dataframe.phonemes.values\n",
    "simu_wave, simu_label, simu_phoneme = create_dataset(wav_array, label_array, phonemes_array, CREATE_SIZE)\n",
    "simu_wave = np.vstack(simu_wave)\n",
    "\n",
    "print(f\"\\nExample Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = tokenize(simu_phoneme)\n",
    "wav_tensor = tf.convert_to_tensor(simu_wave, dtype=tf.float32)\n",
    "\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "print(\"Input Shape: {}\".format(wav_tensor.shape))\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    convert(phoneme_tokenizer, tensor)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "RNN_UNITS = 512\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input Shape: (4, 80000)\n",
      "Original Output Shape: (4, 44)\n",
      "Reshaped Input Shape: (4, 80000, 1)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "print(f\"Original Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Original Output Shape: {example_target_batch.shape}\")\n",
    "\n",
    "example_input_batch = tf.expand_dims(example_input_batch, 2)\n",
    "print(f\"Reshaped Input Shape: {example_input_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, input_sz, encoder_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.encoder_units = encoder_units\n",
    "        self.gru = tf.keras.layers.GRU(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, feat_extract=\"conv\", hidden=None):\n",
    "        if feat_extract == \"conv\":\n",
    "            conv = self.conv_feature(x)\n",
    "            output, state = self.gru(conv, initial_state=hidden)\n",
    "            \n",
    "        elif feat_extract == \"same\":\n",
    "            output, state = self.gru(x, initial_state=hidden)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    def conv_feature(self, x):\n",
    "        ## 1st Conv1D layer\n",
    "        self.conv1_size = 32\n",
    "        self.conv2_size = 64\n",
    "        \n",
    "        conv = tf.keras.layers.Conv1D(self.conv1_size, 15, padding='valid', activation='relu', strides=10)(x)\n",
    "        conv = tf.keras.layers.MaxPooling1D(3)(conv)\n",
    "        conv = tf.keras.layers.Dropout(0.2)(conv)\n",
    "\n",
    "        ## 2nd Conv1D layer\n",
    "        conv = tf.keras.layers.Conv1D(self.conv2_size, 13, padding='valid', activation='relu', strides=10)(conv)\n",
    "        conv = tf.keras.layers.MaxPooling1D(3)(conv)\n",
    "        # conv = tf.keras.layers.Dropout(0.2)(conv)\n",
    "        \n",
    "        # print(f\"Convolution Output Shape: (batch size, feature, filters) {conv.shape}\\n\")\n",
    "        # return tf.keras.layers.TimeDistributed(conv)\n",
    "        return conv\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 88, 512)\n",
      "Encoder hidden state shape: (batch size, units) (4, 512)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(WAV_SIZE, RNN_UNITS, BATCH_SIZE)\n",
    "\n",
    "# If set the batch size greater than 8, memory of GPU will run out\n",
    "sample_output, sample_hidden = encoder(example_input_batch, feat_extract=\"conv\")\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 512)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 88, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, target_sz, embedding_dim, decoder_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.decoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(target_sz)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.decoder_units)\n",
    "\n",
    "    def call(self, x, hidden, encoder_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (4, 22)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(PHONEME_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, feat_extract=\"conv\")\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.9388\n",
      "Epoch 1 Batch 100 Loss 0.4954\n",
      "Epoch 1 Batch 200 Loss 0.4326\n",
      "Epoch 1 Batch 300 Loss 0.5127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-fca1d20a11ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEP_PER_EPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoneme_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-e2d728f59596>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, targ_tokenizer, enc_hidden)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# using teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-651c1bbc2382>\u001b[0m in \u001b[0;36mloss_function\u001b[1;34m(real, pred)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    126\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m--> 128\u001b[1;33m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[1;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0minput_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     weighted_losses = tf_losses_utils.scale_losses_by_sample_weight(\n\u001b[1;32m--> 107\u001b[1;33m         losses, sample_weight)\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\losses\\util.py\u001b[0m in \u001b[0;36mscale_losses_by_sample_weight\u001b[1;34m(losses, sample_weight)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;31m# Broadcast weights if possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m   \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights_broadcast_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\weights_broadcast_ops.py\u001b[0m in \u001b[0;36mbroadcast_weights\u001b[1;34m(weights, values)\u001b[0m\n\u001b[0;32m    165\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massert_broadcastable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m       return math_ops.multiply(\n\u001b[0;32m    169\u001b[0m           weights, array_ops.ones_like(values), name=scope)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\weights_broadcast_ops.py\u001b[0m in \u001b[0;36massert_broadcastable\u001b[1;34m(weights, values)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mweights_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m       \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0mweights_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m       \u001b[0mweights_rank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rank\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mweights_rank_static\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m   \"\"\"\n\u001b[1;32m--> 458\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   8959\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   8960\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Shape\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8961\u001b[1;33m         name, _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[0;32m   8962\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8963\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the line below is a debugger which will make tensorflow run step by step\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # enc_hidden = encoder.initialize_hidden_state()\n",
    "    enc_hidden = None\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(STEP_PER_EPOCH)):\n",
    "        inp = tf.expand_dims(inp, 2)\n",
    "        batch_loss = train_step(inp, targ, phoneme_tokenizer, enc_hidden)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / STEP_PER_EPOCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(wave, max_input_len, max_output_len, tokenizer=None):\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    \n",
    "    inputs = tf.expand_dims(wave, 0)\n",
    "    inputs = tf.expand_dims(inputs, 2)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "\n",
    "    # hidden = [tf.zeros((1, units))]\n",
    "    hidden = None\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden=hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, wave, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, wave, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    # ax.set_xticklabels([''] + input_wav, fontdict=fontdict, rotation=90)\n",
    "#     ax.set_xticklabels(range(len(input_wav)))\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "\n",
    "    # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, _, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "\n",
    "    print(f'Original Input Length: {len(wave)}')\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2a506a4b6d8>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\t<1s COUNT\n",
      "-----\t-----\t----\t---------\n",
      "testing\t4\t4\t0\n",
      "\n",
      "MISSION COMPLETE!!!\n"
     ]
    }
   ],
   "source": [
    "testing, testing_label, total, loss = read_wav(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input Length: 8000\n",
      "Predicted translation: f ao r ow s eh v ah n ay v ah n ay v ah n ay v ah n ay \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(testing[0], 6, sample_decoder_output.shape[1], phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
