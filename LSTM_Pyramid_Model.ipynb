{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.0.0\n",
      "\n",
      "GPU available: True\n",
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reference: \n",
    "    https://arxiv.org/abs/1508.01211\n",
    "    https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    https://github.com/jameslyons/python_speech_features\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "    \n",
    "data source: \n",
    "    https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"CUDA enabled: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveReader:\n",
    "    def __init__(self, path, sample_rate, padding_type, read_size):\n",
    "        '''\n",
    "        Args:\n",
    "            path: train path containing directory which one would like to load\n",
    "            sample_rate: sample rate for reading .wav file\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "            read_size: size that one would like to read\n",
    "        '''\n",
    "        \n",
    "        self.path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.padding_type = padding_type\n",
    "        self.read_size = read_size\n",
    "\n",
    "    def read(self, labels=None):\n",
    "        '''\n",
    "        read all the data under the labels(directories) one select\n",
    "        \n",
    "        Args:\n",
    "            labels: labels(directories) one would like to load\n",
    "                    None means read all the directories under that directory\n",
    "        '''\n",
    "        print(\"LABEL\\tTOTAL\\tREAD\\tSAVED\\t<1s COUNT\")\n",
    "        print(\"-----\\t-----\\t----\\t-----\\t---------\")\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f for f in os.listdir(path) if os.path.isdir(path + \"\\\\\" + f)]\n",
    "            \n",
    "        elif type(labels) == str:\n",
    "            samples, total_wave_count, total_wave_read, total_loss_count = self.read_dir(dir_name=labels)\n",
    "            sample_labels = np.repeat(labels, total_wave_read)\n",
    "            \n",
    "            print(\"\\n\\nMISSION COMPELTE!!!\")\n",
    "            return samples, sample_labels, total_wave_count, total_loss_count\n",
    "                    \n",
    "        label_len = len(labels)\n",
    "        total_wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "        total_wave_read = np.zeros(label_len, dtype=np.int32)\n",
    "        total_loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for i, lab in enumerate(labels):\n",
    "            samp, total_wave_count[i], total_wave_read[i], total_loss_count[i] = self.read_dir(dir_name=lab)\n",
    "            \n",
    "            if i == 0:\n",
    "                samples = samp\n",
    "                sample_labels = np.repeat(lab, total_wave_read[i])\n",
    "            else:\n",
    "                samples = np.concatenate((samples, samp), axis=0)\n",
    "                sample_labels = np.concatenate((sample_labels, np.repeat(lab, total_wave_read[i])), axis=None)\n",
    "        \n",
    "        print(\"\\n\\nMISSION COMPELTE!!!\")\n",
    "        return samples, sample_labels, total_wave_count, total_loss_count\n",
    "    \n",
    "    def read_dir(self, dir_name):\n",
    "        '''\n",
    "        read one directory of given directory name\n",
    "        \n",
    "        Args:\n",
    "            dir_name: directory name\n",
    "        '''\n",
    "        dir_path = os.path.join(self.path, dir_name)\n",
    "        wave_files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n",
    "        total_wave_files = len(wave_files)\n",
    "\n",
    "        if self.read_size is not None:\n",
    "            wave_files_read = self.read_size\n",
    "        else:\n",
    "            wave_files_read = total_wave_files\n",
    "\n",
    "        samples = np.zeros((wave_files_read, self.sample_rate))\n",
    "        less_than_1s_count = 0\n",
    "        num_of_file_read = 0\n",
    "        for i, wav_file in enumerate(wave_files):\n",
    "            wave_file_path = os.path.join(dir_path, wav_file)\n",
    "            samp, _ = librosa.load(wave_file_path, sr=self.sample_rate)\n",
    "\n",
    "            pad_size = self.sample_rate - len(samp)\n",
    "            if pad_size > 0:\n",
    "                less_than_1s_count += 1\n",
    "                if self.padding_type is None:\n",
    "                    # None: than skip this wave file\n",
    "                    continue\n",
    "\n",
    "                elif self.padding_type == \"white_noise\":\n",
    "                    # white_noise: pad white noise data behind\n",
    "                    padding = np.random.normal(0, 0.02, pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # zero: pad zeros behind\n",
    "                    padding = np.zeros(pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "            else:\n",
    "                samples[num_of_file_read, :] = samp\n",
    "                num_of_file_read += 1\n",
    "\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(dir_name, \n",
    "                                              total_wave_files, \n",
    "                                              i+1, \n",
    "                                              num_of_file_read, \n",
    "                                              less_than_1s_count), end=\"\\r\")\n",
    "            \n",
    "            if num_of_file_read == wave_files_read:\n",
    "                break\n",
    "                \n",
    "        print()\n",
    "\n",
    "        return samples, total_wave_files, wave_files_read, less_than_1s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\tSAVED\t<1s COUNT\n",
      "-----\t-----\t----\t-----\t---------\n",
      "zero\t2376\t2160\t2000\t160\n",
      "one\t2370\t2262\t2000\t262\n",
      "two\t2373\t2222\t2000\t222\n",
      "three\t2356\t2207\t2000\t207\n",
      "four\t2372\t2201\t2000\t201\n",
      "five\t2357\t2185\t2000\t185\n",
      "six\t2369\t2164\t2000\t164\n",
      "seven\t2377\t2194\t2000\t194\n",
      "eight\t2352\t2232\t2000\t232\n",
      "nine\t2364\t2178\t2000\t178\n",
      "\n",
      "\n",
      "MISSION COMPELTE!!!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "reader = WaveReader(path=train_audio_path, \n",
    "                    sample_rate=SAMPLE_RATE, \n",
    "                    padding_type=None, \n",
    "                    read_size=2000)\n",
    "\n",
    "wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words)\n",
    "# wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words[0])\n",
    "\n",
    "# print(\"\\nCheck the existence of NaN and Inf\")\n",
    "# print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "# print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, create_size, min_sz=6, max_sz=8, padding_type=\"zero\"):\n",
    "        '''\n",
    "        Args:\n",
    "            create_size: size of binding wave one would like to create\n",
    "            min_sz: minimum size of wave data\n",
    "            max_sz: maximum size of wave data\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "        '''\n",
    "        self.create_size = create_size\n",
    "        self.min_sz = min_sz\n",
    "        self.max_sz = max_sz\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "    def simulate_wave(self, waves):\n",
    "        '''\n",
    "        method for simulating wave inputs\n",
    "        which will concatenate audio inputs for building longer audio dataset\n",
    "        \n",
    "        Args:\n",
    "            waves: input wave data\n",
    "        '''\n",
    "        # get picker for combining waves and labels(phonemes)\n",
    "        self.wave_shape = waves.shape\n",
    "        self.pickers = self.get_picker()\n",
    "        \n",
    "        print(\"Wave Data Simulation ... \", end=\"\")\n",
    "        \n",
    "        binded_length = self.wave_shape[1]*self.max_sz\n",
    "        simu_wave = np.zeros((self.create_size, binded_length))\n",
    "        \n",
    "        \n",
    "        for i, picker in enumerate(self.pickers):        \n",
    "            tmp_simu_wave = np.array([waves[p] for p in picker]).flatten()\n",
    "            \n",
    "            pad_size = binded_length - len(tmp_simu_wave)\n",
    "            if pad_size > 0:\n",
    "                if self.padding_type == \"white_noise\":\n",
    "                    # padding white noise\n",
    "                    padding = np.random.normal(0, 0.02, size=pad_size)\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # padding zeros\n",
    "                    padding = np.zeros(pad_size)\n",
    "\n",
    "                simu_wave[i] = np.concatenate((tmp_simu_wave, padding), axis=None)\n",
    "                \n",
    "            else:\n",
    "                simu_wave[i] = tmp_simu_wave\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_wave\n",
    "    \n",
    "    def get_picker(self):\n",
    "        '''\n",
    "        picker stands for index pick\n",
    "        this is for combining audio data with decided minimum and maximum size\n",
    "        '''\n",
    "        size = np.random.randint(low=self.min_sz, \n",
    "                                 high=self.max_sz+1, \n",
    "                                 size=self.create_size)\n",
    "\n",
    "        picker = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, s in enumerate(size):\n",
    "            picker[i] = np.random.choice(self.wave_shape[0]-1, size=self.max_sz, replace=False)[:s]\n",
    "            \n",
    "        return picker\n",
    "\n",
    "    def simulate_label(self, labels):\n",
    "        '''\n",
    "        method for simulating label inputs which will concatenate labels following simulated waves\n",
    "        \n",
    "        Args:\n",
    "            labels: input labels following with audio dataset\n",
    "        '''\n",
    "        print(\"Label Simulation ... \", end=\"\")\n",
    "        \n",
    "        simu_label = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, picker in enumerate(self.pickers):\n",
    "            simu_label[i] = np.array([labels[p] for p in picker])\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_label\n",
    "\n",
    "    def simulate_phoneme(self, labels, label_dict, phoneme_dict):\n",
    "        '''\n",
    "        method for sumulating phoneme inputs\n",
    "        which will concatenate audio phonemes with labels we concated by simulate_label()\n",
    "        \n",
    "        Args:\n",
    "            labels: labels that one would like to transfer\n",
    "            label_dict: label dictionary\n",
    "            phoneme_dict: phoneme dictionary\n",
    "        '''\n",
    "        print(\"Phoneme Simulation... \", end=\"\")\n",
    "        \n",
    "        self.label_dict = label_dict\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "\n",
    "        simu_phoneme = np.empty(self.create_size, dtype=np.object)\n",
    "        for i, label in enumerate(labels):\n",
    "            simu_phoneme[i] = \" \".join([self.phoneme_translator(lab) for lab in label])\n",
    "            simu_phoneme[i] = \"<start> \" + simu_phoneme[i] + \" <end>\"\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_phoneme\n",
    "\n",
    "    def phoneme_translator(self, input_label):\n",
    "        '''\n",
    "        translate labels to phoneme if simulate_phoneme is called\n",
    "        \n",
    "        Args:\n",
    "            input_label: label that one would like to transfer into phonemes\n",
    "        '''\n",
    "        for i, label in enumerate(self.label_dict):\n",
    "            if input_label == label:\n",
    "                return self.phoneme_dict[i]\n",
    "            \n",
    "    def tokenize(self, phoneme):\n",
    "        '''\n",
    "        with tensorflow we can simply apply Tokenizer for text(in our case, phoneme)\n",
    "        to generate phoneme outputs\n",
    "        \n",
    "        Args:\n",
    "            phoneme: phoneme string with '<start>' and '<end>'\n",
    "        '''\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(phoneme)\n",
    "        tensor = tokenizer.texts_to_sequences(phoneme)\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, tokenizer\n",
    "\n",
    "    def show_convert(self, tensor, tokenizer):\n",
    "        '''\n",
    "        showing case of tokenized word according to its index\n",
    "        \n",
    "        Args:\n",
    "            tensor: phoneme tensor\n",
    "            tokenizer: phoneme tokenizer\n",
    "        '''\n",
    "        print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "        print(\"=======================\")\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave Data Simulation ... Done\n",
      "Label Simulation ... Done\n",
      "Phoneme Simulation... Done\n",
      "\n",
      "Example Label Display: ['eight']\n",
      "Example Phoneme Display: <start> EY T <end>\n"
     ]
    }
   ],
   "source": [
    "CREATE_SIZE = 20000\n",
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(create_size=CREATE_SIZE, \n",
    "                            min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "simu_wave = preprocesser.simulate_wave(wav_array)\n",
    "simu_label = preprocesser.simulate_label(label_array)\n",
    "simu_phoneme = preprocesser.simulate_phoneme(labels=simu_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {simu_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [16195]\n",
      "[array(['three'], dtype='<U5')]\n",
      "['<start> TH R IY <end>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "index = np.random.randint(len(simu_wave), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "print(simu_label[index])\n",
    "print(simu_phoneme[index])\n",
    "ipd.Audio(simu_wave[index], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    '''\n",
    "    This is the Mel-Frequency Cepstral Coefficients, MFCCs Transformation\n",
    "    including\n",
    "        1. Pre-emphasis\n",
    "        2. Framing\n",
    "        3. Hamming window\n",
    "        4. Short-time Fourier Transform\n",
    "        5. Mel triangular bandpass filters\n",
    "        6. Log energy\n",
    "        \n",
    "    not including\n",
    "        7. Discrete cosine transform\n",
    "        8. Delta cepstrum\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Args:\n",
    "            alpha: coefficient applied when applying pre-emphasis, usually between (0.95, 0.98)\n",
    "            frame_size: duration of one frame in second\n",
    "            frame_stride: stride duration of one frame in second\n",
    "            n_fft: decided number while applying Fast Fourier Transformation\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        self.apply_delta = apply_delta\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        mfcc_features = np.column_stack((energy, fbank))\n",
    "        \n",
    "        if self.apply_delta:\n",
    "            mfcc_feat = mfcc_features.T\n",
    "            \n",
    "            d_mfcc_feat = self.delta(mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_feat.T, d_mfcc_feat.T), axis=1)\n",
    "\n",
    "            dd_mfcc_feat = self.delta(d_mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_features, dd_mfcc_feat.T), axis=1)\n",
    "\n",
    "        return mfcc_features\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                              # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                           # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz\n",
    "    \n",
    "    def delta(self, mfcc_features, neighbor_len=2):\n",
    "        denominator = 2 * sum([i**2 for i in np.arange(1, neighbor_len+1)])\n",
    "        delta_feat = np.empty_like(mfcc_features)\n",
    "        padded = np.pad(mfcc_features, ((neighbor_len, neighbor_len), (0, 0)), mode='edge') # padded version of feat\n",
    "        for t in range(len(mfcc_features)):\n",
    "            delta_feat[t] = np.dot(np.arange(-neighbor_len, neighbor_len+1), \n",
    "                                      padded[t : t+2*neighbor_len+1]) / denominator\n",
    "        return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCApplier:\n",
    "    '''\n",
    "    This is the MFCC applier for applying MFCC \n",
    "    and pad zeros for fitting the data into Encoder-Decoder Model with Attention\n",
    "    which we will build later\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Arg:\n",
    "            mfcc: build by MFCC class for transform inputs\n",
    "            decide_size: a 2^k number which will make the inputs reshape into X*decide_size\n",
    "                         which will help us build the pyramidal RNN encoder\n",
    "        '''\n",
    "        self.mfcc = MFCC(alpha=alpha, \n",
    "                         frame_size=frame_size, \n",
    "                         frame_stride=frame_stride, \n",
    "                         n_fft=n_fft, \n",
    "                         n_filter=n_filter, \n",
    "                         apply_delta=apply_delta)\n",
    "        \n",
    "        \n",
    "    def apply(self, inputs, sample_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: wave data that one would like to process\n",
    "        '''\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        sample = self.mfcc.mfcc(inputs[0, :], sample_rate)\n",
    "        sample_shape = sample.shape\n",
    "        \n",
    "        outputs = np.zeros(((input_shape[0], sample_shape[0], sample_shape[1])))\n",
    "        \n",
    "        for i in np.arange(input_shape[0]):\n",
    "            outputs[i, :, :] = self.mfcc.mfcc(inputs[i, :], sample_rate)\n",
    "            print(f\"Applying MFCC to {i+1}th case\", end=\"\\r\")\n",
    "            \n",
    "        print()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying MFCC to 20000th case\n",
      "Simulated MFCC wave: (input size, time steps, MFCC dimension) (20000, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.95\n",
    "FRAME_SIZE = 0.025\n",
    "FRAME_STRIDE = 0.01\n",
    "N_FFT = 512\n",
    "N_FILTER = 12\n",
    "\n",
    "mfcc_applier = MFCCApplier(alpha=ALPHA, \n",
    "                           frame_size=FRAME_SIZE, \n",
    "                           frame_stride=FRAME_STRIDE, \n",
    "                           n_fft=N_FFT, \n",
    "                           n_filter=N_FILTER, \n",
    "                           apply_delta=True)\n",
    "\n",
    "mfcced_simu_wave = mfcc_applier.apply(simu_wave, sample_rate=SAMPLE_RATE)\n",
    "print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_simu_wave.shape))\n",
    "\n",
    "simu_wave = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from python_speech_features import logfbank\n",
    "\n",
    "# ex_shape = logfbank(simu_wave[0], SAMPLE_RATE).shape\n",
    "# mfcced_simu_wave = np.zeros(((len(simu_wave), ex_shape[0], ex_shape[1])))\n",
    "# for i, wave in enumerate(simu_wave):\n",
    "#     mfcc = logfbank(wave, SAMPLE_RATE)\n",
    "    \n",
    "# #     mean = np.mean(mfcc, axis=0)\n",
    "# #     std = np.std(mfcc, axis=0)\n",
    "# #     mfcced_simu_wave[i, :, :] = (mfcc - mean) / std\n",
    "#     mfcced_simu_wave[i, :, :] = mfcc\n",
    "    \n",
    "#     print(f\"Transfering {i+1}th case\", end=\"\\r\")\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (20000, 99, 39)\n",
      "Output Shape: (20000, 7)\n",
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "18\t--->\tey\n",
      "7\t--->\tt\n",
      "2\t--->\t<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = preprocesser.tokenize(simu_phoneme)\n",
    "\n",
    "print(\"Input Shape: {}\".format(mfcced_simu_wave.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()\n",
    "    \n",
    "# Split the data into size (training set, testing set) (18000, 2000)\n",
    "TRAIN_SIZE = 0.9\n",
    "\n",
    "wav_tensor, wav_tensor_val, phoneme_tensor, phoneme_tensor_val = train_test_split(mfcced_simu_wave, \n",
    "                                                                                  phoneme_tensor, \n",
    "                                                                                  train_size=TRAIN_SIZE, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  shuffle=True)\n",
    "\n",
    "mfcced_simu_wave = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "VAL_WAV_SIZE = len(wav_tensor_val)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "wav_tensor = tf.convert_to_tensor(wav_tensor, dtype=tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "wav_tensor_val = tf.convert_to_tensor(wav_tensor_val, dtype=tf.float32)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((wav_tensor_val, phoneme_tensor_val)).shuffle(VAL_WAV_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input Shape: (4, 99, 39)\n",
      "Example Output Shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(f\"Example Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Example Output Shape: {example_target_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters, stride=None):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        \n",
    "        if stride is None:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.filters1, self.stride, padding='valid', activation=\"relu\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.filters2, kernel_size, padding='same', activation=\"relu\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv1D(self.filters3, 1, padding='valid', activation=\"relu\")\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        if self.stride != 1:\n",
    "            short_cut = tf.keras.layers.Conv1D(self.filters3, \n",
    "                                               self.stride, \n",
    "                                               padding='valid', \n",
    "                                               activation=\"relu\")(input_tensor)\n",
    "            x += short_cut\n",
    "        else:\n",
    "            x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "resnet_block = ResnetIdentityBlock(32, [1, 2, example_input_batch.shape[-1]])\n",
    "resnet_output = resnet_block(example_input_batch)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(resnet_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_identity_block\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              multiple                  40        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  4         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  66        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  8         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  117       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  156       \n",
      "=================================================================\n",
      "Total params: 391\n",
      "Trainable params: 307\n",
      "Non-trainable params: 84\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_block.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder for MFCC transformed wave data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate, \n",
    "                 squeeze_time, \n",
    "                 rnn_initial_weight=None):\n",
    "        '''\n",
    "        Args:\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size            \n",
    "            dropout_rate: layer dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units    \n",
    "        self.squeeze_time = squeeze_time\n",
    "        self.rnn_initial_weight = rnn_initial_weight\n",
    "        \n",
    "        # normalization\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # conv1d\n",
    "        conv_units = 64\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=conv_units, \n",
    "                                           kernel_size=5, \n",
    "                                           strides=1, \n",
    "                                           padding='same', \n",
    "                                           activation=\"relu\", \n",
    "                                           kernel_initializer='glorot_uniform')\n",
    "        \n",
    "        # ResNet\n",
    "#         self.resnet1 = ResnetIdentityBlock(32, filters=[conv_units//4, conv_units//2, conv_units])\n",
    "#         self.resnet2 = ResnetIdentityBlock(64, [conv_units//4, conv_units//2, conv_units])\n",
    "#         self.resnet3 = ResnetIdentityBlock(128, [conv_units//4, conv_units//2, conv_units])\n",
    "        \n",
    "        # pBLSTM1\n",
    "        self.fw_lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        self.bw_lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate, \n",
    "                                             go_backwards=True)\n",
    "        self.bn_pblstm1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # pBLSTM2\n",
    "#         self.fw_lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "#                                              return_sequences=True, \n",
    "#                                              return_state=True, \n",
    "#                                              kernel_initializer=\"lecun_normal\",\n",
    "#                                              activation='tanh', \n",
    "#                                              recurrent_activation='sigmoid', \n",
    "#                                              recurrent_initializer='orthogonal', \n",
    "#                                              dropout=dropout_rate)\n",
    "        \n",
    "#         self.bw_lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "#                                              return_sequences=True, \n",
    "#                                              return_state=True, \n",
    "#                                              kernel_initializer=\"lecun_normal\",\n",
    "#                                              activation='tanh', \n",
    "#                                              recurrent_activation='sigmoid', \n",
    "#                                              recurrent_initializer='orthogonal', \n",
    "#                                              dropout=dropout_rate, \n",
    "#                                              go_backwards=True)\n",
    "#         self.bn_pblstm2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Encoder lstm\n",
    "        self.enc_lstm = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call pyramidal LSTM neural network encoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: wave input\n",
    "        '''\n",
    "        self.layer_info[\"Input\"] = inputs.shape\n",
    "        x = self.bn(inputs)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # ResNet\n",
    "#         x = self.resnet1(x)\n",
    "#         x = self.resnet2(x)\n",
    "#         x = self.resnet3(x)\n",
    "        \n",
    "        # pBLSTM 1\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.fw_lstm1(x)\n",
    "        bw_outputs, bw_state_h, bw_state_c = self.bw_lstm1(x)\n",
    "        x = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        x = self.bn_pblstm1(x)\n",
    "        \n",
    "        # pBLSTM 2\n",
    "#         fw_outputs, fw_state_h, fw_state_c = self.fw_lstm2(x)\n",
    "#         bw_outputs, bw_state_h, bw_state_c = self.bw_lstm2(x)\n",
    "#         x = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "#         x = self.reshape_pyramidal(x)\n",
    "#         x = self.bn_pblstm2(x)\n",
    "        \n",
    "        # encoder output layer\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.enc_lstm(x)\n",
    "            \n",
    "        return fw_outputs, fw_state_h, fw_state_c\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        \n",
    "        Args:\n",
    "            outputs: outputs from LSTM\n",
    "            squeeze_time: time step one would like to squeeze in pyramidal LSTM\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "\n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * self.squeeze_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 33, 256)\n",
      "Encoder forward state h shape: (batch size, units) (4, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_DROPOUT_RATE = 0.0\n",
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, fw_sample_state_h, fw_sample_state_c = encoder(example_input_batch)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch multiple                  156       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            multiple                  12544     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  328704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  328704    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  6144      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  1836032   \n",
      "=================================================================\n",
      "Total params: 2,512,284\n",
      "Trainable params: 2,509,134\n",
      "Non-trainable params: 3,150\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.V = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder for output phonemes\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 target_sz, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            target_sz: target size, total phoneme size in this case\n",
    "            embedding_dim: embedding dimension\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size\n",
    "            dropout_rate: dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.target_sz = target_sz\n",
    "        self.lstm_units = lstm_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        \n",
    "        # attention model\n",
    "        self.attention = LuongAttention(lstm_units)\n",
    "        \n",
    "        # decoder rnn            \n",
    "        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "        \n",
    "        self.lstm2 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "\n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(32, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.resnet2 = ResnetIdentityBlock(64, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet2_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.resnet3 = ResnetIdentityBlock(128, [lstm_units//4, lstm_units//2, lstm_units])\n",
    "        self.resnet3_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "\n",
    "#         self.conv1 = tf.keras.layers.Conv1D(64, 13, padding=\"same\", activation=\"relu\")\n",
    "#         self.conv2 = tf.keras.layers.Conv1D(128, 11, padding=\"same\", activation=\"relu\")\n",
    "#         self.conv3 = tf.keras.layers.Conv1D(256, 9, padding=\"same\", activation=\"relu\")\n",
    "    \n",
    "        # Fully-connected\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(target_sz, activation=\"softmax\")\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        call LSTM decoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: target output, following phoneme for wave data input in this case\n",
    "            enc_hidden_h: encoder hidden state h\n",
    "            enc_hidden_c: encoder hidden state c\n",
    "            enc_output: encoder outputs\n",
    "        '''\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the 2-layer LSTM (Decoder)\n",
    "        outputs, state_h, state_c = self.lstm1(x)\n",
    "        outputs, state_h, state_c = self.lstm2(outputs)\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet1(outputs)\n",
    "        x = self.resnet1_dropout(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x = self.resnet2_dropout(x)\n",
    "        \n",
    "        x = self.resnet3(x)\n",
    "        x = self.resnet3_dropout(x)\n",
    "\n",
    "        # Convolution\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "    \n",
    "        # dense layer before final predict output dense layer\n",
    "        x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        # output shape == (batch_size, phoneme size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, phoneme size) (4, 22)\n"
     ]
    }
   ],
   "source": [
    "DECODER_DROPOUT_RATE = 0.0\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, phoneme size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2816      \n",
      "_________________________________________________________________\n",
      "luong_attention_1 (LuongAtte multiple                  131841    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  656384    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  525312    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_1 (Res multiple                  313536    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_2 (Res multiple                  575680    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_3 (Res multiple                  1099968   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  1430      \n",
      "=================================================================\n",
      "Total params: 3,323,415\n",
      "Trainable params: 3,320,727\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "'''\n",
    "Candidate optimizer:\n",
    "    1. Adam\n",
    "    2. Nadam\n",
    "    \n",
    "    the preformence of other optimizers are not good\n",
    "'''\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "# optimizer = tf.keras.optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "# optimizer = tf.keras.optimizers.Adadelta(learning_rate=LEARNING_RATE)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  rnn_initial_weight=\"normal\", \n",
    "                  squeeze_time=3)\n",
    "\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "#     clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)  # clipping for avoiding gradient explosion\n",
    "#     optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "\n",
    "    inp = tf.expand_dims(inp, 0)\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * 1, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[0]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "        loss += loss_function(targ[t], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if targ_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return loss\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1/5 Batch - 0 Loss - 1.7413 Time - 1s\n",
      "Epoch - 1/5 Batch - 1000 Loss - 0.4498 Time - 406s\n",
      "Epoch - 1/5 Batch - 2000 Loss - 0.3775 Time - 861s\n",
      "Epoch - 1/5 Batch - 3000 Loss - 0.3965 Time - 1294s\n",
      "Epoch - 1/5 Batch - 4000 Loss - 0.4004 Time - 1700s\n",
      "Epoch - 1/5 Batch - 4499 Loss - 0.3280 Time - 1900s\n",
      "================================\n",
      "Epoch 1 Loss 0.4315\n",
      "Time taken for epoch 1 -- 31.6992 min\n",
      "Total Time taken -- 31.6992 min\n",
      "================================\n",
      "\n",
      "Epoch - 2/5 Batch - 0 Loss - 0.3354 Time - 1s\n",
      "Epoch - 2/5 Batch - 1000 Loss - 0.3244 Time - 405s\n",
      "Epoch - 2/5 Batch - 2000 Loss - 0.3486 Time - 809s\n",
      "Epoch - 2/5 Batch - 3000 Loss - 0.3977 Time - 1213s\n",
      "Epoch - 2/5 Batch - 4000 Loss - 0.3639 Time - 1627s\n",
      "Epoch - 2/5 Batch - 4499 Loss - 0.3433 Time - 1832s\n",
      "================================\n",
      "Epoch 2 Loss 0.3554\n",
      "Time taken for epoch 2 -- 30.5565 min\n",
      "Total Time taken -- 62.2557 min\n",
      "================================\n",
      "\n",
      "Epoch - 3/5 Batch - 0 Loss - 0.3603 Time - 1s\n",
      "Epoch - 3/5 Batch - 1000 Loss - 0.3504 Time - 401s\n",
      "Epoch - 3/5 Batch - 2000 Loss - 2.7241 Time - 800s\n",
      "Epoch - 3/5 Batch - 3000 Loss - 0.9536 Time - 1205s\n",
      "Epoch - 3/5 Batch - 4000 Loss - 0.3269 Time - 1609s\n",
      "Epoch - 3/5 Batch - 4499 Loss - 0.3567 Time - 1809s\n",
      "================================\n",
      "Epoch 3 Loss 1.1039\n",
      "Time taken for epoch 3 -- 30.1555 min\n",
      "Total Time taken -- 92.4113 min\n",
      "================================\n",
      "\n",
      "Epoch - 4/5 Batch - 0 Loss - 0.3640 Time - 1s\n",
      "Epoch - 4/5 Batch - 1000 Loss - 0.3249 Time - 411s\n",
      "Epoch - 4/5 Batch - 2000 Loss - 0.3098 Time - 819s\n",
      "Epoch - 4/5 Batch - 3000 Loss - 0.3155 Time - 1218s\n",
      "Epoch - 4/5 Batch - 4000 Loss - 0.2860 Time - 1619s\n",
      "Epoch - 4/5 Batch - 4499 Loss - 0.2192 Time - 1819s\n",
      "================================\n",
      "Epoch 4 Loss 0.3114\n",
      "Time taken for epoch 4 -- 30.3404 min\n",
      "Total Time taken -- 122.7516 min\n",
      "================================\n",
      "\n",
      "Epoch - 5/5 Batch - 0 Loss - 0.2247 Time - 1s\n",
      "Epoch - 5/5 Batch - 1000 Loss - 0.2896 Time - 399s\n",
      "Epoch - 5/5 Batch - 2000 Loss - 0.3410 Time - 798s\n",
      "Epoch - 5/5 Batch - 3000 Loss - 0.2513 Time - 1196s\n",
      "Epoch - 5/5 Batch - 4000 Loss - 0.2824 Time - 1594s\n",
      "Epoch - 5/5 Batch - 4499 Loss - 0.2536 Time - 1792s\n",
      "================================\n",
      "Epoch 5 Loss 0.2865\n",
      "Time taken for epoch 5 -- 29.9038 min\n",
      "Total Time taken -- 152.6555 min\n",
      "================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run eagerly will make tensorflow run step by step or else it will raise\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "# similar to Pytorch, which is a dynamic graph for deep learning\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 10\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # train the encoder-decoder model\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "    for inp, targ in dataset.take(STEP_PER_EPOCH):\n",
    "        batch += 1\n",
    "        \n",
    "        batch_loss = train_step(inp, targ, phoneme_tokenizer)\n",
    "        total_loss += batch_loss\n",
    "            \n",
    "        print(\"Epoch - {}/{} Batch - {} Loss - {:.4f} Time - {:.0f}s\".format(\n",
    "            epoch, EPOCHS, batch, batch_loss.numpy(), time.time()-epoch_start),\n",
    "              end=\"\\r\")\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            print()\n",
    "\n",
    "    # saving (checkpoint) the model when total loss is less than 0.9\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    # validation process\n",
    "    total_val_loss = 0\n",
    "    for val_inp, val_targ in dataset_val.take(VAL_WAV_SIZE):\n",
    "        val_loss = validate_step(val_inp, val_targ, phoneme_tokenizer)\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "    # print out the epoch results\n",
    "    mean_total_loss = total_loss / STEP_PER_EPOCH\n",
    "    mean_val_loss = total_val_loss / VAL_WAV_SIZE\n",
    "    print(\"\\n================================\")\n",
    "    print('Epoch {} Loss {:.4f} Val-Loss {:.4f}'.format(epoch, mean_total_loss, mean_val_loss))\n",
    "    print('Time taken for epoch {} -- {:.2f} min'.format(epoch, (time.time() - epoch_start)/60))\n",
    "    print('Total Time taken -- {:.2f} min'.format((time.time() - start)/60))\n",
    "    print(\"================================\\n\")\n",
    "    \n",
    "    if mean_total_loss < 0.05:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, max_input_len, max_output_len, tokenizer=None):\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    \n",
    "#     inputs = tf.expand_dims(inputs, 0)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    \n",
    "    \n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    \n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            inputs=dec_input, \n",
    "            enc_hidden_h=dec_hidden_h, \n",
    "            enc_hidden_c=dec_hidden_c, \n",
    "            enc_output=enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    # ax.set_xticklabels([''] + input_wav, fontdict=fontdict, rotation=90)\n",
    "#     ax.set_xticklabels(range(len(input_wav)))\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "\n",
    "    # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "\n",
    "    print(f'Original Input Length: {len(wave)}')\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [802]\n",
      "Original Input Length: 1\n",
      "Predicted translation: ey v <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "18\t--->\tey\n",
      "7\t--->\tt\n",
      "2\t--->\t<end>\n"
     ]
    }
   ],
   "source": [
    "# testing with test wave data\n",
    "index = np.random.randint(len(wav_tensor_val), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "test_wave = tf.convert_to_tensor(wav_tensor_val[index], dtype=tf.float32)\n",
    "translate(test_wave, sample_output.shape[1], sample_decoder_output.shape[1], phoneme_tokenizer)\n",
    "\n",
    "test_phoneme = phoneme_tensor_val[index, :]\n",
    "for tensor in test_phoneme:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
