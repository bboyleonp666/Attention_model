{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.0.0\n",
      "\n",
      "GPU available: True\n",
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reference: \n",
    "    https://arxiv.org/abs/1508.01211\n",
    "    https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    https://github.com/jameslyons/python_speech_features\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "    \n",
    "data source: \n",
    "    https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"CUDA enabled: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveReader:\n",
    "    def __init__(self, path, sample_rate, padding_type, read_size):\n",
    "        '''\n",
    "        Args:\n",
    "            path: train path containing directory which one would like to load\n",
    "            sample_rate: sample rate for reading .wav file\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "            read_size: size that one would like to read\n",
    "        '''\n",
    "        \n",
    "        self.path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.padding_type = padding_type\n",
    "        self.read_size = read_size\n",
    "\n",
    "    def read(self, labels=None):\n",
    "        '''\n",
    "        read all the data under the labels(directories) one select\n",
    "        \n",
    "        Args:\n",
    "            labels: labels(directories) one would like to load\n",
    "                    None means read all the directories under that directory\n",
    "        '''\n",
    "        print(\"LABEL\\tTOTAL\\tREAD\\tSAVED\\t<1s COUNT\")\n",
    "        print(\"-----\\t-----\\t----\\t-----\\t---------\")\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f for f in os.listdir(path) if os.path.isdir(path + \"\\\\\" + f)]\n",
    "            \n",
    "        elif type(labels) == str:\n",
    "            samples, total_wave_count, total_wave_read, total_loss_count = self.read_dir(dir_name=labels)\n",
    "            sample_labels = np.repeat(labels, total_wave_read)\n",
    "            \n",
    "            print(\"\\nMISSION COMPELTE!!!\")\n",
    "            return samples, sample_labels, total_wave_count, total_loss_count\n",
    "                    \n",
    "        label_len = len(labels)\n",
    "        total_wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "        total_wave_read = np.zeros(label_len, dtype=np.int32)\n",
    "        total_loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for i, lab in enumerate(labels):\n",
    "            samp, total_wave_count[i], total_wave_read[i], total_loss_count[i] = self.read_dir(dir_name=lab)\n",
    "            \n",
    "            if i == 0:\n",
    "                samples = samp\n",
    "                sample_labels = np.repeat(lab, total_wave_read[i])\n",
    "            else:\n",
    "                samples = np.concatenate((samples, samp), axis=0)\n",
    "                sample_labels = np.concatenate((sample_labels, np.repeat(lab, total_wave_read[i])), axis=None)\n",
    "        \n",
    "        print(\"\\nMISSION COMPELTE!!!\")\n",
    "        return samples, sample_labels, total_wave_count, total_loss_count\n",
    "    \n",
    "    def read_dir(self, dir_name):\n",
    "        '''\n",
    "        read one directory of given directory name\n",
    "        \n",
    "        Args:\n",
    "            dir_name: directory name\n",
    "        '''\n",
    "        dir_path = os.path.join(self.path, dir_name)\n",
    "        wave_files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n",
    "        total_wave_files = len(wave_files)\n",
    "\n",
    "        if self.read_size is not None:\n",
    "            wave_files_read = self.read_size\n",
    "        else:\n",
    "            wave_files_read = total_wave_files\n",
    "\n",
    "        samples = np.zeros((wave_files_read, self.sample_rate))\n",
    "        less_than_1s_count = 0\n",
    "        num_of_file_read = 0\n",
    "        for i, wav_file in enumerate(wave_files):\n",
    "            wave_file_path = os.path.join(dir_path, wav_file)\n",
    "            samp, _ = librosa.load(wave_file_path, sr=self.sample_rate)\n",
    "\n",
    "            pad_size = self.sample_rate - len(samp)\n",
    "            if pad_size > 0:\n",
    "                less_than_1s_count += 1\n",
    "                if self.padding_type is None:\n",
    "                    # None: than skip this wave file\n",
    "                    continue\n",
    "\n",
    "                elif self.padding_type == \"white_noise\":\n",
    "                    # white_noise: pad white noise data behind\n",
    "                    padding = np.random.normal(0, 0.02, pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # zero: pad zeros behind\n",
    "                    padding = np.zeros(pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "            else:\n",
    "                samples[num_of_file_read, :] = samp\n",
    "                num_of_file_read += 1\n",
    "\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(dir_name, \n",
    "                                              total_wave_files, \n",
    "                                              i+1, \n",
    "                                              num_of_file_read, \n",
    "                                              less_than_1s_count), end=\"\\r\")\n",
    "            \n",
    "            if num_of_file_read == wave_files_read:\n",
    "                break\n",
    "                \n",
    "        print()\n",
    "\n",
    "        return samples, total_wave_files, wave_files_read, less_than_1s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\tSAVED\t<1s COUNT\n",
      "-----\t-----\t----\t-----\t---------\n",
      "zero\t2376\t2160\t2000\t160\n",
      "one\t2370\t2262\t2000\t262\n",
      "two\t2373\t2222\t2000\t222\n",
      "three\t2356\t2207\t2000\t207\n",
      "four\t2372\t2201\t2000\t201\n",
      "five\t2357\t2185\t2000\t185\n",
      "six\t2369\t2164\t2000\t164\n",
      "seven\t2377\t2194\t2000\t194\n",
      "eight\t2352\t2232\t2000\t232\n",
      "nine\t2364\t2178\t2000\t178\n",
      "\n",
      "MISSION COMPELTE!!!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "reader = WaveReader(path=train_audio_path, \n",
    "                    sample_rate=SAMPLE_RATE, \n",
    "                    padding_type=None, \n",
    "                    read_size=2000)\n",
    "\n",
    "wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words)\n",
    "# wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words[0])\n",
    "\n",
    "# print(\"\\nCheck the existence of NaN and Inf\")\n",
    "# print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "# print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, min_sz=6, max_sz=8, padding_type=\"zero\"):\n",
    "        '''\n",
    "        Args:\n",
    "            create_size: size of binding wave one would like to create\n",
    "            min_sz: minimum size of wave data\n",
    "            max_sz: maximum size of wave data\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "        '''\n",
    "        self.min_sz = min_sz\n",
    "        self.max_sz = max_sz\n",
    "        self.padding_type = padding_type\n",
    "        \n",
    "    def get_picker(self):\n",
    "        '''\n",
    "        picker stands for index pick\n",
    "        this is for combining audio data with decided minimum and maximum size\n",
    "        '''\n",
    "        size = np.random.randint(low=self.min_sz, \n",
    "                                 high=self.max_sz+1, \n",
    "                                 size=self.create_size)\n",
    "\n",
    "        picker = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, s in enumerate(size):\n",
    "            picker[i] = np.random.choice(self.wave_shape[0]-1, size=self.max_sz, replace=False)[:s]\n",
    "            \n",
    "        return picker\n",
    "\n",
    "    def simulate_wave(self, waves, create_size=None):\n",
    "        '''\n",
    "        method for simulating wave inputs\n",
    "        which will concatenate audio inputs for building longer audio dataset\n",
    "        \n",
    "        Args:\n",
    "            waves: input wave data\n",
    "        '''\n",
    "        # get picker for combining waves and labels(phonemes)\n",
    "        self.wave_shape = waves.shape\n",
    "        if create_size is not None:\n",
    "            self.create_size = create_size\n",
    "        else:\n",
    "            self.create_size = self.wave_shape[0]\n",
    "            \n",
    "        self.pickers = self.get_picker()\n",
    "        \n",
    "        print(\"Wave Data Simulation ... \", end=\"\")\n",
    "        \n",
    "        binded_length = self.wave_shape[1]*self.max_sz\n",
    "        simu_wave = np.zeros((self.create_size, binded_length))\n",
    "        \n",
    "        \n",
    "        for i, picker in enumerate(self.pickers):        \n",
    "            tmp_simu_wave = np.array([waves[p] for p in picker]).flatten()\n",
    "            \n",
    "            pad_size = binded_length - len(tmp_simu_wave)\n",
    "            if pad_size > 0:\n",
    "                if self.padding_type == \"white_noise\":\n",
    "                    # padding white noise\n",
    "                    padding = np.random.normal(0, 0.02, size=pad_size)\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # padding zeros\n",
    "                    padding = np.zeros(pad_size)\n",
    "\n",
    "                simu_wave[i] = np.concatenate((tmp_simu_wave, padding), axis=None)\n",
    "                \n",
    "            else:\n",
    "                simu_wave[i] = tmp_simu_wave\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_wave\n",
    "\n",
    "    def simulate_label(self, labels):\n",
    "        '''\n",
    "        method for simulating label inputs which will concatenate labels following simulated waves\n",
    "        \n",
    "        Args:\n",
    "            labels: input labels following with audio dataset\n",
    "        '''\n",
    "        print(\"Label Simulation ... \", end=\"\")\n",
    "        \n",
    "        simu_label = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, picker in enumerate(self.pickers):\n",
    "            simu_label[i] = np.array([labels[p] for p in picker])\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_label\n",
    "\n",
    "    def simulate_phoneme(self, labels, label_dict, phoneme_dict):\n",
    "        '''\n",
    "        method for sumulating phoneme inputs\n",
    "        which will concatenate audio phonemes with labels we concated by simulate_label()\n",
    "        \n",
    "        Args:\n",
    "            labels: labels that one would like to transfer\n",
    "            label_dict: label dictionary\n",
    "            phoneme_dict: phoneme dictionary\n",
    "        '''\n",
    "        print(\"Phoneme Simulation... \", end=\"\")\n",
    "        \n",
    "        self.label_dict = label_dict\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "\n",
    "        simu_phoneme = np.empty(self.create_size, dtype=np.object)\n",
    "        for i, label in enumerate(labels):\n",
    "            simu_phoneme[i] = \" \".join([self.phoneme_translator(lab) for lab in label])\n",
    "            simu_phoneme[i] = \"<start> \" + simu_phoneme[i] + \" <end>\"\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_phoneme\n",
    "\n",
    "    def phoneme_translator(self, input_label):\n",
    "        '''\n",
    "        translate labels to phoneme if simulate_phoneme is called\n",
    "        \n",
    "        Args:\n",
    "            input_label: label that one would like to transfer into phonemes\n",
    "        '''\n",
    "        for i, label in enumerate(self.label_dict):\n",
    "            if input_label == label:\n",
    "                return self.phoneme_dict[i]\n",
    "            \n",
    "    def tokenize(self, phoneme):\n",
    "        '''\n",
    "        with tensorflow we can simply apply Tokenizer for text(in our case, phoneme)\n",
    "        to generate phoneme outputs\n",
    "        \n",
    "        Args:\n",
    "            phoneme: phoneme string with '<start>' and '<end>'\n",
    "        '''\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(phoneme)\n",
    "        tensor = tokenizer.texts_to_sequences(phoneme)\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, tokenizer\n",
    "\n",
    "    def show_convert(self, tensor, tokenizer):\n",
    "        '''\n",
    "        showing case of tokenized word according to its index\n",
    "        \n",
    "        Args:\n",
    "            tensor: phoneme tensor\n",
    "            tokenizer: phoneme tokenizer\n",
    "        '''\n",
    "        print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "        print(\"=======================\")\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave Data Simulation ... Done\n",
      "Label Simulation ... Done\n",
      "Phoneme Simulation... Done\n",
      "\n",
      "Example Label Display: ['nine']\n",
      "Example Phoneme Display: <start> N AY N <end>\n"
     ]
    }
   ],
   "source": [
    "CREATE_SIZE = 20000\n",
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "simu_wave = preprocesser.simulate_wave(wav_array, create_size=CREATE_SIZE)\n",
    "simu_label = preprocesser.simulate_label(label_array)\n",
    "simu_phoneme = preprocesser.simulate_phoneme(labels=simu_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {simu_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [19587]\n",
      "[array(['seven'], dtype='<U5')]\n",
      "['<start> S EH V AH N <end>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "index = np.random.randint(len(simu_wave), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "print(simu_label[index])\n",
    "print(simu_phoneme[index])\n",
    "ipd.Audio(simu_wave[index], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    '''\n",
    "    This is the Mel-Frequency Cepstral Coefficients, MFCCs Transformation\n",
    "    including\n",
    "        1. Pre-emphasis\n",
    "        2. Framing\n",
    "        3. Hamming window\n",
    "        4. Short-time Fourier Transform\n",
    "        5. Mel triangular bandpass filters\n",
    "        6. Log energy\n",
    "        \n",
    "    not including\n",
    "        7. Discrete cosine transform\n",
    "        8. Delta cepstrum\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Args:\n",
    "            alpha: coefficient applied when applying pre-emphasis, usually between (0.95, 0.98)\n",
    "            frame_size: duration of one frame in second\n",
    "            frame_stride: stride duration of one frame in second\n",
    "            n_fft: decided number while applying Fast Fourier Transformation\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        self.apply_delta = apply_delta\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        mfcc_features = np.column_stack((energy, fbank))\n",
    "        \n",
    "        if self.apply_delta:\n",
    "            mfcc_feat = mfcc_features.T\n",
    "            \n",
    "            d_mfcc_feat = self.delta(mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_feat.T, d_mfcc_feat.T), axis=1)\n",
    "\n",
    "            dd_mfcc_feat = self.delta(d_mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_features, dd_mfcc_feat.T), axis=1)\n",
    "\n",
    "        return mfcc_features\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                              # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                           # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz\n",
    "    \n",
    "    def delta(self, mfcc_features, neighbor_len=2):\n",
    "        denominator = 2 * sum([i**2 for i in np.arange(1, neighbor_len+1)])\n",
    "        delta_feat = np.empty_like(mfcc_features)\n",
    "        padded = np.pad(mfcc_features, ((neighbor_len, neighbor_len), (0, 0)), mode='edge') # padded version of feat\n",
    "        for t in range(len(mfcc_features)):\n",
    "            delta_feat[t] = np.dot(np.arange(-neighbor_len, neighbor_len+1), \n",
    "                                      padded[t : t+2*neighbor_len+1]) / denominator\n",
    "        return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCApplier:\n",
    "    '''\n",
    "    This is the MFCC applier for applying MFCC \n",
    "    and pad zeros for fitting the data into Encoder-Decoder Model with Attention\n",
    "    which we will build later\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Arg:\n",
    "            mfcc: build by MFCC class for transform inputs\n",
    "            decide_size: a 2^k number which will make the inputs reshape into X*decide_size\n",
    "                         which will help us build the pyramidal RNN encoder\n",
    "        '''\n",
    "        self.mfcc = MFCC(alpha=alpha, \n",
    "                         frame_size=frame_size, \n",
    "                         frame_stride=frame_stride, \n",
    "                         n_fft=n_fft, \n",
    "                         n_filter=n_filter, \n",
    "                         apply_delta=apply_delta)\n",
    "        \n",
    "        \n",
    "    def apply(self, inputs, sample_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: wave data that one would like to process\n",
    "        '''\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        sample = self.mfcc.mfcc(inputs[0, :], sample_rate)\n",
    "        sample_shape = sample.shape\n",
    "        \n",
    "        outputs = np.zeros(((input_shape[0], sample_shape[0], sample_shape[1])))\n",
    "        \n",
    "        for i in np.arange(input_shape[0]):\n",
    "            outputs[i, :, :] = self.mfcc.mfcc(inputs[i, :], sample_rate)\n",
    "            print(f\"Applying MFCC to {i+1}th case\", end=\"\\r\")\n",
    "            \n",
    "        print()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying MFCC to 20000th case\n",
      "Simulated MFCC wave: (input size, time steps, MFCC dimension) (20000, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.95\n",
    "FRAME_SIZE = 0.025\n",
    "FRAME_STRIDE = 0.01\n",
    "N_FFT = 512\n",
    "N_FILTER = 12\n",
    "\n",
    "mfcc_applier = MFCCApplier(alpha=ALPHA, \n",
    "                           frame_size=FRAME_SIZE, \n",
    "                           frame_stride=FRAME_STRIDE, \n",
    "                           n_fft=N_FFT, \n",
    "                           n_filter=N_FILTER, \n",
    "                           apply_delta=True)\n",
    "\n",
    "mfcced_simu_wave = mfcc_applier.apply(simu_wave, sample_rate=SAMPLE_RATE)\n",
    "print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_simu_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(waves, labels, phonemes, train_size, random_seed=None):\n",
    "    total_wave_count = len(waves)\n",
    "    \n",
    "    # decide the training and validation size\n",
    "    if train_size < 1:\n",
    "        train_size = int(total_wave_count * train_size)\n",
    "    test_size = total_wave_count - train_size\n",
    "    \n",
    "    print(f\"Train Size: {train_size} Test Size: {test_size}\")\n",
    "    \n",
    "    # create a shuffled index for split training and validation\n",
    "    idx = np.arange(total_wave_count)\n",
    "    np.random.seed(seed=random_seed)\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    # assign memory to train dataset\n",
    "    train_wav = np.zeros(((train_size, waves.shape[1], waves.shape[2])))\n",
    "    train_lab = np.zeros(train_size, dtype=np.object)\n",
    "    train_pho = np.zeros((train_size, phonemes.shape[1]), dtype=np.int32)\n",
    "    \n",
    "    # assign memory to test dataset\n",
    "    test_wav = np.zeros(((test_size, waves.shape[1], waves.shape[2])))\n",
    "    test_lab = np.zeros(test_size, dtype=np.object)\n",
    "    test_pho = np.zeros((test_size, phonemes.shape[1]), dtype=np.int32)\n",
    "\n",
    "    # create dataset\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    IsTrain = True\n",
    "    print(\"Spliting Train-Test ... \", end=\"\")\n",
    "    for i in idx:\n",
    "        if IsTrain:\n",
    "            train_wav[train_count] = waves[i]\n",
    "            train_lab[train_count] = labels[i]\n",
    "            train_pho[train_count] = phonemes[i]\n",
    "        \n",
    "            train_count += 1            \n",
    "            if train_count == train_size:\n",
    "                IsTrain = False\n",
    "            continue\n",
    "        \n",
    "        test_wav[test_count] = waves[i]\n",
    "        test_lab[test_count] = labels[i]\n",
    "        test_pho[test_count] = phonemes[i]\n",
    "        \n",
    "        test_count += 1\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "    train_lab = np.array([lab for lab in train_lab])\n",
    "    test_lab = np.array([lab for lab in test_lab])\n",
    "    return (train_wav, train_lab, train_pho), (test_wav, test_lab, test_pho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (20000, 99, 39)\n",
      "Output Shape: (20000, 7)\n",
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "3\t--->\tn\n",
      "8\t--->\tay\n",
      "3\t--->\tn\n",
      "2\t--->\t<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = preprocesser.tokenize(simu_phoneme)\n",
    "\n",
    "print(\"Input Shape: {}\".format(mfcced_simu_wave.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 19000 Test Size: 1000\n",
      "Spliting Train-Test ... Done\n",
      "\n",
      "Train Wave Tensor Shape: (19000, 99, 39)\n",
      "Train Label Tensor Shape: (19000, 1)\n",
      "Train Phoneme Tensor Shape: (19000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into size (training set, testing set) (18000, 2000)\n",
    "TRAIN_SIZE = 0.95\n",
    "\n",
    "(wav_tensor, label_tensor, phoneme_tensor), (wav_tensor_val, label_tensor_val, phoneme_tensor_val) = train_test_split(\n",
    "    mfcced_simu_wave, simu_label, phoneme_tensor, train_size=TRAIN_SIZE, random_seed=777)\n",
    "\n",
    "print()\n",
    "print(f\"Train Wave Tensor Shape: {wav_tensor.shape}\")\n",
    "print(f\"Train Label Tensor Shape: {label_tensor.shape}\")\n",
    "print(f\"Train Phoneme Tensor Shape: {phoneme_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "VAL_WAV_SIZE = len(wav_tensor_val)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "wav_tensor = tf.convert_to_tensor(wav_tensor, dtype=tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "wav_tensor_val = tf.convert_to_tensor(wav_tensor_val, dtype=tf.float32)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((wav_tensor_val, phoneme_tensor_val)).shuffle(VAL_WAV_SIZE)\n",
    "\n",
    "# avoid running out of memory\n",
    "simu_wave = None\n",
    "mfcced_simu_wave = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input Shape: (4, 99, 39)\n",
      "Example Output Shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(f\"Example Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Example Output Shape: {example_target_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.filters1, 1, padding='valid')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.filters2, kernel_size, padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv1D(self.filters3, 1, padding='valid')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "resnet_block = ResnetIdentityBlock(32, [1, 2, example_input_batch.shape[-1]])\n",
    "resnet_output = resnet_block(example_input_batch)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(resnet_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_block.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder for MFCC transformed wave data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate, \n",
    "                 units,\n",
    "                 squeeze_time):\n",
    "        '''\n",
    "        Args:\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size            \n",
    "            dropout_rate: layer dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units    \n",
    "        self.squeeze_time = squeeze_time\n",
    "        \n",
    "        # conv1d\n",
    "        self.feat_extract = tf.keras.layers.Dense(units=units, activation=\"relu\")\n",
    "        self.feat_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(kernel_size=11, filters=[units, units, units])\n",
    "        \n",
    "        units *= squeeze_time\n",
    "        self.resnet2 = ResnetIdentityBlock(kernel_size=7, filters=[units, units, units])\n",
    "        \n",
    "        units *= squeeze_time\n",
    "        self.resnet3 = ResnetIdentityBlock(kernel_size=3, filters=[units, units, units])\n",
    "        \n",
    "        # Encoder lstm\n",
    "        self.enc_lstm = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call pyramidal LSTM neural network encoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: wave input\n",
    "        '''\n",
    "        x = self.feat_extract(inputs)\n",
    "        x = self.feat_dropout(x)\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet1(x)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        x = self.resnet3(x)\n",
    "        \n",
    "        # encoder output layer\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.enc_lstm(x)\n",
    "            \n",
    "        return fw_outputs, fw_state_h, fw_state_c\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        \n",
    "        Args:\n",
    "            outputs: outputs from LSTM\n",
    "            squeeze_time: time step one would like to squeeze in pyramidal LSTM\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "\n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * self.squeeze_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 11, 256)\n",
      "Encoder forward state h shape: (batch size, units) (4, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_DROPOUT_RATE = 0.2\n",
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  squeeze_time=3, \n",
    "                  units=64)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, fw_sample_state_h, fw_sample_state_c = encoder(example_input_batch)\n",
    "# sample_output, sample_state = encoder(example_input_batch)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  2560      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_1 (Res multiple                  54208     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_2 (Res multiple                  334656    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_3 (Res multiple                  1667520   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  852992    \n",
      "=================================================================\n",
      "Total params: 2,911,936\n",
      "Trainable params: 2,906,944\n",
      "Non-trainable params: 4,992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.V = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder for output phonemes\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 target_sz, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            target_sz: target size, total phoneme size in this case\n",
    "            embedding_dim: embedding dimension\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size\n",
    "            dropout_rate: dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.target_sz = target_sz\n",
    "        self.lstm_units = lstm_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        \n",
    "        # attention model\n",
    "        self.attention = LuongAttention(lstm_units)\n",
    "        \n",
    "        # decoder rnn            \n",
    "        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "    \n",
    "        # Fully-connected\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(target_sz, activation=\"softmax\")\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        call LSTM decoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: target output, following phoneme for wave data input in this case\n",
    "            enc_hidden_h: encoder hidden state h\n",
    "            enc_hidden_c: encoder hidden state c\n",
    "            enc_output: encoder outputs\n",
    "        '''\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the 2-layer LSTM (Decoder)\n",
    "        x, state_h, state_c = self.lstm1(x)\n",
    "\n",
    "        # dense layer before final predict output dense layer\n",
    "        x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        # output shape == (batch_size, phoneme size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, phoneme size) (4, 22)\n"
     ]
    }
   ],
   "source": [
    "DECODER_DROPOUT_RATE = 0.2\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, phoneme size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2816      \n",
      "_________________________________________________________________\n",
      "luong_attention_1 (LuongAtte multiple                  131841    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  656384    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  1430      \n",
      "=================================================================\n",
      "Total params: 808,919\n",
      "Trainable params: 808,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "'''\n",
    "Candidate optimizer:\n",
    "    1. Adam\n",
    "    2. Nadam\n",
    "    \n",
    "    the preformence of other optimizers are not good\n",
    "'''\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.math.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    if tf.is_tensor(real):\n",
    "        real = real.numpy()\n",
    "        \n",
    "    if tf.is_tensor(pred):\n",
    "        pred = pred.numpy()\n",
    "        \n",
    "    if np.isscalar(real):\n",
    "        if real == pred:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    length = len(real)\n",
    "    count_ = 0\n",
    "    for i in range(length):\n",
    "        if real[i] == pred[i]:\n",
    "            count_ += 1\n",
    "\n",
    "    return tf.math.reduce_sum(count_) / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  squeeze_time=3, \n",
    "                  units=64)\n",
    "\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/ResNet_LSTM'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            \n",
    "            target_id = targ[:, t]\n",
    "            loss += loss_function(target_id, predictions)\n",
    "            \n",
    "            predicted_id = tf.math.argmax(predictions, axis=1)\n",
    "            acc += accuracy_function(target_id, predicted_id)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "#     clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)  # clipping for avoiding gradient explosion\n",
    "#     optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    batch_accuracy = (acc / int(targ.shape[1]))\n",
    "\n",
    "    return batch_loss, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    inp = tf.expand_dims(inp, 0)\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']], 1)\n",
    "\n",
    "    for t in range(1, targ.shape[0]):\n",
    "        predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "        \n",
    "        target_id = targ[t]\n",
    "        loss += loss_function(target_id, predictions)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        acc += accuracy_function(target_id, predicted_id)\n",
    "        \n",
    "        if targ_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return loss / int(targ.shape[0]), acc / int(targ.shape[0])\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return loss / int(targ.shape[0]), acc / int(targ.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10  Batch: 1000  Loss: 0.4836  Accuracy: 0.4643  Time: 177s\n",
      "Epoch: 1/10  Batch: 2000  Loss: 0.4585  Accuracy: 0.3571  Time: 346s\n",
      "Epoch: 1/10  Batch: 3000  Loss: 0.4049  Accuracy: 0.4643  Time: 516s\n",
      "Epoch: 1/10  Batch: 4000  Loss: 0.4063  Accuracy: 0.4643  Time: 687s\n",
      "Epoch: 1/10  Batch: 4750  Loss: 0.3548  Accuracy: 0.3929  Time: 816s\n",
      "\n",
      "================================\n",
      "Epoch 1/10\n",
      "Accuracy: 0.3987  Loss: 0.5508  val_acc: 0.0969  val_loss: 3.5164\n",
      "Time taken for epoch 1: 14.99 min\n",
      "Total Time taken: 14.99 min\n",
      "================================\n",
      "\n",
      "Epoch: 2/10  Batch: 1000  Loss: 0.3634  Accuracy: 0.4643  Time: 173s\n",
      "Epoch: 2/10  Batch: 2000  Loss: 0.3753  Accuracy: 0.3571  Time: 346s\n",
      "Epoch: 2/10  Batch: 3000  Loss: 0.3811  Accuracy: 0.4286  Time: 519s\n",
      "Epoch: 2/10  Batch: 4000  Loss: 0.3748  Accuracy: 0.6071  Time: 695s\n",
      "Epoch: 2/10  Batch: 4750  Loss: 0.3642  Accuracy: 0.3929  Time: 841s\n",
      "\n",
      "================================\n",
      "Epoch 2/10\n",
      "Accuracy: 0.4408  Loss: 0.3624  val_acc: 0.1094  val_loss: 3.9975\n",
      "Time taken for epoch 2: 15.60 min\n",
      "Total Time taken: 30.59 min\n",
      "================================\n",
      "\n",
      "Epoch: 3/10  Batch: 1000  Loss: 0.3309  Accuracy: 0.3571  Time: 177s\n",
      "Epoch: 3/10  Batch: 2000  Loss: 0.3386  Accuracy: 0.4286  Time: 356s\n",
      "Epoch: 3/10  Batch: 3000  Loss: 0.3167  Accuracy: 0.4643  Time: 529s\n",
      "Epoch: 3/10  Batch: 4000  Loss: 0.3322  Accuracy: 0.5000  Time: 702s\n",
      "Epoch: 3/10  Batch: 4750  Loss: 0.3366  Accuracy: 0.4286  Time: 830s\n",
      "\n",
      "================================\n",
      "Epoch 3/10\n",
      "Accuracy: 0.4572  Loss: 0.3368  val_acc: 0.1516  val_loss: 3.7626\n",
      "Time taken for epoch 3: 15.18 min\n",
      "Total Time taken: 45.77 min\n",
      "================================\n",
      "\n",
      "Epoch: 4/10  Batch: 1000  Loss: 0.3193  Accuracy: 0.5000  Time: 172s\n",
      "Epoch: 4/10  Batch: 2000  Loss: 0.3329  Accuracy: 0.3929  Time: 344s\n",
      "Epoch: 4/10  Batch: 3000  Loss: 0.3367  Accuracy: 0.4643  Time: 516s\n",
      "Epoch: 4/10  Batch: 4000  Loss: 0.3703  Accuracy: 0.4286  Time: 688s\n",
      "Epoch: 4/10  Batch: 4750  Loss: 0.3311  Accuracy: 0.4286  Time: 817s\n",
      "\n",
      "================================\n",
      "Epoch 4/10\n",
      "Accuracy: 0.4548  Loss: 0.3454  val_acc: 0.1357  val_loss: 4.0813\n",
      "Time taken for epoch 4: 14.97 min\n",
      "Total Time taken: 60.73 min\n",
      "================================\n",
      "\n",
      "Epoch: 5/10  Batch: 1000  Loss: 0.3304  Accuracy: 0.4643  Time: 173s\n",
      "Epoch: 5/10  Batch: 2000  Loss: 0.3490  Accuracy: 0.3571  Time: 348s\n",
      "Epoch: 5/10  Batch: 3000  Loss: 0.3681  Accuracy: 0.4286  Time: 520s\n",
      "Epoch: 5/10  Batch: 4000  Loss: 0.3152  Accuracy: 0.3929  Time: 692s\n",
      "Epoch: 5/10  Batch: 4750  Loss: 0.3441  Accuracy: 0.4286  Time: 821s\n",
      "\n",
      "================================\n",
      "Epoch 5/10\n",
      "Accuracy: 0.4540  Loss: 0.3395  val_acc: 0.1407  val_loss: 3.4926\n",
      "Time taken for epoch 5: 14.89 min\n",
      "Total Time taken: 75.62 min\n",
      "================================\n",
      "\n",
      "Epoch: 6/10  Batch: 1000  Loss: 0.3317  Accuracy: 0.4643  Time: 173s\n",
      "Epoch: 6/10  Batch: 2000  Loss: 0.3403  Accuracy: 0.5000  Time: 347s\n",
      "Epoch: 6/10  Batch: 3000  Loss: 0.3200  Accuracy: 0.4643  Time: 519s\n",
      "Epoch: 6/10  Batch: 4000  Loss: 0.3225  Accuracy: 0.3929  Time: 691s\n",
      "Epoch: 6/10  Batch: 4750  Loss: 0.3351  Accuracy: 0.5714  Time: 821s\n",
      "\n",
      "================================\n",
      "Epoch 6/10\n",
      "Accuracy: 0.4560  Loss: 0.3316  val_acc: 0.1514  val_loss: 3.9224\n",
      "Time taken for epoch 6: 14.89 min\n",
      "Total Time taken: 90.51 min\n",
      "================================\n",
      "\n",
      "Epoch: 7/10  Batch: 1000  Loss: 0.3671  Accuracy: 0.3214  Time: 173s\n",
      "Epoch: 7/10  Batch: 2000  Loss: 0.3299  Accuracy: 0.5000  Time: 346s\n",
      "Epoch: 7/10  Batch: 3000  Loss: 0.3221  Accuracy: 0.5714  Time: 519s\n",
      "Epoch: 7/10  Batch: 3855  Loss: 0.3267  Accuracy: 0.4286  Time: 669s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4fd2d41d6175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoneme_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-d6c654939a9b>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, targ_tokenizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# forward algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdec_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_hidden_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden_c\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdec_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarg_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<start>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-0edf004a6db0>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# encoder output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mfw_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfw_state_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfw_state_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfw_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfw_state_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfw_state_c\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    891\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;31m# LSTM does not support constants. Ignore it during process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    796\u001b[0m       \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m       \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       init_state = get_initial_state_fn(\n\u001b[1;32m--> 606\u001b[1;33m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[0m\u001b[0;32m    607\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m       init_state = _generate_zero_filled_state(batch_size, self.cell.state_size,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2312\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2313\u001b[0m     return list(_generate_zero_filled_state_for_cell(\n\u001b[1;32m-> 2314\u001b[1;33m         self, inputs, batch_size, dtype))\n\u001b[0m\u001b[0;32m   2315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[1;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2750\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2751\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2752\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[1;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[0;32m   2766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2767\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2768\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2769\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2770\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[1;34m(unnested_state_size)\u001b[0m\n\u001b[0;32m   2763\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2765\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2767\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   2359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2360\u001b[0m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure it's a vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2361\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2362\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2363\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m   \"\"\"\n\u001b[1;32m--> 171\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow200\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m   3584\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   3585\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Fill\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3586\u001b[1;33m         name, _ctx._post_execution_callbacks, dims, value)\n\u001b[0m\u001b[0;32m   3587\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3588\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run eagerly will make tensorflow run step by step or else it will raise\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "# similar to Pytorch, which is a dynamic graph for deep learning\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 10\n",
    "TOLERANCE = 0.015\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # train the encoder-decoder model\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for inp, targ in dataset.take(STEP_PER_EPOCH):\n",
    "        batch += 1\n",
    "        \n",
    "        batch_loss, batch_accuracy = train_step(inp, targ, phoneme_tokenizer)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "            \n",
    "        print(\"Epoch: {}/{}  Batch: {}  Loss: {:.4f}  Accuracy: {:.4f}  Time: {:.0f}s\".\n",
    "              format(epoch, EPOCHS, batch, batch_loss.numpy(), batch_accuracy.numpy(), time.time()-epoch_start), \n",
    "              end=\"\\r\")\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            print()\n",
    "    print()\n",
    "    # saving (checkpoint) the model when total loss is less than 0.9\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    # validation process\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    for val_inp, val_targ in dataset_val.take(VAL_WAV_SIZE):\n",
    "        val_loss, val_acc = validate_step(val_inp, val_targ, phoneme_tokenizer)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "\n",
    "    # print out the epoch results\n",
    "    mean_total_acc = total_accuracy / STEP_PER_EPOCH\n",
    "    mean_total_loss = total_loss / STEP_PER_EPOCH\n",
    "    \n",
    "    mean_val_acc = total_val_acc / VAL_WAV_SIZE\n",
    "    mean_val_loss = total_val_loss / VAL_WAV_SIZE\n",
    "    \n",
    "    print(\"\\n================================\")\n",
    "    print(\"Epoch {}/{}\".format(epoch, EPOCHS))\n",
    "    print('Accuracy: {:.4f}  Loss: {:.4f}  val_acc: {:.4f}  val_loss: {:.4f}'.format(\n",
    "        mean_total_acc, \n",
    "        mean_total_loss, \n",
    "        mean_val_acc,\n",
    "        mean_val_loss))\n",
    "    print('Time taken for epoch {}: {:.2f} min'.format(epoch, (time.time() - epoch_start)/60))\n",
    "    print('Total Time taken: {:.2f} min'.format((time.time() - start)/60))\n",
    "    print(\"================================\\n\")\n",
    "    \n",
    "    if mean_total_loss < TOLERANCE and mean_val_acc > 0.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, max_input_len, max_output_len, tokenizer):\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    \n",
    "    \n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    predicted_ids = np.zeros(max_output_len, dtype=np.int32)\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            inputs=dec_input, \n",
    "            enc_hidden_h=dec_hidden_h, \n",
    "            enc_hidden_c=dec_hidden_c, \n",
    "            enc_output=enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        pred_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_ids[t] = pred_id\n",
    "\n",
    "        result += tokenizer.index_word[pred_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[pred_id] == '<end>':\n",
    "            return result, predicted_ids, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "    return result, predicted_ids, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "#     ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "    ax.set_yticklabels(\"\", fontdict=fontdict)\n",
    "    \n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, _, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "#     plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))\n",
    "    \n",
    "    attention_plot = attention_plot[:1, :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [972]\n",
      "Predicted translation: f ay n ay n ay \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "5\t--->\ts\n",
      "21\t--->\teh\n",
      "11\t--->\tv\n",
      "10\t--->\tah\n",
      "3\t--->\tn\n",
      "2\t--->\t<end>\n"
     ]
    }
   ],
   "source": [
    "# testing with test wave data\n",
    "index = [i for i in np.random.randint(len(wav_tensor_val), size=1)]\n",
    "print(f\"Index: {index}\")\n",
    "test_wave = wav_tensor_val[index]\n",
    "test_phoneme = phoneme_tensor_val[index, :]\n",
    "\n",
    "max_input_length = sample_output.shape[1]\n",
    "max_output_length = test_phoneme.shape[1] - 1 # minus the '<start>' since it won't show up in prediction\n",
    "\n",
    "translate(test_wave, max_input_length, max_output_length, phoneme_tokenizer)\n",
    "for tensor in test_phoneme:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_tensor = np.array([label for label in label_tensor])\n",
    "label_tensor_val = np.array([label for label in label_tensor_val])\n",
    "le = LabelEncoder().fit(label_tensor)\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Classes integer: {le.transform(le.classes_)}\")\n",
    "\n",
    "labels = le.transform(label_tensor)\n",
    "# labels_val = le.transform(label_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_iter = len(wav_tensor)\n",
    "pred_id = np.zeros((total_iter, max_output_length))\n",
    "for i in range(total_iter):\n",
    "    _, IDs, _ = predict(wav_tensor[i], max_input_length, max_output_length, phoneme_tokenizer)\n",
    "    pred_id[i] = IDs.astype(np.int32)\n",
    "    \n",
    "    time_consumed = (time.time() - start) / 60\n",
    "    print(\"predicting {}th wave tensor - time {:.2f} min\".format(i+1, time_consumed), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_iter_val = len(wav_tensor_val)\n",
    "pred_id_val = np.zeros((total_iter_val, max_output_length))\n",
    "for i in range(total_iter_val):\n",
    "    _, IDs, _ = predict(wav_tensor_val[i], max_input_length, max_output_length, phoneme_tokenizer)\n",
    "    pred_id_val[i] = IDs.astype(np.int32)\n",
    "    \n",
    "    time_consumed = (time.time() - start) / 60\n",
    "    print(\"predicting {}th wave tensor - time {:.2f} min\".format(i+1, time_consumed), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_output_length = 6\n",
    "phoneme2label = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(phoneme_tokenizer.word_index)+1, 256), \n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "phoneme2label.compile(loss='sparse_categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "phoneme2label.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input:\\n {pred_id}\")\n",
    "print()\n",
    "print(f\"Target:\\n {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = phoneme2label.fit(pred_id, \n",
    "                            labels, \n",
    "                            epochs=5, \n",
    "                            validation_split=0.2, \n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccLossPlot(TrainHistory):    \n",
    "    acc = TrainHistory.history['accuracy']\n",
    "    val_acc = TrainHistory.history['val_accuracy']\n",
    "\n",
    "    loss = TrainHistory.history['loss']\n",
    "    val_loss = TrainHistory.history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AccLossPlot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels\n",
    "pred_label = np.argmax(phoneme2label.predict(pred_id_val), 1)\n",
    "\n",
    "# plot the confusion\n",
    "pred_label = le.inverse_transform(pred_label)\n",
    "label_tensor_val = label_tensor_val.reshape(label_tensor_val.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(y_true=label_tensor_val, \n",
    "                      y_pred=pred_label, \n",
    "                      labels=None, \n",
    "                      true_labels=None, \n",
    "                      pred_labels=None, \n",
    "                      title=\"Linear Discriminant Analysis Confusion Matrix\", \n",
    "                      normalize=False, \n",
    "                      hide_zeros=False, \n",
    "                      hide_counts=False, \n",
    "                      x_tick_rotation=0, \n",
    "                      ax=None, \n",
    "                      figsize=(10, 10), \n",
    "                      cmap='Blues', \n",
    "                      title_fontsize='large', \n",
    "                      text_fontsize='medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data collected from classmates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "test_audio_path = \"testing\"\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "test_reader = WaveReader(path=test_audio_path, \n",
    "                         sample_rate=SAMPLE_RATE, \n",
    "                         padding_type=None, \n",
    "                         read_size=None)\n",
    "\n",
    "wav_array_test, label_array_test, total_test, loss_test = test_reader.read(labels=phoneme_dataframe.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "test_wave = preprocesser.simulate_wave(wav_array_test)\n",
    "test_label = preprocesser.simulate_label(label_array_test)\n",
    "test_phoneme = preprocesser.simulate_phoneme(labels=test_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {test_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {test_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcced_test_wave = mfcc_applier.apply(test_wave, sample_rate=SAMPLE_RATE)\n",
    "print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_test_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_tensor_test = tf.convert_to_tensor(mfcced_test_wave, dtype=tf.float32)\n",
    "\n",
    "# must use the same tokenizer\n",
    "phoneme_tensor_test = phoneme_tokenizer.texts_to_sequences(test_phoneme)\n",
    "phoneme_tensor_test = tf.keras.preprocessing.sequence.pad_sequences(phoneme_tensor_test, padding='post')\n",
    "\n",
    "print(\"Input Shape: {}\".format(wav_tensor_test.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor_test.shape))\n",
    "\n",
    "for tensor in phoneme_tensor_test[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with test wave data\n",
    "# index = [i for i in np.random.randint(len(wav_tensor_test), size=1)]\n",
    "index = [14]\n",
    "print(f\"Index: {index}\")\n",
    "test_wave = wav_tensor_test[index]\n",
    "test_phoneme = phoneme_tensor_test[index, :]\n",
    "\n",
    "max_input_length = sample_output.shape[1]\n",
    "max_output_length = test_phoneme.shape[1] - 1 # minus the '<start>' since it won't show up in prediction\n",
    "\n",
    "translate(test_wave, max_input_length, max_output_length, phoneme_tokenizer)\n",
    "for tensor in test_phoneme:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_iter = len(wav_tensor_test)\n",
    "pred_id_test = np.zeros((total_iter, max_output_length))\n",
    "for i in range(total_iter):\n",
    "    _, IDs, _ = predict(wav_tensor_test[i], max_input_length, max_output_length, phoneme_tokenizer)\n",
    "    pred_id_test[i] = IDs.astype(np.int32)\n",
    "    \n",
    "    time_consumed = (time.time() - start) / 60\n",
    "    print(\"predicting {}th wave tensor - time {:.2f} min\".format(i+1, time_consumed), end=\"\\r\")\n",
    "\n",
    "# time_consumed = (time.time() - start) / 60\n",
    "# print(\"\\nTime Consumed: {:.2f} min\".format(time_consumed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict label from phonemes\n",
    "pred_label_test = np.argmax(phoneme2label.predict(pred_id_test), 1)\n",
    "pred_label_test = le.inverse_transform(pred_label_test)\n",
    "\n",
    "# true test label\n",
    "test_label = np.array([label for label in test_label])\n",
    "test_label = test_label.reshape(test_label.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(pred_label_test)):\n",
    "    if pred_label_test[i] == test_label[i]:\n",
    "        count += 1\n",
    "        \n",
    "count /= len(pred_label_test)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the confusion\n",
    "plot_confusion_matrix(y_true=test_label, \n",
    "                      y_pred=pred_label_test, \n",
    "                      labels=None, \n",
    "                      true_labels=None, \n",
    "                      pred_labels=None, \n",
    "                      title=\"Linear Discriminant Analysis Confusion Matrix\", \n",
    "                      normalize=False, \n",
    "                      hide_zeros=False, \n",
    "                      hide_counts=False, \n",
    "                      x_tick_rotation=0, \n",
    "                      ax=None, \n",
    "                      figsize=(10, 10), \n",
    "                      cmap='Blues', \n",
    "                      title_fontsize='large', \n",
    "                      text_fontsize='medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### figure out single speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wave(samples, sample_rate=16000):\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.set_title('Raw wave of ' + '../input/train/audio/yes/0a7c2a8d_nohash_0.wav')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 13\n",
    "\n",
    "test_wave = wav_array_test[index]\n",
    "test_label = label_array_test[index]\n",
    "test_phoneme = \"<start> \" + preprocesser.phoneme_translator(test_label) + \" <end>\"\n",
    "\n",
    "mfcced_test_wave = mfcc_applier.apply(test_wave[np.newaxis, :], sample_rate=SAMPLE_RATE)\n",
    "wav_tensor_test = tf.convert_to_tensor(mfcced_test_wave, dtype=tf.float32)\n",
    "\n",
    "phoneme_tensor_test = phoneme_tokenizer.texts_to_sequences(test_phoneme)\n",
    "phoneme_tensor_test = tf.keras.preprocessing.sequence.pad_sequences(phoneme_tensor_test, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = sample_output.shape[1]\n",
    "max_output_length = 7 - 1 # minus the '<start>' since it won't show up in prediction\n",
    "\n",
    "print(\"Real Phonemes: {}\".format(test_phoneme))\n",
    "plot_wave(test_wave)\n",
    "translate(wav_tensor_test[0], max_input_length, max_output_length, phoneme_tokenizer)\n",
    "\n",
    "ipd.Audio(test_wave, rate=SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
