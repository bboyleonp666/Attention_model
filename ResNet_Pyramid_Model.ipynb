{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.0.0\n",
      "\n",
      "GPU available: True\n",
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reference: \n",
    "    https://arxiv.org/abs/1508.01211\n",
    "    https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    https://github.com/jameslyons/python_speech_features\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers\n",
    "    \n",
    "data source: \n",
    "    https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import time\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"CUDA enabled: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa # for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile # for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveReader:\n",
    "    def __init__(self, path, sample_rate, padding_type, read_size):\n",
    "        '''\n",
    "        Args:\n",
    "            path: train path containing directory which one would like to load\n",
    "            sample_rate: sample rate for reading .wav file\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "            read_size: size that one would like to read\n",
    "        '''\n",
    "        \n",
    "        self.path = path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.padding_type = padding_type\n",
    "        self.read_size = read_size\n",
    "\n",
    "    def read(self, labels=None):\n",
    "        '''\n",
    "        read all the data under the labels(directories) one select\n",
    "        \n",
    "        Args:\n",
    "            labels: labels(directories) one would like to load\n",
    "                    None means read all the directories under that directory\n",
    "        '''\n",
    "        print(\"LABEL\\tTOTAL\\tREAD\\tSAVED\\t<1s COUNT\")\n",
    "        print(\"-----\\t-----\\t----\\t-----\\t---------\")\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f for f in os.listdir(path) if os.path.isdir(path + \"\\\\\" + f)]\n",
    "            \n",
    "        elif type(labels) == str:\n",
    "            samples, total_wave_count, total_wave_read, total_loss_count = self.read_dir(dir_name=labels)\n",
    "            sample_labels = np.repeat(labels, total_wave_read)\n",
    "            \n",
    "            print(\"\\nMISSION COMPELTE!!!\")\n",
    "            return samples, sample_labels, total_wave_count, total_loss_count\n",
    "                    \n",
    "        label_len = len(labels)\n",
    "        total_wave_count = np.zeros(label_len, dtype=np.int32)\n",
    "        total_wave_read = np.zeros(label_len, dtype=np.int32)\n",
    "        total_loss_count = np.zeros(label_len, dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for i, lab in enumerate(labels):\n",
    "            samp, total_wave_count[i], total_wave_read[i], total_loss_count[i] = self.read_dir(dir_name=lab)\n",
    "            \n",
    "            if i == 0:\n",
    "                samples = samp\n",
    "                sample_labels = np.repeat(lab, total_wave_read[i])\n",
    "            else:\n",
    "                samples = np.concatenate((samples, samp), axis=0)\n",
    "                sample_labels = np.concatenate((sample_labels, np.repeat(lab, total_wave_read[i])), axis=None)\n",
    "        \n",
    "        print(\"\\nMISSION COMPELTE!!!\")\n",
    "        return samples, sample_labels, total_wave_count, total_loss_count\n",
    "    \n",
    "    def read_dir(self, dir_name):\n",
    "        '''\n",
    "        read one directory of given directory name\n",
    "        \n",
    "        Args:\n",
    "            dir_name: directory name\n",
    "        '''\n",
    "        dir_path = os.path.join(self.path, dir_name)\n",
    "        wave_files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n",
    "        total_wave_files = len(wave_files)\n",
    "\n",
    "        if self.read_size is not None:\n",
    "            wave_files_read = self.read_size\n",
    "        else:\n",
    "            wave_files_read = total_wave_files\n",
    "\n",
    "        samples = np.zeros((wave_files_read, self.sample_rate))\n",
    "        less_than_1s_count = 0\n",
    "        num_of_file_read = 0\n",
    "        for i, wav_file in enumerate(wave_files):\n",
    "            wave_file_path = os.path.join(dir_path, wav_file)\n",
    "            samp, _ = librosa.load(wave_file_path, sr=self.sample_rate)\n",
    "\n",
    "            pad_size = self.sample_rate - len(samp)\n",
    "            if pad_size > 0:\n",
    "                less_than_1s_count += 1\n",
    "                if self.padding_type is None:\n",
    "                    # None: than skip this wave file\n",
    "                    continue\n",
    "\n",
    "                elif self.padding_type == \"white_noise\":\n",
    "                    # white_noise: pad white noise data behind\n",
    "                    padding = np.random.normal(0, 0.02, pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # zero: pad zeros behind\n",
    "                    padding = np.zeros(pad_size)\n",
    "                    samples[num_of_file_read, :] = np.concatenate((samp, padding), axis=None)\n",
    "                    num_of_file_read += 1\n",
    "            else:\n",
    "                samples[num_of_file_read, :] = samp\n",
    "                num_of_file_read += 1\n",
    "\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(dir_name, \n",
    "                                              total_wave_files, \n",
    "                                              i+1, \n",
    "                                              num_of_file_read, \n",
    "                                              less_than_1s_count), end=\"\\r\")\n",
    "            \n",
    "            if num_of_file_read == wave_files_read:\n",
    "                break\n",
    "                \n",
    "        print()\n",
    "\n",
    "        return samples, total_wave_files, wave_files_read, less_than_1s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL\tTOTAL\tREAD\tSAVED\t<1s COUNT\n",
      "-----\t-----\t----\t-----\t---------\n",
      "zero\t2376\t2160\t2000\t160\n",
      "one\t2370\t2262\t2000\t262\n",
      "two\t2373\t2222\t2000\t222\n",
      "three\t2356\t2207\t2000\t207\n",
      "four\t2372\t2201\t2000\t201\n",
      "five\t2357\t2185\t2000\t185\n",
      "six\t2369\t2164\t2000\t164\n",
      "seven\t2377\t2194\t2000\t194\n",
      "eight\t2352\t2232\t2000\t232\n",
      "nine\t2364\t2178\t2000\t178\n",
      "\n",
      "MISSION COMPELTE!!!\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "train_audio_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train\", \"audio\")\n",
    "phoneme_path = os.path.join(os.getcwd(), \"Phonemes\")\n",
    "phoneme_dataframe = pd.read_csv(os.path.join(phoneme_path, \"phonemes.csv\"))\n",
    "\n",
    "reader = WaveReader(path=train_audio_path, \n",
    "                    sample_rate=SAMPLE_RATE, \n",
    "                    padding_type=None, \n",
    "                    read_size=2000)\n",
    "\n",
    "wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words)\n",
    "# wav_array, label_array, total, loss = reader.read(labels=phoneme_dataframe.words[0])\n",
    "\n",
    "# print(\"\\nCheck the existence of NaN and Inf\")\n",
    "# print(f\"NaN Number: {np.sum(np.isnan(wav_array))}\")\n",
    "# print(f\"Inf Number: {np.sum(np.isinf(wav_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, create_size, min_sz=6, max_sz=8, padding_type=\"zero\"):\n",
    "        '''\n",
    "        Args:\n",
    "            create_size: size of binding wave one would like to create\n",
    "            min_sz: minimum size of wave data\n",
    "            max_sz: maximum size of wave data\n",
    "            padding_type: padding for .wav data length less than 1 second\n",
    "        '''\n",
    "        self.create_size = create_size\n",
    "        self.min_sz = min_sz\n",
    "        self.max_sz = max_sz\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "    def simulate_wave(self, waves):\n",
    "        '''\n",
    "        method for simulating wave inputs\n",
    "        which will concatenate audio inputs for building longer audio dataset\n",
    "        \n",
    "        Args:\n",
    "            waves: input wave data\n",
    "        '''\n",
    "        # get picker for combining waves and labels(phonemes)\n",
    "        self.wave_shape = waves.shape\n",
    "        self.pickers = self.get_picker()\n",
    "        \n",
    "        print(\"Wave Data Simulation ... \", end=\"\")\n",
    "        \n",
    "        binded_length = self.wave_shape[1]*self.max_sz\n",
    "        simu_wave = np.zeros((self.create_size, binded_length))\n",
    "        \n",
    "        \n",
    "        for i, picker in enumerate(self.pickers):        \n",
    "            tmp_simu_wave = np.array([waves[p] for p in picker]).flatten()\n",
    "            \n",
    "            pad_size = binded_length - len(tmp_simu_wave)\n",
    "            if pad_size > 0:\n",
    "                if self.padding_type == \"white_noise\":\n",
    "                    # padding white noise\n",
    "                    padding = np.random.normal(0, 0.02, size=pad_size)\n",
    "\n",
    "                elif self.padding_type == \"zero\":\n",
    "                    # padding zeros\n",
    "                    padding = np.zeros(pad_size)\n",
    "\n",
    "                simu_wave[i] = np.concatenate((tmp_simu_wave, padding), axis=None)\n",
    "                \n",
    "            else:\n",
    "                simu_wave[i] = tmp_simu_wave\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_wave\n",
    "    \n",
    "    def get_picker(self):\n",
    "        '''\n",
    "        picker stands for index pick\n",
    "        this is for combining audio data with decided minimum and maximum size\n",
    "        '''\n",
    "        size = np.random.randint(low=self.min_sz, \n",
    "                                 high=self.max_sz+1, \n",
    "                                 size=self.create_size)\n",
    "\n",
    "        picker = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, s in enumerate(size):\n",
    "            picker[i] = np.random.choice(self.wave_shape[0]-1, size=self.max_sz, replace=False)[:s]\n",
    "            \n",
    "        return picker\n",
    "\n",
    "    def simulate_label(self, labels):\n",
    "        '''\n",
    "        method for simulating label inputs which will concatenate labels following simulated waves\n",
    "        \n",
    "        Args:\n",
    "            labels: input labels following with audio dataset\n",
    "        '''\n",
    "        print(\"Label Simulation ... \", end=\"\")\n",
    "        \n",
    "        simu_label = np.zeros(self.create_size, dtype=np.object)\n",
    "        for i, picker in enumerate(self.pickers):\n",
    "            simu_label[i] = np.array([labels[p] for p in picker])\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_label\n",
    "\n",
    "    def simulate_phoneme(self, labels, label_dict, phoneme_dict):\n",
    "        '''\n",
    "        method for sumulating phoneme inputs\n",
    "        which will concatenate audio phonemes with labels we concated by simulate_label()\n",
    "        \n",
    "        Args:\n",
    "            labels: labels that one would like to transfer\n",
    "            label_dict: label dictionary\n",
    "            phoneme_dict: phoneme dictionary\n",
    "        '''\n",
    "        print(\"Phoneme Simulation... \", end=\"\")\n",
    "        \n",
    "        self.label_dict = label_dict\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "\n",
    "        simu_phoneme = np.empty(self.create_size, dtype=np.object)\n",
    "        for i, label in enumerate(labels):\n",
    "            simu_phoneme[i] = \" \".join([self.phoneme_translator(lab) for lab in label])\n",
    "            simu_phoneme[i] = \"<start> \" + simu_phoneme[i] + \" <end>\"\n",
    "            \n",
    "        print(\"Done\")\n",
    "        return simu_phoneme\n",
    "\n",
    "    def phoneme_translator(self, input_label):\n",
    "        '''\n",
    "        translate labels to phoneme if simulate_phoneme is called\n",
    "        \n",
    "        Args:\n",
    "            input_label: label that one would like to transfer into phonemes\n",
    "        '''\n",
    "        for i, label in enumerate(self.label_dict):\n",
    "            if input_label == label:\n",
    "                return self.phoneme_dict[i]\n",
    "            \n",
    "    def tokenize(self, phoneme):\n",
    "        '''\n",
    "        with tensorflow we can simply apply Tokenizer for text(in our case, phoneme)\n",
    "        to generate phoneme outputs\n",
    "        \n",
    "        Args:\n",
    "            phoneme: phoneme string with '<start>' and '<end>'\n",
    "        '''\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "        tokenizer.fit_on_texts(phoneme)\n",
    "        tensor = tokenizer.texts_to_sequences(phoneme)\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, tokenizer\n",
    "\n",
    "    def show_convert(self, tensor, tokenizer):\n",
    "        '''\n",
    "        showing case of tokenized word according to its index\n",
    "        \n",
    "        Args:\n",
    "            tensor: phoneme tensor\n",
    "            tokenizer: phoneme tokenizer\n",
    "        '''\n",
    "        print(\"\\nTOKEN\\t--->\\tWORDS\")\n",
    "        print(\"=======================\")\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(\"{}\\t--->\\t{}\".format(t, tokenizer.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave Data Simulation ... Done\n",
      "Label Simulation ... Done\n",
      "Phoneme Simulation... Done\n",
      "\n",
      "Example Label Display: ['seven']\n",
      "Example Phoneme Display: <start> S EH V AH N <end>\n"
     ]
    }
   ],
   "source": [
    "CREATE_SIZE = 20000\n",
    "MIN_BINDING_SIZE = 1\n",
    "MAX_BINDING_SIZE = 1\n",
    "\n",
    "preprocesser = Preprocesser(create_size=CREATE_SIZE, \n",
    "                            min_sz=MIN_BINDING_SIZE, \n",
    "                            max_sz=MAX_BINDING_SIZE, \n",
    "                            padding_type=\"white_noise\")\n",
    "\n",
    "simu_wave = preprocesser.simulate_wave(wav_array)\n",
    "simu_label = preprocesser.simulate_label(label_array)\n",
    "simu_phoneme = preprocesser.simulate_phoneme(labels=simu_label, \n",
    "                                             label_dict=phoneme_dataframe.words.values, \n",
    "                                             phoneme_dict=phoneme_dataframe.phonemes.values)\n",
    "\n",
    "print(f\"\\nExample Label Display: {simu_label[0]}\")\n",
    "print(f\"Example Phoneme Display: {simu_phoneme[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [10929]\n",
      "[array(['two'], dtype='<U3')]\n",
      "['<start> T UW <end>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,UklGRiR9AABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACABAAZGF0YQB9AADz/+b/2f/m/+b/5v/m/+b/8/8AAA0A8//m/xoAGgDz/xoAJwDz//P/DQDz/+b/8//z//P/AADz/w0AGgDz/+b/DQAaAPP/8//z/+b/GgAnAPP/8/8NAA0AGgAaABoA8//z/xoAGgDm//P/AAANABoAGgDz/w0AGgDm//P/DQAaAA0ADQDz//P/DQAAAA0AGgAaABoAGgAaABoAGgDz/9n/DQAaAA0A8/8AAPP/AAAaABoA8//M/+b/2f/m/w0A8/8NACcADQDm/8z/8/8NABoAGgAaAA0A8//z/w0AGgAaAOb/5v/z/wAA8/8AAA0ADQAaABoAGgAaAAAA8//z/w0AGgANAA0ADQAAAA0AAAAAACcAGgDm//P/8//m/w0AAAANACcAAADz/+b/AAANAPP/JwAaABoAJwAAAPP/5v/z//P/DQAaABoAAADz/wAAGgAnABoAGgAaAA0AAAAAAA0AJwAnAA0AGgAaAPP/5v8aACcADQAaACcAGgAaAPP/8/8nABoAGgAnAA0A8//m//P/JwAaAPP/8/8NAPP/8//m/wAAAAANABoA5v8aAA0A8/8aABoADQAaAA0A8/8NAAAA8/8NABoA8//m//P/8/8AAPP/2f/m/xoA2f8AABoA2f/Z//P/8//z/w0AGgDZ/xoAGgDm/xoAGgDz/9n/5v/m/+b/8//m/+b/8//Z/8z/GgAnABoAJwANAAAAGgDz/w0AGgDm//P/GgAaAOb/AAAaAPP/DQAaAA0AGgAnAAAA5v8nABoA8/8NAPP/5v/z//P/AAAnAA0ADQAAAOb/AAANAAAADQAnAA0A8/8NAPP/8/8NABoAAAANAA0A8//z/+b/8//m/w0AAADm/wAA8//m/+b/DQAaAA0ADQAAAPP/8/8AAA0ADQAaABoA8//m/xoAGgDz//P/AAAAAA0AGgAaAAAAAADz/xoAGgANAA0A8//z//P/8/8NACcAGgDz/wAA8//m/w0AJwAaABoADQAAAPP/5v8aABoA5v8AAAAA5v8AAAAADQANABoAGgAAAAAA8//M/+b/AADm/w0AGgDm/9n/5v/m/wAANAANAOb/DQAAAMz/2f8NAPP/2f/z/+b/2f/Z//P/8/8NABoA8/8NABoA8//m/wAA8//z/xoADQDz/wAAGgANAAAADQANABoAAAANACcADQAAABoAAAANACcA8//m/w0A8//z/xoAJwAAABoAJwDz//P/DQDz/wAAGgAaAPP/GgANAAAA8//m/+b/AAANAPP/GgAaAPP/GgANAPP/JwAAAOb/JwAaAAAAGgAaAAAA8/8aACcA5v8aACcA8/8nAAAA5v8nACcAGgAaABoAAADz/w0A8/8NABoA8//z/w0AAADz//P/AAAaABoAAADz/xoAJwAAAA0AJwAaABoADQAAAA0AAAANAAAADQAAAA0AAAAAAAAADQDz/+b/DQAAAAAADQAAAA0AJwDZ/+b/DQDm/8z/AAANAOb/GgAnANn/2f8NAPP/zP8AABoAJwANAAAA8//m/w0ADQDm/wAADQANANn/2f8aAPP/DQA0AA0AAADz/w0ADQAAAA0AGgDz//P/GgAnAOb/8/8aAPP/8/8nAA0A8/8AAA0AJwAAABoADQDz/xoAGgAaABoAJwAaAPP/DQAaAOb/AAAaAPP/8//z/xoAGgDz/xoAGgANAPP/DQDz/+b/AAANAA0A5v8NAPP/5v8NAA0ADQANABoAGgDz//P/GgAnAPP/5v8aABoA5v8NABoA8/8aABoA8//m//P/8//z//P/AADz//P/GgANAPP/GgAaAPP/8/8aABoA8//z//P/5v/m//P/AAAAAAAAGgAaAA0A8/8aABoA8/8aABoA5v/z/w0A8//z//P/GgAAAPP/DQANAAAA8//m//P/5v/m/w0AAADZ/xoA8//Z/ycAGgAaABoAGgAaABoAGgANABoAAADZ/+b/8//z/+b/2f8AAOb/8/8aAOb/5v8aABoA5v/m/w0A8//z/w0AGgAaAAAAGgDm//P/JwAaAA0AJwAnAA0AGgAnABoA8/8NABoA8//z//P/DQDz/+b/GgAaAOb/8/8nAA0ADQAaAA0A5v/z/xoAAADz//P/GgDz//P/GgDz/wAADQAaAPP/8/8NAPP/8/8aABoAAADm/ycAGgDz/w0AAADz//P/AAAnABoA8/8aAPP/8/8nABoA8//z/ycAGgDz/wAAGgAaAAAADQAaAPP/8/8NAAAA8/8aAA0A8/8aADQAAAAAACcAGgANAAAAAADZ/+b/AADz//P/8/8AAPP/5v8AABoADQDz/xoAGgDm//P/8//Z/wAAGgDm/+b/DQAAAA0AAAANABoA2f/m/xoAGgDm/9n/DQAaAOb/GgAaANn/5v8aAAAA2f8aACcAzP8NACcADQDz//P/5v/z//P/5v/z/w0AGgANAPP/GgAaAPP/8//z/xoAGgANABoAAADz//P/8//z//P/AADz//P/8//z//P/8/8aACcAGgAaAA0AGgDz/+b/GgDz/w0AGgDz//P/8/8NAA0A8/8AAA0ADQDm/w0ADQDm/w0ADQDz/w0AGgAAAPP/AAAaACcAJwANAPP/5v8NAA0A5v/m//P/DQAaABoA8//z/+b/8/8aAPP/5v8AAPP/AAAaABoADQAAACcAJwAaABoAJwAaABoAGgANAOb/8/8aAOb/5v/z//P/AAANAAAAGgAaAA0AJwANABoADQAAAA0A8/8NAPP/5v/z/wAAGgDz/xoAGgANABoAGgAaANn/5v8NANn/DQAaABoA8//Z/9n/zP/m/+b/2f/z/8z/8/8NAOb/DQAnABoA8//m//P/DQANAA0AGgDm/+b/GgAaAPP/GgAaAPP/AAANAPP/5v8AAA0AAAAaACcADQAAAA0AJwANAAAAGgDz//P/8//m/w0A8//z//P/5v8NABoADQAaABoAGgAaAAAA5v/z/w0A8/8aAAAA8/8NAPP/8//z//P/8//z//P/8/8aABoAGgAaAPP/DQAAAAAA8//z/xoAAADz/wAA8//z/xoA8/8AAAAADQANAPP/DQAaABoAAADm//P/8/8AABoAGgANAAAAGgAnABoAJwAaABoADQANAA0A8//z/xoAGgDz/xoAJwDm/wAADQDz//P/8//z/+b/8/8AAA0AGgDz/+b/2f/Z/9n/5v/m/9n/2f8NABoAJwAaABoAGgAAAA0A5v/M/9n/2f/m//P/5v/z/ycA2f/z/xoADQANAPP/DQAaABoAGgANACcAJwAaABoA8//z/+b/5v/z/+b/8//z/wAAGgAaABoA8//m/wAA5v/z/w0AGgAaABoA8//z/xoAAADm//P/8//z/xoAGgANAPP/8//z/wAA8//m//P/8/8aAA0ADQDz//P/DQDz/+b/5v/m/+b/5v8NAPP/5v/m/+b/5v/z/wAAGgAaAPP/5v/z//P/5v/m/+b/5v/Z//P/GgDm//P/DQDz/xoAGgAaABoAGgAaAPP/DQAaABoAGgAaAAAA8/8nABoA8/8AAOb/5v8aABoADQAAAPP/8//m/wAA8//z//P/5v8NABoADQAaABoADQAAAPP/DQDz/+b/DQAaABoAGgAnABoA8/8NAAAA2f/Z/9n/8//Z/9n/zP/Z/xoA8//m//P/zP/Z/+b/DQDz/9n/GgAaABoA8//z/xoADQDz//P/8/8NABoADQANAPP/8/8AABoADQAaABoAGgANAAAADQAAAOb/8//z/+b/8//z/xoAGgDz/xoAJwANABoAGgAaABoA8//z/w0AGgAnABoA5v/z//P/8/8NAPP/GgAaABoAJwAaABoADQAaABoA8//z//P/DQAAAPP/8//z//P/8//z/wAA8/8AAA0A8//z/wAAGgAaAPP/8/8AAPP/8/8AABoAAADz/xoAGgAAAPP/8/8aAAAA8/8aABoAGgAaABoA8//z//P/5v/m//P/GgAnAPP/8//z/wAAGgDz//P/GgAaAPP/DQAaAPP/8//z//P/8//z//P/GgAaAA0A8//z/xoADQDz/w0ADQAaAA0A8/8NABoADQDm/xoA8//Z//P/5v/m/+b/2f/Z/+b/5v/Z/wAAGgDm//P/GgAAAPP/AAAaABoAGgAaABoA8//z//P/DQAAAPP/GgAaAA0AAAANAAAA8/8AAA0AGgAaAPP/8/8AABoAGgDz/xoA8//m//P/5v8NAPP/5v/z//P/DQDz//P/DQAAAPP/8//z//P/8/8NABoADQDm//P/DQDm/wAADQDm//P/DQANAA0AAAANABoAJwAaAPP/GgANAPP/AAANAA0A8/8AAAAADQAaAA0AGgAaAPP/DQANAPP/8//z//P/8//z/w0AJwAaAPP/8/8AAPP/5v/z/xoAGgAAAOb/AAAaAA0ADQDz//P/8//m/w0ADQAAAPP/8//z/xoAAADm/xoA8//z/w0AJwAaANn/GgAaANn/2f/Z/9n/5v/M/+b/DQDz//P/2f/M/9n/5v8NABoAGgANABoAGgDz/9n/5v/z/9n/5v/z/w0AGgANAA0ADQANABoAJwANAPP/5v/z/w0A5v8NABoAAAAaAPP/8/8AAA0AAADz/xoAGgAAABoAJwAaABoA8/8aABoAGgANAPP/GgDz/xoAGgAaABoAJwANAAAADQAAABoAAADm//P/5v8AAAAAAAAAAOb/AAANAPP/8//z/+b/8//z//P/8/8NACcADQAAABoAJwANAPP/5v8AABoAGgAaAA0AGgANAPP/8//z/w0AAAANABoADQAAAPP/8//m/wAA8//m//P/8//z/wAAGgAaAA0AAADz//P/5v8AABoAGgAaAA0ADQAaABoAGgANABoAGgAaABoAGgAaAA0AAAANAA0AGgDm//P/GgDZ/8z/zP/Z/9n/5v/Z//P/GgAaABoAGgAAANn/GgAaANn/5v/z/9n/8/8aACcAGgDZ//P/DQAaAAAAAAAnAPP/8/8AAAAAGgDz//P/AAANAAAAGgAaAPP/GgAaABoAGgAaABoAAAAAAPP/8/8NABoAGgAnABoAAAAaACcADQDz/+b/DQAaAPP/GgAaABoADQDz/xoA5v/m//P/5v/z/+b/8/8NABoADQANABoADQDz//P/8//m/wAAGgANAAAAGgAaACcAGgANABoADQAaAAAADQANAA0AGgAaAA0A5v8aABoA8//z//P/8//z/wAA8//z//P/5v/z/wAA8//m//P/8//m//P/8//m/wAAAAANAA0ADQANAPP/8//z//P/DQAaABoAGgAnABoADQAaAA0AGgAaANn/2f/Z/9n/2f/M/+b/DQAAAMz/5v8NAPP/8//Z/9n/zP/m/xoADQAaANn/2f8aABoADQANAPP/5v8NAA0AGgANAPP/AADz/w0AAADm//P/5v8AABoAAADm//P/GgAaAA0A8//z/w0A8//z/xoAJwAaABoAGgANACcADQDz/w0AAAANABoAGgAaABoAGgAaAAAA8//z//P/8//z/+b/8/8aABoAJwAaAAAAJwAnAA0AGgANAAAA8//z//P/8/8NABoAJwAnAA0AAAAaABoA8//z/xoAGgAAAPP/8//z//P/8//z/+b/8//z/xoAGgDz//P/8//z//P/DQAAAPP/8//z//P/AAAaABoAGgAnABoAGgAaAAAA8//z/xoAAADz/wAAGgAaABoAGgAaABoADQDz//P/8//m//P/8/8aAA0A2f/m/w0A5v8NABoADQAnAA0A8/8AANn/zP/m/9n/2f/Z/+b/2f/m/xoAJwAaANn/AAANAOb/DQAnABoA8/8NAAAA2f/z//P/8/8nABoA5v8aAAAA8/8AAPP/5v/z//P/5v/z/xoAGgAaACcAGgANAA0AAADz/wAAGgAaAAAA8//z/xoAGgDz//P/GgAaABoAGgAaABoAGgAaABoAGgAaAA0AGgANAPP/DQAaAA0AAADm/+b/AADm/+b/5v/m/+b/8//z//P/AAANABoAGgAaAAAA8//z/wAA8//m/w0AGgANABoA5v/m/w0A8//z//P/8//z//P/8//z//P/8//z//P/8/8aABoAGgANAPP/8/8AAPP/8/8AABoAGgANAA0AGgDz/+b/8/8NACcAGgANABoAGgAaAAAA8//z//P/DQANABoAJwANAPP/8//m/w0ADQDm/9n/5v8AAA0AAADZ/9n/2f/Z/9n/2f/M/9n/GgAnAPP/zP/Z/9n/DQAaAOb/DQAaAPP/GgAaAA0ADQAaABoAGgANABoAGgAaABoAGgAAABoAGgAaABoA8//z/+b/8//m//P/DQAAAPP/5v/m/wAADQAaAA0AAAAaAAAA5v/z/+b/8//z/xoAGgANABoAAADz/+b/5v/z/wAAAAANACcADQAAABoAAADm/wAADQAAAA0AJwAaACcAGgDz/w0AAADz//P/8/8NAA0ADQAAAAAAGgANAPP/5v8AAA0AAAANAPP/8/8AABoAGgAaACcADQAaACcA5v/z//P/GgAnAPP/8/8aABoAJwAnABoA8//m/ycA8//z/w0AGgAaABoA8//z/xoADQDz/w0A8/8NABoA5v/m/8z/8/8aANn/2f/Z/w0AGgDz/wAA2f/Z/xoAGgAAAMz/AAANAAAAGgDm/w0AGgAnABoAzP/Z/9n/8//z//P/DQDz/w0AJwAaABoAGgAnABoAAADm/+b/GgAaAAAA8/8AABoADQANAPP/8/8AAA0AGgAaAPP/8/8aAPP/8/8aABoAGgAAABoAGgAaABoADQDz//P/5v/m/w0AGgANAA0AGgDz//P/8/8AAA0ADQDm/+b/AADm/w0AGgDz//P/8/8NABoADQDz//P/GgDz/+b/5v/m/w0AGgAaAA0A8//z//P/DQANABoAGgDz/w0AGgDz//P/AAANABoAGgAnABoA8//m//P/8/8aABoAJwANAOb/8//z//P/8/8AABoAAADz/w0A8//z/w0ADQDz//P/8/8NABoAGgAaAAAA5v/M/9n/GgAnABoADQAaAPP/2f/z/+b/AAAAABoA8//Z/xoADQDZ/9n/zP/z/xoAJwANANn/2f/Z/+b/5v/m/+b/5v8NABoADQANAOb/5v8NABoADQDz/w0AAADZ//P/DQAaACcAGgANABoAGgAaABoA8//z/w0ADQAaAPP/DQAaABoAGgDz/wAA8/8AABoAGgANAAAA8/8AAPP/8//z/+b/8/8NAPP/8//z//P/DQDm/w0ADQDm/wAAGgAnABoAGgAaAAAAJwAaABoAGgAAABoA8/8aACcAGgAAAOb/AADz//P/AAAaABoA8/8NAOb/8//z//P/GgDm/xoAGgAAACcAJwAaAPP/8//z/w0AJwANAPP/8//z//P/GgAaAA0AGgAnAA0A8//z//P/5v/z//P/8//z/xoAGgANAA0A8//Z/wAA2f/Z/xoAGgANANn/zP/M/9n/2f/m/xoAJwAnABoA8//m/xoAJwANABoAGgAnACcAGgAaANn/zP/z/xoAJwAnABoAGgAaABoAGgAaABoADQDz//P/8//m//P/GgDm/+b/AADz/wAA8//Z/wAAGgANAPP/8//z//P/8//z//P/AADz/+b/8//z//P/8/8AABoAGgANAPP/5v/z/wAA8/8NABoADQAaABoA5v/z/w0ADQANAA0ADQANABoAJwANAPP/AADz/wAADQAAAA0AAAAaAPP/8/8NAA0ADQDz//P/5v/z//P/5v/m/+b/AAANAAAADQDz/+b/8//z//P/8//m//P/GgAaABoAGgAAABoADQDz/w0AGgAaABoAJwANABoA8//m/xoAGgAaAAAAGgANAPP/8/8NACcAGgAnABoA2f/m/wAAAADz/xoAGgAaACcAGgDm/9n/5v/z/+b/8/8aAPP/2f/Z/9n/2f/Z/+b/5v/m/9n/2f/Z/+b/8//m/+b/DQANAAAA8//z/wAA8//m//P/GgAaAA0A8//z/wAA8/8NABoA8//z//P/GgANABoAJwAaABoAJwAaABoAGgDm/+b/AAANABoAJwAnAA0ADQANABoAGgAaABoAAAANAPP/5v8aABoA8//z//P/DQAaAOb/5v8aABoAAADz//P/8/8aAA0A8/8NABoAGgDz//P/AAAAABoAGgDz/wAAGgAaAA0A8//z/wAAGgAaABoAGgAaABoAJwAaABoAGgAaABoAGgAnACcAGgDz//P/GgAaABoADQANAPP/8//z//P/8//m//P/8//z/+b/5v8AAPP/5v/z/wAA8//m/+b/2f/Z/+b/2f/M/9n/5v/m/9n/2f/Z/xoAGgDZ/+b/2f/m/xoAGgDz/ycAGgDm/9n/zP/M/9n/JwANANn/GgAaAPP/AAANABoAGgAaACcAJwAnABoAGgANABoADQAAABoAAAAaABoADQAaAPP/8/8nACcAGgANAAAA8//z/wAAAADz//P/8/8AAPP/8//z//P/8/8aABoA8//z/+b/5v/z//P/5v/z//P/8//m//P/GgAaAOb/5v/m//P/DQANAA0AGgAaABoAGgAnACcAGgDz/w0ADQDm/w0ADQANACcADQAAAA0AGgANAAAADQDz/+b/8/8AAAAA8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/AADz//P/DQDz/+b/8//z/xoAGgDz/xoAAADz/xoA8//m/w0AGgAaABoAGgANABoA8//M/+b/5v/m//P/GgAaAPP/DQAaAA0AGgAaANn/zP/M/9n/zP/M/xoAGgANAA0A8//z//P/2f/Z/w0AGgANABoAGgDm//P/GgAaABoADQAaAA0AGgANAA0A8//z/w0ADQAaAPP/5v/z//P/8//z//P/8/8AAA0AAADz//P/JwDz//P/GgDz//P/8//m/wAAGgDz//P/DQANAPP/5v8aAA0ADQAaAA0AGgAaABoAGgAaAA0A8/8aACcAGgAAACcAGgAnABoA8/8NABoAGgAAAPP/8//z/wAAGgAaABoAGgAAAPP/8//z//P/8//z//P/8/8AAA0A8//m//P/AAAaABoAAAANAAAA5v/m//P/8/8NAPP/8/8NAPP/8//z/xoAGgANABoAAAAaAAAA8/8NABoAGgAaABoAGgAaABoADQDz/xoAJwAaABoAGgANAA0AGgANANn/5v/m/+b/8//m/9n/2f/Z/9n/zP/z/+b/2f8NANn/2f/m/8z/8//z//P/GgAaAA0AGgDz//P/GgAaABoAGgAAAA0AGgANAAAAGgA0AA0AAAAaAPP/8/8nABoAGgAaAAAADQAnABoA8//z/wAAGgDz//P/GgANAA0AAAAaABoAGgDz//P/8//z//P/5v/z//P/8//m/+b/5v8AAOb/2f/m/+b/8//z/xoAGgDz/xoA8/8AABoA8/8NAA0AGgANAA0AGgAaAPP/5v8NABoADQDm//P/JwANAPP/8//m/ycAAADm/xoAAADz//P/DQDz//P/8/8aABoA8//z//P/GgAnABoAGgAnABoA8//m//P/DQAaAA0A8//z/+b/AAANAPP/DQAaAA0AGgAaAPP/8//m/+b/5v/m/wAA5v/m/+b/5v/Z/9n/DQDz//P/GgANANn/GgAaANn/DQDm/8z/8/8NACcAGgAnAA0A8/8aAPP/5v8aAPP/8/8NAPP/5v/z//P/AADz/+b/JwAaAA0AJwAnABoAAAANAAAAAAAaABoAGgAaACcAGgANAAAAGgANAAAADQAaABoAAAANAAAA5v/z/+b/8/8NABoAGgDz/w0AGgAAABoADQAaABoAJwAnABoA8//z/xoA8//z/+b/8/8aABoA8//z/xoA8//z/xoAAAANAA0A8/8nAPP/8//z//P/GgDz/xoADQDm/xoADQAnABoAAAANAAAA5v8AAA0A8/8NABoAGgAaABoAJwAaAA0ADQAaAA0AJwAaAAAAGgAaAA0A8/8NABoAAAAaACcADQAnABoADQAnANn/zP/Z/+b/DQDm/w0ADQAaABoAzP/Z/9n/8/8aAAAADQAaANn/5v/Z/9n/GgDm/9n/AAAaABoADQDz//P/DQANAPP/8/8AAOb/8//z/+b/8//z/+b/AAAaAPP/8/8AAA0AGgDm//P/GgANAA0A8//z/xoADQDz//P/GgAaAPP/AAAAABoAGgDm//P/GgANAPP/8//z/w0A8//m/wAADQANAAAA8/8AACcAJwDz/+b/DQAnAA0A8//z/wAADQDz/wAAGgAAABoADQANABoAAAANAPP/8//z/+b/5v8aAAAA8/8NAPP/8//z/w0ADQAAAAAAAADz//P/AADz/+b/5v/z//P/5v8AAOb/8//z//P/DQDm//P/AAAAAAAA8/8NAOb/AAAnABoADQDm/+b/8/8NABoADQAAABoANAAnAPP/zP/z/zQANAAAAMz/DQAnABoAGgDm/+b/8//z/w0AAADm/ycAGgDm/w0AJwDz/+b/DQAAAOb/DQAAAPP/NAAaAA0AGgAaAPP/DQAaAAAADQANAAAADQANAA0A8/8AAPP/8//z/+b/8//z//P/GgAAAPP/8//m/ycADQDz//P/5v8NAA0ADQANAAAAAADz/+b/DQAAAPP/5v/z/wAADQAAAPP/AADz/wAADQDz/wAA8//z//P/AAAaACcAAAAAAA0AGgAaAPP/AAAnAPP/8/8NAA0ADQDz/wAADQAaAA0A5v/z/ycAGgAAACcAJwDm//P/DQAnACcA8//z/wAAGgAaAA0ADQAaABoAAADz/wAAAAAAAAAAAAANAA0ADQDz//P/8//z/w0ADQANAPP/8/8aAA0AGgDz/+b/AADz//P/8//m/9n/5v/m/9n/2f/z/wAA8/8aACcAAAANAAAAAAANAA0A8/8AAA0A5v/m/wAA5v/m//P/JwAnAOb/zP/m/ycAaQBBAOb/2f8aAEEANAAaABoADQDm/+b/DQA0APP/v/+//9n/DQAaAA0AAADZ/+b/JwBBAA0A2f/Z/xoAQQBBAA0AAAAnAFwAdgBcABoAzP/Z/xoAGgDZ/7L/pP+//xoANAAAABT/0v4aAN8A5v9B/vkAjAUBAmb4Hff1BVkUDAu48RbpTv7jFeAPcfTh6Eb7xg8SCs300e9WAeYOSgUC9Ur2ZQUZC5AA/Pfl/acHuwYn/iz7XABeBHcCuP7l/Uj/AADM/xT/qv75/qT/JwDz/2P/l/8NADQADQDZ/xoAkAD5ANIApP8H/6sA0wKKARr+R/2QAIsDsgF8/an85v/ZAQ0A8v07/wECVgHL/an8GgB+AzUCaf46/Zf/vwF9AU4AXADsAJAAv//m/8UA7ADm/wf/If+X/8z/Y/+4/rj+Y/9pAMUADQD5/rj+2f99AfMB0gCy/3D/5v9pAIMAxQDSAA0Al//z/8UAqwCX/xT/2f/SAAAAdv4U/xQBVgG4/nX8kP6EAvoC7P6V+1z+AgSSBOz+qPr//TAF7warABj6vfvgAjcGFAGO+pX7rAJrBAf//fmc/GoCmAPFAG/9J/4aAJECagJI/7f8GgCFBA8Eo/10+jQA1ATGAsT81/vz/3EDzAFV/4n9R/1cAHEDPAPY/Ub7J/7UBIUEOv0z/PP/CQXSADL6y/0CBHIFO/8F+8z/cQPfAJb9qv6RAt8Ay/3k+6sAngKrAEH+DPydABUDsgEz/GH7JwDcB+0CWvq3/JIEWwuD/n70Bv3dCawPCvjc68T8KA9xEHfzeerE/JIRBgxA74ryVgEADa8IlvDp+HkGlgwUAdr0mvg0DU4N8ftA71n4uxPLDDv/Auhb/NcKIAw9BRvz0fw0/jAFpAF3Amb4RvtWAQMGugRv8Pj8/QhODYL8gu+p/OER6Qdl9uXw7QJCDwcByfnW7NsSrgYa/rv34+y3GNkBZQUL7cj3vw5/BQMGaO++/Qf/mwkM/NoDvPmP/MYCTwJHDGb4vft9AQsW8wF07WPyUhU2Hnzwkugd6tsf8BV95fTnH/uwF+YBZelo72sRAA2V7rgAt/xwDlT9mPR3HDoMAgRz6y4Bkh4rFfPyDOL6Ak0YlwEK+L3UAQKnIfX2xfFS0ncPjiNwDunRNs4kITs17Q9cymnXqyfkPvkA9cKb7UAyDSei+8i2sv+TOu4RKurCt/oPCDfQFojhB76uE14rLg694cfbiBfKJJP3J+Rh7t0wPxbd7SrqJOsWRm7uRPdX5+v8rR6N670KctycGEP1/fnAA67qExkBAvwGhPP2+Pj8ExknDYvnTfwVAx8khfUH5Qb9VBnQI7Pau92aB2M1Vf+N0XPrqQsaNMPtFNgm/HgR+hwW3JDkrxWKG2wGDNX3+ikRbCAUAXDY5QwcBBscNvVF7KgW/flFCBjt0wJODfQDc/hT7iMFSwcLCfXp3w30A6X0hfXV9zMlGgA18+fnwBAIHRb26u3P+OIgB/9k57nmNBplLODmItoS7lkhkyBR3XrfyBO9F5cBIOMH8jAf8gzs8TboUgibFu8GHPVH8LoEoRXk+w4C1BGX8lcQ0fzF/uEEGPpZBwMGh/lO8fn+wAN2AHfz7vWK8kwJqv6m9pIE3PiQDeL3Eu4wEtUGPwnOBYjufxLjCGf6FA4I52UfwwnE75IEUer1H74MAOYk68UA5PtDHlvVyu6AFFzxwRKV1O0CORfBBS7/9t6hCOEreARi8AzvXABUJukHf+ksCsv9iBdu+zvlKBx2/sUNbu5s97oRXwac/JTsWvqy/+oJ5vKj/Zb9Evt+9BL7KAJPAnABP+1SFer6rgYxB5X7gBRI8j/6SAEHDuoJIgPZ8hz1uQJNC7nzigGrAODmySJN764GJ/7z8vQQoPen+EwJBRfqCZ4CvN8JBbQSbBNF+XfmFPIgDCAM1Og0/iT4XgRb/GPluABXHa4GHvme80XsLyoVEEfwNP6G6ukh3hjh6BfrhgbD+qIXl/9h4XEQlvALFlT97/fuBLj+W/xO8WAISgWnFB75OPmH+Vr6egh4EQEPtPbT85fy8wHkCsn5Evte9cvwkA0w9pkFjwsE+Xb+iuVK9tsfLBcM/Er2uOS6EbwIKxUG/bnmcRCy5dQeqv5V/5IRYe50CZX7DxE+B9QE9/p17w0AbAZWAbz5uP7W7HsKafER+U4NyOp4EUfjYe4LCWH7RxnA9A4CuQJ49cIHcQPBEr8OpvZv/V0CLxBZFHbxMPZo/A0Aywy09qwCSwd/6RH5C+1LB8cea/Vo/Kjt1QbgD5T5l/+p/IAUD/X9+UH+ugQHDvHupfTc+JgQxg/y/YH6af6U+d0JaPyRAmEXWOmeAqT/0wIVEAvt3PiQAFkH7P4k+HP4YwGyASIDu/dG+xMMLPuX/5fyJgs/CZANFPLE7yQUePVjG/PyLvIMC7IBuQ9I/5P3H/voBTrw/BORAngENgSe2WkNLgE8A4MA9Of3+jEHzwca/hn8Eey/AbL/vftCDzQAAxMg/dz4TAk4CN4LCgfm/yn1ixAt/VoJ7P7Q7bUHLvKB+o76SweDABj6H+6KAaIKi/TvBnfmfwVHDED86wse+ZAA0gDx+/kAoww0AHv77/fx+2ka4/nZ/8UAL/S7EyP2zweyDhT/fgOz9GP/dPqdDTsB3fouAaD3VwNc/hTysvJMCRH5OQrb9nbxpxRN/JcB9Oca8bsT0AmlA2wG3/5o/KYFJfr2BxkLeARbGFwAnfGK/xf4lRd3Auzx3/6U7GEXnvOQ8df7bOopEdfu9fZqD/H7agJr9TvywhSDAEEN+g+79zYEnvNeBB0G0gCdAHABcgUm/Az8X/cNAAgDcP/gAj4HMwtjAT325vLj+f4K4QSP/MwBigE99pf/if0S+ysIVf+rDSH/A/d+A476lgyk/6frjQcq988HSxRR6v0ITvF88CkRh/kHDrj+X/e4AAbwhRHMATv//BPB6QUKQgKB+o8Ye+7fAFAExvNyEqH5NvWRAjPvbgrgAg7zCAOI+9cK6Qcr+ev8P/ruBD0FqfwRCCD9FQOkAbvqjQf5/iwKAQ907VEGevk0AKEVcfRj/wkF+O8MC2kAQPzAEDf33frb9uDzPAPJCLwI2P0U8ijzQQBRBlwNW/xa+qwCEPeUCIUESP9yEnD/TPpu+z32dg1BDQcBGPrS8S39SQM7AUD8FAFMCf/9Bv2z9OAClw5pALj++fFO/h4I0v4vA/j8vPlOACz7IgNaCUj/TwL6Aq/5AQLF/g0Ang+QAMr7H/uz9NH8dgC3/JMGFQOX/0EARfmyAXD/gAcWBdkBNQKo+k8C1ARjAcn56Pa2+pAAAgSLA2/9IQGp/M/4W/zl/foCMgkUAV3zxgKd/loJYAh0+l4ETv56CCEBgwB+A2kAKgbd+q/5if36AqsATfxi/ff6sPt3An/2+PxoC277fAzY/WrzOQrC+LoEdwK2+jIJVf/zAUH+aQDsALj+l/9t+R0G3/6/AaQBXvUhAdn/LPvUBI76Bv0DBjH4CgdcANkBzgUl+nEDJvzMARQBXQJ+A/oC2gNA/In9gvz5/l0CBwET/aUD5Pto/OACgfraA7j+e/uEAub/SQMiA//9nPzM/6UD2QGrAE38g/5SCDQAgwBjAZT5NQJM+hj6swNwAZMG6vol+n0BkAD7BHT6j/xPAnABMAXz/9oDzAET/Qz8dPr8BmwG7QJw/+H1Ov2YA2MBsgHx+2P/hAJO/mP/Df7TAl4E2P0T/Qb9ngLiBor/dgBG+9L+IgNG+3ED0fyD/iH/1ffGAjT+SP+RAk38QQAn/qQBWAU3BgMGif3e/C39FP/1BUMEZANqAqn8cAHM/yD9Tfx6+VX/0v7y/d/+VP0a/or/Wfht+XABVf+FBDsBJwCeAkf92f/SAOgFRQimBUoF5wMUAdQEnf52/pMGTfwnAKH5lPlJA6/5kP4M/Kn8o/1L+Jv61/scBAkF2gOrAA3+DgIBAgcBrQT7BJAA8wFV/xQBpgUh/3cCagIY+qQB7AAh/+ACcP99/xT/5PsG/U7+5f1V/xT/y/12/tIAuQJo/G/9NABOAGQD0PpU/dkB3wCK/8T8NABCAt8ATv5j/7IBzweXAX0BhAIu/6gJnQDm/08CTfyyAU38Lf1j/y39qwC3/MP6xf57+9L+TgDy/XABSP+eAmoCFP+dAOACVwM7AXz9NP5JA8ADpP+D/mf69vhOAOX9xQAbAtIAQgLl/fL9XP4BAl0C3wCsAhUDZAPm/34DqwAt/VYB6vou/xQBqfyyAWj8VP0u/5b9AAAg/dkBugTzAWQDfQEU/+wA3/4UAawCkAB3AkEAO/9i/dH8l//sACgC3wCsAiH/SAHNA5D+zAG4AJIE2gOQADUCQQCK/6P9Z/rl/ZcBFQMiA+v8Qf6K/7L/E/3r/DUC2gNQBMUA+f47AXcCDQCq/nX8xf5BANj9g/4N/on9xf7e/Ej/BwF2AEMEBwE0ACgC0gC4ABT/j/yQ/uz+j/wU/6T/VgEHAbf8E/2w+1T9uADm/wAAvwEnAE4ATgAN/mn+Vf/z/2kAt/yk/50ASP87AWj8nQB2ADUChAKyAQcBlwEHAWj8dgDz/2oCAQJWAfkAaQCdAPkAigHfAPP/JwDmAdIAOwEAAC4BaQDl/SH/+f5OABQBcAHSAMz/XACX/+cDNgSeAmMBaQC/AeYBAQKsAnABaQDZAcz/NQLsAKr+Lf0G/d8AIf9OAAAAxQBpANL+3wDSADUCDQD5/lT92P3sAIr/FAFcACD9Gv4u/wf/Ov0M/P/9nf4u/3X8qv7S/vL9zP/++5f/kP5j/6T/W/xV/wX7/vuW/W77+PxG+737t/yo+m77gvz4/HD/J/6c/Oz+8/+kAZgDcAGXAWkA7ACXAVX/uAAN/sUA7QLM/3cCsgEcBMADkP6//7//2QEuAbL/7P6Q/k7+If/sABoAhAIOAswBKQTOBe8Gzwe7BnkGKwinB9UG3Ae8CJQIXwaLA38F+wToBTcG+gLCByQHrgazA/kAhgaSBPoCFAFWAawCrAKKAQAAJwBp/t78W/yJ/RoAxQCDALIBpAEBAigCXADFAIoB7AC4AEj/Gv6q/hoAGv51/Hr5vPm2+sn5u/fO9u/3mPRx9ITznvOf9Sn1xvPs8UfwB/Kd8d7vt+9U8L/yivIu8nDyQfHU9Rb2L/Rr9Un0ofmo+tH83/7SAKwCfgNxA34DjAVEBtYIUQbIBjYEAwbdCekHyAa0BRcHHgjJCDgIewrYDBMMnAtnCU4NAhH6D0IPpA5kEFYODQ34C3wM7Q8oD3ULbgqHCOMIJgtgCGAIzweGBvwGYAjYDNoQKhNyElARwRJnFuYbhR66Hn4dPB30HbkcnhyPGPYUpRBiDD8JPAP5/gP3//Aq6kfjLOGY2v7U1dAAzBrKWsbVw0rCPcIpwXHAbMPvw7DHDcrsyq3OZtGQ1+DZ6NwS4b/lW+++8H/2wvib+g0AdgA3Bo0HEgo0Db4M2Q4vEFcQ7Q+JDI8LBw65DyIQDQ1uCmgL2AxwDhoNIAwzCz8JWwu3C6sNqw0hDg4PqQtuCsMJtwsTDAUKOAgrCB8KRQigBpMGPgdFCIsDUQYPBBYF7gQcBDwdXSmwPrc/LzerQZFD91e3WaxdGltGWKtbR023TENFHUeLROowUB4eCCkEdv6m9krpYdSky1jCRbhGreuh3Z/cnbKXaoung8qG7IkCjfOKfIjNjKOV9ZszofSmDbB9vqfEicllz5fY0+Zp8Tj5Gv4qBtgMSxSQGn8fsijoLN0w3C7VLU0yqzQDOng4aTQeL4Ur9SzhK5ks0ymkKIkmPiHNHU0Y6xjEGGgY4hMgDJUK9QU+B2MBLf2o+lf0KPPK7qnvlOzV6inofeVV5ergGuRq5trnrejz5a3oHeoq6mvoA+qC7+zxafEl7evvCfYk+Lv37vXB9ib8aPwz/N78Tv4oAnAB5gEuAboE9gcxB7wIFwfjCP4KTQvfDcUNGw/5DWoPhhNsE2AVtBLTHPYujz+iS+hGU0v6UBRcRmXbYAZnxGZPapdpUF8VXkBZMldGS2Q3sSYAGsIUoQhg+RDqUNu10UbHUrg/rPqllqKaneORdYeNg9aEJofXhhKGEoarixGRM5QpmrehH62Ys5O2W7vUwV/QeNuc4kLm++iR81v8yAZGCkQTSR0qIHUl1yRZLlc3Kzy2PRE8XEFxRFpKBElLSFNLmkgeSYRDRj5ZO0k3DjZzLicnnx46GT8WnQ0YCS8DDQB0+vLwtevT5qzm3+SC4qjgx9us2azZ+9uu3YbdMd7j31videIm4v/ji+fC64fsBOxn7VzxZfaN+Dj5evnW+f77W/zR/Mv9zP+lAy8DpQP0A4wFTAkYCVMKTQt1C4oO+g8pEWQQ2Q43E58RKhOXDq0RNSnUOBhKkkXyQGdKF1XjY5JfRGHbYB5jZ2RFVs9Vu1QeVi5PzjkhKPEXChRuCjL6r+yt29bS18dhulywcaZ8ou6a5JOWiIWA3IMehAuFz4MBgEaGwIxmkFuUCJlJprGvZbWAt5a8PM0V2tjj7ujD7Yf5+gKcCwkSJxpjKP0vFTcxO/4+m0oBUDFVgVeoV0hcaVtPXeVaYVh7WK1SSVHcSNlCWj2/NZUxyyZ5IHUYchKpC+b/vPlJ9KrxZuuV4cHcQtkC27nZaNXd01nRGdWc1ZzVy9ZV2Gzdod9o4mji8uNK6UztvvBH8G7uuPHn9Ev4ofn89+L3Jfqi+3v73vxv/Yr/xf6q/oMArAIDBp8ErgYFCsMJMgmtBCsInQ2sDw4PGQvZG0Qt8kAkSCdBqEo2UhdijmRSY0Bm0GQ7aQheLFiCWTpaTlsRSUk3uCcJH48YtQfr/JvtduRl3JfLi8A3toqxPqrwntCS+YmqifmJqYcXg7qAWYO+iACLAo3JkXiaVaTbqDKs8q8GvF3M19SS2yXgHOj09Pn+mgdjDtEYPyNXKgswaTRGPrtHvk2sUIRQFVEJUwtX41ZzVeFS0k5TSypHakPDPcE50TJrKzIjBhmuE6sNYAh2AAr4g/HI6trnaOJ634bd2tpX2k3VC9My05zVpNg12e7brt1G4TvlI+n97OvvDvPN9BD3tfjK+07+xf4U/2kA3wDfAHYAQQBOAJ0ADQDl/TT+B/8hAdMCXQJxA/MBTwJlBa4G/QjCBxEIWgmvCDIJJQmMEskiRjExO2w6sD4QRwFQZ1cXVQRWYFYEVqBUqUwfS+RLRUlpQZsw0CNiGQIRNwZl9nrsM+KY2vvO07/PtzSw/aujomOX6I6diWOKE4jqhdyDjoX/iMaLX48zlAqdxqX3rLmy9relwIrLltaA3hvmDO9S+SkEVAwlFj0fmCqrNBE8xkN0SglTylggWstaYlpOWzpatldmVX5RrFAsS8FGaUGTOhs2Ki3eJWoc4xVxEI4JdwKv+Un0OvAE7JHmgeAK3vvbrNnL1gvT99P41UjY7NeQ10/Za9vi3T/go+Ok5V7oN+or7MTvQvMw9j74UvkL+vj8uP4u/ycAdgCEAi8DVwNxA58EvAiOCdAJoQjQCS0MWwveC/cJbgqpCxgJyQi7BiAM3xoaJ94y5TOzN+Q+bEfZT8VOD1K5UF1Q2E37RftFwUZmSDVDxjYWLB0gNBqSEegFqPpn7TTk59rc0WjIusHfvTe2oasfoL+XdpYTlSWSa43MiimN6ZCplN+WFpvEoQ+n76kSrdqzT78GyeHOYdR/3CnoY/LX+w8EBw5iGbQfqSUWLEk3XUOoSmlOxlD8VIJZXFs0W9FZjldgVkNSiU3wSe9HpUSvPPgyTylTJIwfVBlDEdYI+gK3/HL2OvAe7HLp8+Va4ODZo9YG1r7WxNUl0+PSiNRp13DYG9n72z/go+MN5JDknuZz6zPvW+808frz1fdF+eP5bvsN/p4CXQJXAwIEoAYlCTIJOQrWCG4KOQrxCkcMlgy4DZwL5ApTCvEKvgxHDDML3QkGDLsTpR0AJwMtUzGyNcg6R0BdQ8dFmUZ4RZFD2EBNP00/OT7cO7g0NiskIbYW4A9zB7H9xfE75dTbjtNJzd3GPMD+usCzoatVpAufk5xQmvOXIJXxkyeWmJnPnSejf6g6r5izUriqvQTFrc7e1YbdTuTW7K73PAO+DOMVkh6xJq4tqTJkN0s72EBJRHhF1EUwRvxHHkkqRyJEcEItQJQ8rDZ0MDAsTic+IdIa9hTaEIMNYAg8A5D+Jfp/9pDxM+8Y7THrI+nt5n7nFecP6Dbox+iT6h3qyezq7b3udvEz71f0n/UL+iH/Tv78Bq0EmgcPBDUCwQX6AjcGQgJLB8kIMwudDW8MHBEuDhoNyQgqBl4ExQDz/3z9l/9j/4r/iv/m/y8DswM2BLQFRgrnEGAVlhm6HtckRC3kMfk0DjbzNQ42+DJ7Mckvwi5ZLvopOSRXHf4XNxNbC4QCjfix8NToYN9B1wPQ+crdxi/AWrmftCKzcLE6r+qsS6rvqeKptaoFreaxerhVvh3DE8lR0E/ZiOE853TtavM/+i7/IwXeCz0S6xjMG/wgACdKLCUwUzEgMy0zYjPEMqIxTTKOMOkuciyEKZ0nOSQ4IjcgahwzGGUS0g3XCksHnwTGAvMB8//Y/cr7vfs6/Y/8qfxG+zL6P/qB+pz8dv75/hT/Y/9BACgCKQSZBRAGwQU9BXkGjQcLCQUKjgkLCTcGhQStBEoFMAV+AygC8wGkAfn+lv2p/Gj8ovv89/v1KfUP9cD05/QJ9nr5Bv1xAwUKxg+8FUcZ6B8YI1Qm3yeLKqAtoC3vLZ8roC2TLQ8r7CdMI4AhXRzqFl0PEQhXA+r6cfRa7WTn3uK621TWUdD5yh/HAsG4vT+5RLbhtLmyFbPzsYW0c7cmuxu/CcJhxyjM1tIO2T7eduRK6anvpvZo/NMC1QaCC6UQjRR1GC4b7h6NIXQjTSViJrIo7Sn0Ktoq0yldKdInOiaIJPYhwR9+HWkaRhcDE80Q2Q4ADb0KoQhzB6YF1ATgAoQCAQJIAXYAO//z/5f/GgAaAC7/Lv/F/kEAfQGeAvoC+gL7BK0EugRYBcgG1gj9CGcJbgqxDJ0NiQypC0AL0QuVCtwHNwYwBTYEvwFj/8X+J/4t/W77lPmh+c/4tfjJ+bD7FAHnA9UGWgl2De8T9xZcGgAaSBukG8UaOxsgGcsZUxe1FO4Rvw5cDSwKoQgCBLj+evnG8+vvROrf5OPfx9uK2HvUmtHbzwnPs83FykDIw8bDxlPHvccTyZfLks4k0W7U2dgX3mLjzefv6qLuv/Lo9o765f3zAZkFrwgMC30OFhLJFfgYaRp3HKUdhR43IO8ggCHiIOgfPR94HtodkRykGxQbExm2FmAVWRTiE6YSkhGsDwcOsQzxCjkK/QjwCM8HJAeGBqYFSgWFBJ8EHAR4BAIE+gIbAt8ASAGkAZcBSAEhAdkBngKYA9QEZQWZBVEGyAb2B2AIYAhFCCsIlAhmB64G7wb2B5QISwcwBeACfQE0AGP/O/99/1X/kP64/hoAfgNfBh4ItgnXCuUMLg5WDtkOQg8bD8UNEwycC00LiAqUCLQFswO4AJb9WvoD99Pz0e9g7NToIeWV4ZPd2toO2fLWj9XQ01rTGNN60snSTNM51OTUb9Zj2M3ak92H3xniSOXn56DqMu0G8ELz1PU++Ez6E/3Z/5ECwQURCN0JtwuDDV0PHBGtEQkSwRLVE1IV8BWoFh8XohdoGI8YxBgMGGEXRheVF+QXexebFrwVwhTIE+gS1BHaEDwQng87DicNEwyPC3UL5AqoCUUIyAbBBeEESQPmAbgAv/9V/7j+Gv6x/Zb9vv2j/dj9kP7s/jv/iv+QACgCfgMjBUQGwgfWCIEJEgpbCwcOIhD7EVEToRWVF48Y0RhAGCYYjhbIE2QQQQ2VCtwH1ARIAd/+gvzW+QP3nvPL8CzuMes859Hiwt6F25fYnNX90tXQtM9DznHN7cwizQ/Oks7Oz8/RBdQT1i7YmNoK3qLhY+Uj6QvtdvGF9UX56/zSACMFZwlvDOYOzRBlElkUoRWOFiwXlRfKFzMYQBgMGLAXexfqFq8V6RRfEwkSsxABDw0N5AoyCekH7wZ5Bs4F+wThBFgFRAYXB/wGJAdLB6cHZgeGBqYFWAV/BWUFPQXhBEoF+wQWBXgELwM1AkgBIQG4ADQAO////bH9b/1v/Q3+Tv5c/nb+Vf8nAKsASAFdArMD9AO6BBYFzgVFCMMJ1wqPC8sMvw5JEFAR7hEqE3MUKxVLFBATWBLUEXgRFRDmDk4NjwsmCywKwwlnCbUHAwYiA3ABGgCj/ff6Ufci9Inwoez76Oblo+PK4Tjf+9t32WPYFNjS107X5dZB1+zXv9ie2WTa4dsq3RHfe+F843Dlnubn53/p9uv+7p3xxvOt9cj3/fmi+978kP6rAKwCDwTHBCoGHgjQCTMLLQwNDWMOXQ/6D1cQ5xAwEvUShhM+FKEV5BdUGb4ZbxnRGMQYnBhoGJUXwxbwFQoUphJlEn8SphI9EjYRixDtD0IPLg58DL0K1gj8BscEuQIuAQ0A7P5H/d78W/z++8r7Yfvk+8r75PsF+/35Jfqo+pX7Gfwz/Ov8Df5w/xQBdwJDBJkFeQYkB9wH8Ah0CdAJLArqCRIKbgqICnsKRgrqCcMJdAltCGYHtAV+AwcBGv5T++/3ZPS+8LzsmenG5ifkQOKB4PbeoN1Y3K3bQ9sp217beNvn2gjahNmE2e3Zs9r7267dbd8s4RPjO+Uc6FnriO4n8Qj02/bP+BL7Bv07/6QBAgSGBgsJ5AqWDGMOfhAdE7wVDBgGGZYZqxp9G5Ecsx1rHlgfwR/1H/UfjB9yHxYfkh4cHi8dARx2GmIZqRhhF1oWwhT1EjYRdw9wDjQNRwwMC2cJOAjiBrQFQwQIA5cBff+x/VP7RfkE+YD4/PcQ96b2A/dR9wr44vfv98j3k/fV9673UffO9oz2tPYD91/3CvgR+Qv6EvuC/EH+v/+4APMBZAPUBEQG1QYkB7sGAwZYBdoD8wHZ//L9vft6+ZP3NvVd853x+O+w7qHs1epy6V7ozees5i7lVOMM4qLh6uB04KjgEuHK4ani5ePM5VDooOrw7JzvVfJ+9MH26fgf+zr9xf5OAOYB2gNyBXkGZgdtCN0JjwtODTUPaxEdE2YURRUlFh8XdRiJGRoauBpOGoMa0hr5GtkbNRyEHHccKBzZG2Mb+RpBGi0ZDBgsF+MVFxSZEjYR+g+KDvIMTQs/CY0HHQZQBLkCFAG//7j+5f0g/UD8ovu2+v35h/np+Pz3Kvdy9uH1+/X79cf1AvWl9AL1ePUj9rT2EPcQ94b3S/gr+TL69/oS+8P6Z/oy+gv6h/mN+JP3ZfYC9XfzLvJB8ZbwfPD479fuEu5n7T/tZ+2o7d3tWu0Y7UztJe1a7cPtRu6w7vHuxO+W8MXxd/Pa9CP2oPen+If5Rvvr/DT+Y//z/6sAOwHMAcYCiwNrBH8F9QWgBtwHyQgfCrcLsQzSDSgPCBBeEcESChTcFHoVMhbDFkYXexe9F70XvRd7F/cWthY/Fm0VWRRsE5kSrRGzEAgQdw+KDnYNbwxAC/cJ/QhtCOkHZgegBrQFSgXhBF4EQwQPBA8EwAPGAg4CcAH5AFwAB//l/dH8ovuB+of5wvg++KD3mfaf9Q/1AvXn9H704POe8zXzv/I78lzx2PAt8EDviO4S7tDtP+1M7dDtEu4f7tDtw+0s7m7ubu5T7pXuDO+3777wafF98sbzwPTh9ab2X/en+Fr6e/uc/Hz9Df4H/+b/uACkAeACwAOfBDAFcgXBBQMGKgYQBkQGhgbIBrsGeQbVBoAH3AdFCMkI/QhMCXQJtgkfCmEK1wr+CsQL5QwaDRoNxQ1IDkgODg+sD8YPCBDgD+0PkQ9dD3cPKA+/Di4OaQ3LDCAMMwsfCgsJ3AeTBmUFUARxA0ICSAFcAH3/If80/uX9o/0T/fj8E/1H/b79//0a/rj+uP6q/rj+dv5B/r79Ov3E/Fv8Jvyw+/f6jvo/+nr53PhZ+F/3ZfZD9e3z2fK48VTwGe857oHt8Oxg7Nzr9utt7P3sge2b7erte+7X7nXvE/DY8KrxO/Ib8/rzpfTU9R33Pviv+QX7TfyW/ar+ff+DAHABTwJXA0MEMAXbBfwGOAg/CWEKaAsgDLEMGg2rDUgOVg5wDnAOSA6kDr8OzA4OD10PuQ+RDxsP5g6kDgcOXA0ADW8MjwvKCjkKwwl0CUwJ1ggeCI0HjQe1B2YHPgf8BmwGKgYDBtsFfwUjBe4EQwQCBKUDCAMIA4QCXQLmAQcBnQDZ/+z+8v0G/cr7H/uB+of5tfiu90T3pvb79ZL19PSL9PrzQvOX8t/xnfFO8fLwlvBH8Inw8vAN8RrxqvEH8iHySPJj8jXzkfOE85HzkfMI9En0s/Q29e71pvZE9/z3z/iv+bb6GfzR/KP9qv47/wAA7ABwASgC4AJXAzYE7gS0BZMGWQdFCCUJtglhCtcKWwvECy0M2AxpDZANnQ3sDewNFA4hDrgNkA0aDdgMfAw6DBMMtwtoC9cKYQqoCSUJoQjpB40HFweGBrQFxwQPBGQDhALMARQBgwDM/9/+8v0t/cT8nPyc/DP8GfwZ/PH7yvvX+1v8M/z+++T7yvsM/Gj8dfwz/ED8j/z4/CD9Ov2W/b79if1v/VT9xPwM/G77Bft0+pT5BPma+Pz3A/fO9pn2Cfbh9YX1Q/VQ9fT0pfT09Of0wPQC9QL1NvWf9Vj2RPeg9xf46fh6+Rj6m/o5+/H7qfx8/cv9NP6q/i7/5v+DAN8AYwEBAhsC4AJkA6UDwAOlA9oDHASSBPsEjAXBBfUFuwY+B8IHOAjJCHQJBQofCkYKogp7CiYLxAvEC9EL0QvrC+sLEwzrC5wLggsMC6IKRgrqCYEJCwlFCD4HbAbOBTAFkgQcBFcDDgI7AbgA5v/S/uX9Ov0z/GH79/oy+of5Hvnp+ID4Mfgk+Ar4F/ji9wr4F/j89/z34vck+Kf49vge+VL5evl6+Yf58Pk/+gv6P/pa+jL6C/q8+bz5yfmv+RH53Pi1+Ar4Cvjv9wr4gPiN+Nz4Hvmv+TL6GPrj+f35P/q2+lP7Rvtu+/77gvxU/Sf+FP/z/5AALgHZAU8CuQJJA8ADNgSFBLoEPQXOBVEGuwYkB8IHKwiHCNYIGAn9CLwIoQhtCB4ItQe1B3MHCgcKB+IGuwaGBkQGEAbBBZkFPQW6BF4EDwSzAzwDCAOsAl0CQgLzAfMBzAHZARsC8wHzAQECAQIoAkICGwLmAaQBSAEuATsB7ADFAJ0AJwDM/6T/iv9V//n+af40/r79Ov0t/d78Tfyi+wX7jvrJ+Tj5tfgK+Ar4F/jV96D3oPeu99X37/fi9/z3Cvhm+I34p/jp+B75lPkl+pv63fpu+7D71/sM/HX86/zr/FT9sf3//Vz+uP5w/w0ATgC4APkAcAFwATsBcAFWAVYBLgEuAWMBfQHZAeYBAQJdArkCCANxA7MDwAP0AzYEKQQ2BBwEHARDBEMEHAQcBIUEkgStBOEErQTUBK0EkgStBJIEkgReBAIEcQP6AmoC8wGXAUgBYwFIARQBLgFWAdkB8wEbAhsCvwEOAtkBfQFWAUgBVgEHAasAnQC4AIMAdgAnANn/cP/5/vn+dv40/g3+lv1i/Tr96/yc/Lf8QPwM/P77e/ss+5v6P/oY+vD5GPrj+Qv6w/p7+038E/2W/dj9Gv5c/nb+nf64/sX++f7f/uz++f4U/y7/If9j/5f/2f/m/9n/DQDm/9n/AAANABoAGgAnABoADQAAAPP/8//m/w0ANAAnACcANABBAFwAdgCQAJAAkACQAIMAkACdAJ0AqwCrAKsAqwCdAJ0AqwDFAKsAnQCQAIMAqwCdALgAuACrAKsAqwCdAJAAkACQAJAAgwCDAJ0AnQCdAJ0AkACdABQBfQHmAU8ChAKeAtMCZAOLA2QDSQNJA3EDCAO5Ap4CagIBAooBFAG4AIMAJwCy/1X/FP+D/uX9o/1v/d78dfxN/Nf7vftu+1P7RvvD+sP6m/q2+hL79/r3+vf63fo5+5X7Jvy3/N78Bv1v/b79//0n/sX+Lv8u/0j/cP/z/0EAqwCrAPkAigHZAQECAQKEAsYCxgLGAvoCPAM8AxUD+gLgAuAC7QLGAp4CngJqAmoCTwIbAjUCGwIOAuYBsgG/AZcBlwFwAUgBSAFWASEB+QD5APkA0gCrAKsAnQCDAGkATgBBABoADQAAAAAA5v/Z/9n/pP+k/6T/iv+X/33/ff99/33/ff9j/1X/cP9j/2P/cP99/33/ff9j/3D/cP9V/1X/O/9j/1X/SP9V/2P/Y/9j/1X/Y/9j/2P/Y/9j/4r/iv+K/7L/pP+K/7L/v/+//7L/sv+//8z/v/+k/7//zP+//7//zP+//5f/l/+X/5f/pP+y/7//v/+y/7L/v/+//7L/zP/M/+b/zP/M/8z/v/+y/7L/pP+y/7//zP8AAAAA8//m/+b/AAA0AEEAJwA0ADQAJwAnADQANAA0ACcAJwBBAEEAQQA0ADQANAA0ACcANAA0ADQAJwAaAAAAAAAAAAAA8//m/9n/2f/m/9n/2f/M/9n/8//z//P/AAAAAAAAAADz/wAAAAAAAA0ADQAaABoAJwAaABoAJwAnACcAGgAnACcADQDz/wAAAAAAABoAAADz//P/8//z/+b/5v/m/+b/v/+//8z/zP/m/+b/v/+//7//zP/M/8z/zP/Z/9n/2f/z//P/AAAAAAAAAAAAAAAAAAANAAAAAAAAAA0AAAANAA0AAAANAAAADQANAAAAAAAAAAAAAAAaABoAAAAnACcAGgANACcANAAnACcAJwA0ACcAGgAaADQAXABOAFwAXABcAFwAXABOAE4ATgA0AE4AXABOADQANAA0ACcAJwAnACcAJwAnABoADQANAA0AGgAaABoAGgAaABoAGgAaABoAGgAAAPP/8//m//P/8//z//P/AADz//P/AADZ//P/AAAAAAAAAAAAAAAAAADz//P/8//z//P/8//m/+b/5v/m//P/8//m/+b/5v/m//P/8/8NAA0ADQANABoAGgAAAPP/8/8AAPP/8//z//P/8//z//P/8//z//P/8//z//P/2f/Z/9n/zP/M/8z/zP+X/5f/v//M/7//v//m//P/5v/m//P/8/8NABoAGgANAA0AJwA0ADQANAAnACcANAA0ADQAJwAnADQANAA0ADQAJwAnACcANAA0AE4ATgBOAGkAqwC4AKsAqwCrAMUAxQC4AFwAJwAaAPP/zP/M/8z/sv+k/6T/zP/Z/9n/2f/M/8z/zP+k/6T/pP+k/6T/v//M/6T/pP+X/6T/pP+//8z/zP/M/7//v//M/7//iv+//9n/v//M/8z/zP/M/8z/v//M/8z/zP/z//P/8//z/+b/8//z//P/8//z/wAA8//z//P/GgAaAA0ADQAaABoAGgAaABoAGgAaABoAGgAnABoAGgAaABoAAAAaABoAGgAaABoAGgANAA0ADQAaAA0ADQAaAA0ADQAaABoAGgAaABoAGgAaABoAGgANAA0ADQANABoAJwAnADQANAA0ADQANAA0ADQANAAnABoAGgAaAA0ADQANAA0ADQANAA0ADQANAA0A8//z//P/8//z//P/8//z//P/8//z//P/5v/m/+b/5v/m/9n/5v/m/+b/5v/M/7//zP/M/+b/8//z//P/5v/M/7//v/+//8z/v//M/8z/zP/M/8z/zP/M/8z/v/+//8z/zP+//6T/pP+k/7//zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/2f/z//P/8//m/+b/5v/m/+b/5v/z//P/5v/m//P/DQANAA0ADQANAA0ADQAnACcAJwA0ADQANAA0ADQAJwANACcAJwANAA0ADQANAA0ADQANAA0ADQANAA0ADQANAA0ADQANABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoADQDz//P/AAAaACcAJwAnABoAGgAaAA0A8//m/+b/8//z//P/8//z//P/8//z/wAAAADz/9n/zP/Z//P/8//z//P/8//z//P/AAAAAPP/AADz/9n/2f/M/7L/pP+y/5f/sv+X/3D/If+Q/p3+Qf7//Sf+Gv40/oP+7P5w/7//QQCdAJAAxQD5AAcB+QC4ALgAuAC4AKsAqwDfAOwA3wDFALgAxQDFAMUAuACDAIMAgwCDAIMAgwCDAHYAXABOAFwAXABOADQANAA0ADQANAA0ADQAJwAaABoADQDz//P/8//m/+b/2f/m/+b/2f/m/w0AGgANAPP/8//z//P/8//z//P/8//z/+b/5v/z//P/8//z//P/8//z//P/AADz//P/8//z/8z/zP+//7//pP+X/5f/l/+k/6T/pP+k/7//v/+//7//zP/M/8z/zP/M/8z/zP/M/8z/2f/z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8/8NABoADQAaAA0A8//z//P/8//z/xoAGgDz//P/8//z//P/8//z/w0ADQAaABoAGgAnAE4ATgBOAFwATgBcAFwAXABOADQAGgAaABoAGgANAA0AGgANAA0AGgAaABoADQANAA0A8//m/+b/8//z//P/8//z//P/5v/m/+b/8//z//P/5v/m/+b/5v/z/+b/8//z//P/8//z//P/8//z//P/8//z//P/5v/z//P/8//z//P/2f/m//P/8//Z/8z/zP/M/9n/2f/M/8z/zP/M/8z/zP/Z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z/wAAJwAnABoAGgAaABoAGgAaACcAJwAaABoAGgAaAA0ADQAaABoADQDz//P/8//z//P/8/8NABoAGgDz//P/8//z//P/8//z//P/8//z/wAAGgAaABoAGgAaABoAGgAaADQAJwA0AEEAQQA0ADQANAAnACcAJwAnACcAJwAnACcAJwAaABoAGgAaABoAGgAAANn/zP/M//P/8//z/wAA8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/DQAaABoAGgAAAPP/8//z//P/8//m//P/8//z//P/5v/z/+b/2f/M/8z/zP/M/7//v//M/+b/8//z/+b/5v/m/+b/5v/z//P/8//z//P/AAAaABoAGgAaABoAGgAaABoADQANAA0ADQANAA0ADQAaAA0ADQAaAA0ADQANABoAGgANAPP/8//z/w0A8//m//P/8//z//P/8//z//P/DQAaABoAGgAaABoAGgAaABoAGgAaAA0ADQANAA0ADQANABoAGgANABoADQANABoAGgAaABoAGgAaABoADQANAPP/8//z/9n/zP+//7//zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/5v/z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/AAAaACcAGgAaACcAJwAnABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaACcAJwAnABoAGgAnABoAJwAnABoAGgAnACcAJwAaACcAJwAnACcAJwAnACcAJwAnACcAGgAaABoAJwAaAAAA8//z//P/8//z//P/8//z//P/8//m//P/8//z/+b/5v/z//P/5v/z/+b/5v/z//P/8//z//P/8//z/wAA8//z//P/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP/M/8z/zP8aABoA8//m//P/DQANAA0ADQANAA0ADQAaAA0A8/8aABoA5v/z//P/DQDz/+b/8//z//P/8/8aABoA8//z/xoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoADQANABoAGgAaABoADQANAA0ADQAaAA0A8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8/8AAPP/8//z//P/8//z//P/8//z//P/8//z//P/AAANAPP/8//z/w0AGgAaABoAGgAaABoADQDz//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//M/8z/zP/M/8z/zP/M/8z/zP/M/8z/v//M/w0AGgAaABoAGgAaABoAGgAaABoAGgANABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaAAAAAADz//P/8//z//P/8/8AAPP/8//z//P/8//z//P/8/8AAPP/8//z//P/AADz//P/AAAaABoAGgAaABoAGgAaABoAAADz//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//m//P/8//z//P/8//z//P/8//z//P/8//z//P/8/8AAA0AGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaABoAGgAaAPP/8//z//P/8/8aAPP/8/8aAPP/GgAaAPP/GgAaABoAGgAaABoAJwANANn/DQAaABoAGgAaABoAGgAaAPP/8//z/+b/zP/M/8z/zP/M/7//zP/m/8z/8//M/8z/5v/z//P/8//z//P/8//z//P/8//z/xoAGgDz/+b/8//z//P/8//z/xoAGgAaABoADQDz//P/GgAaABoAGgAaABoAGgAaAA0AAAANAAAA5v/z/w0A8//m//P/8//z//P/8//z//P/8//z/xoAGgAaABoA8/8aAA0A8/8aABoAGgAaAA0AGgAaABoAGgAaABoAGgAnABoAGgAaABoAGgAaABoAGgAaABoAGgAaAA0AAADz//P/8/8AAPP/8//z//P/8/8AAPP/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z/wAAGgAnABoA2f+//9n/zP/Z/+b/zP+//8z/v//Z//P/zP/M/9n/GgAaANn/zP/M/8z/zP/Z/8z/zP/Z/8z/2f/z//P/8//z//P/DQAaAA0A8//z//P/8//z//P/8//z//P/8//m//P/DQDz//P/AAAaABoAGgAnAA0A8//z/wAAGgDz//P/8/8AAA0AGgAaABoAJwAaABoAGgAaABoAGgDz/xoAJwAnACcAAAANABoAGgAAAPP/8//z//P/8/8NAPP/8//z//P/8//z//P/8//z//P/5v/z//P/5v/m/+b/8//z//P/8//z//P/8//z//P/5v/z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/GgAAAPP/8//z/w0A8//z/wAA8//z//P/GgAaABoAGgDz//P/8//z/wAA8//M//P/zP/M//P/zP/m//P/zP/M/8z/2f8aABoAGgAaAA0AGgANACcADQDM/w0AGgANABoAGgAaABoADQAaABoA8/8aABoAGgAaABoAGgDz/xoAGgDz/xoAGgAaABoA8//z//P/8//z//P/8//z//P/8//z//P/5v/z//P/8//z//P/8//z//P/8/8aABoA8//z//P/8//z//P/DQAaABoAJwAaAAAADQAaABoAGgAaABoADQDz/+b/AAANAAAADQAaAA0AJwAnAAAAGgAnAA0A8/8NABoADQANACcAGgAAAA0AAAAaABoAJwAnAA0AGgANAA0AGgAnABoAGgAAAPP/8/8AABoAGgANAOb/8/8aABoAGgAaABoAGgAaABoAGgANAA0ADQANABoAGgAaACcAGgAaABoA8/8aAA0AGgANAMz/DQAaABoAGgANABoADQAnABoAzP/M/9n/8/8aABoADQAaABoAGgANAPP/DQAAAAAAzP+//8z/2f8NAMz/8//z//P/GgDz/w0AGgAaABoAAADz/wAADQAAAA0AAADm/wAADQDz/wAAGgAaABoAGgAnAPP/8/8nAA0AGgAnABoADQANAA0AJwAaAA0AGgAaAA0AGgANAA0AJwAAAPP/AADm//P/8//z//P/5v/z//P/8//z//P/8//z//P/8/8AAAAAJwANAAAADQAAAA0A8//m//P/8/8NACcAJwAaACcAJwAaACcAJwAaACcAGgAaACcAJwAnAAAAGgAAAOb/GgAaABoAJwAAABoADQAaACcAAAANABoADQDz//P/8//m//P/8//z/xoAGgAaABoA8//z//P/DQANABoAGgAAAA0AGgANABoADQDZ/7//5v8NACcAAADM/+b/AADZ/ycADQDM/w0A2f/z/xoAGgAnABoA8//M/8z/2f8NABoAGgDz//P/AAANABoAGgDz//P/DQDz/xoAGgAaABoAGgAnAA0AGgAnABoA8//z/wAAGgAAAA0AGgDz/wAA2f/z/xoA5v/m//P/8/8AAPP/8//z//P/8//z/w0AGgAaAA0A8//m/+b/5v8NABoA8//z/+b/GgANAAAADQANAAAA8//m//P/DQAnACcAAADz//P/8/8NAA0AAAANAA0ADQANAPP/5v/z/xoADQANAA0AAAAaAAAADQDz//P/GgAnABoAGgAAAPP/AADz/xoAAADz/xoA8/8AABoA5v/m/wAA8/8AABoADQAaABoAAAAnABoAGgANAAAA5v8AABoA8/8NABoADQAaAA0AzP/Z/8z/2f8aAMz/zP/Z/8z/2f/m/xoAAADM/9n/zP8NABoA8//z/8z/5v8aAMz/DQAaAMz/2f/m//P/DQAAAOb/DQANAPP/8//z/xoAJwDz/w0AGgAnACcAAAAaAAAADQAAAA0AJwAaABoAGgDz//P/AAAaACcAGgAaAA0A8//z/wAAAAAaAA0ADQAaAPP/AAANABoADQDz/wAA8/8NACcADQDz/wAA8//z/xoAAADz/xoAGgAnACcADQANABoAGgANAPP/8/8AABoAJwANAA0A8//z/w0AAADz/wAA8/8AAPP/5v8NABoA8/8AABoA5v/z/xoADQAnACcAGgDz/wAAJwAaAPP/DQAnABoADQAAABoAJwAnABoAGgAaAOb/5v8NABoAJwANAPP/8//z//P/GgAaABoAGgDz/8z/zP/M/8z/v//M/xoADQANABoAGgAaAMz/5v8NAAAA5v/M/8z/zP/M/8z/zP+//9n/zP/M//P/AAANANn/5v8NAPP/GgAaABoAGgDz/+b/8/8NAOb/AADz//P/8//z/ycAGgAaABoA8//z/ycAGgDz//P/JwAaAA0ADQANABoAGgDm//P/GgAaAA0AAADz//P/JwANAPP/GgANAA0A8//z/xoAJwANAPP/DQAaAOb/5v8NABoADQAAAOb/5v/m//P/8//z/wAAGgAnABoA8//z/xoAGgAaAA0A8//z//P/8/8NABoADQAaABoAGgANABoADQDz/xoA8//m/wAAAADz/+b/8/8AAA0AJwAaABoADQAAACcANAAaABoA8//z/w0AGgAaAPP/8/8aAA0A8//z//P/8//m/w0AGgANAPP/zP/M/8z/zP/M/8z/2f/M/+b/DQAaABoAGgDZ/9n/GgANABoAGgANAAAA5v/z/wAAGgANAPP/8/8NACcA2f/m/wAA5v/m//P/8//z//P/5v/m//P/8/8NABoADQAaABoA8//z/w0AGgANAAAAJwAnAA0AGgAaABoAGgAaAAAA8//z//P/8//z/wAADQDz/w0AGgAaABoA5v/z/xoAGgANABoAGgANACcAAAAAACcAGgDz//P/DQDz//P/8//z/xoAGgAaAA0AGgA0ABoA8//z/+b/8/8aAAAAGgAnABoAGgAnABoAGgANAAAAGgAnABoAGgDz/wAADQAAAPP/8//z//P/5v/z/xoAJwAaABoA8/8NACcADQDz/+b/AAAAAAAADQDz/wAAAADz/xoADQAaAA0A8/8AAOb/5v/z/+b/8/8aABoAGgAaACcADQDM//P/DQAaABoAGgDM/8z/2f/M//P/2f/m/+b/v//Z/w0AGgAaAPP/zP/M/8z/DQAaAA0AGgAnABoADQDz//P/5v/m//P/8//z//P/5v/m//P/DQAaAPP/GgDz//P/GgDz/w0AJwAaAPP/DQAnABoAGgAaABoAGgAaAPP/8//z//P/AADz//P/8/8aAA0AGgANAPP/DQDm/+b/AAANAPP/DQAaAOb/5v/Z/9n/5v8NAA0A8/8AAA0A5v/z/wAAAAAAAOb/AAANAA0A2f/m/w0AAAAaAA0ADQANABoAGgANABoA8/8NAPP/8/8nAOb/AADz/+b/8/8AAAAAJwAnAAAAGgDz//P/AAAaAA0ADQANAOb/8/8AABoAGgAaAA0AGgAaAPP/8/8AAA0ADQANAA0AJwAaANn/zP/M/8z/2f/z/w0AGgAaABoAGgANABoAGgANAA0Av/+//8z/zP+//7//v/+//7//v/8NACcAAADM/8z/8//z//P/GgAaAPP/AAAAAPP/DQAaAA0AGgAnABoADQAaAPP/8/8AAPP/8//z//P/8//m/+b/8/8AAAAAGgAnAA0ADQAaAA0AJwAaAAAAAADz/wAAAAANAAAADQAAAA0AAADm/w0AAAAaABoA8//z//P/8//m/wAAGgDm//P/JwAaAAAAGgANAA0AGgAAABoAGgAaAAAA5v/z//P/AADz/wAAGgAnABoAAAAAAPP/8/8aABoAGgAaAAAAGgAnAA0ADQAAAOb/5v/z//P/5v/z//P/8/8AAA0AGgDz/+b/5v/z/w0AAAAAAPP/GgDz//P/JwAaABoAGgAaAA0ADQAaABoA2f/M//P/GgAaAA0AGgANAMz/zP+//9n/DQAAAL//v/8aAMz/5v8aAOb/8//Z//P/8/8NANn/8/8aAMz/AAAaAA0AGgANAA0AGgAaAAAAGgAaAA0AJwANAPP/AADz//P/8/8aABoAGgANAAAAAADm//P/AAAAABoAJwAaAPP/8//z//P/DQAaABoAGgANABoAGgAaABoADQANABoA8//m/w0ADQAaAPP/8/8aAPP/8/8NAAAA2f/Z//P/5v/m/+b/DQAaAPP/8/8AAA0AAADm//P/DQAaABoADQAaAA0ADQAaABoAGgAaAA0ADQAAAPP/5v/z/wAAAAAaAA0ADQAAAPP/AAAAAPP/8/8NAPP/5v/m/wAADQAAAPP/8/8NABoAJwAaABoADQAaABoA8/8NAA0ADQANAAAA8//m/+b/5v/z/xoAGgDZ/7//zP/M/9n/v//Z/w0AzP+//8z/v//M/7//AAAnABoAGgANABoAGgAaAA0AGgAaABoAGgAaAA0AAAANADQADQAAABoAAADm//P/5v8NAA0A8/8aAAAA5v/z//P/AAAaAA0A8//z/+b/8//z/+b/5v8NABoAAADz//P/AAANABoAJwAnABoAGgANAAAAGgAaAPP/8/8aABoA8//z//P/AADz//P/DQDz//P/8//z/+b/AAAaACcAAADz//P/8/8aACcAJwAaABoAGgAaABoAGgAaABoADQANAPP/8//z//P/AADz//P/GgANABoAGgAAAPP/8/8NABoAGgAAAPP/8/8AAAAADQAaAPP/GgAnABoAGgDz//P/DQANABoADQANABoA8//m/w0A5v/z/ycAGgAaABoAAAAaAPP/2f8NAA0A8//Z/9n/zP/M/8z/5v/z/9n/2f/z/w0AGgAaABoAGgAaACcAJwDm/ycAJwDz/xoAzP/M/9n/JwAnAPP/5v/z//P/2f/z/xoAGgANABoAGgAaAAAADQAaAA0A8/8nABoA8/8AAOb/8//z//P/DQAaAPP/8/8aAAAAGgAaABoAGgAAABoAGgANAAAAGgAaABoAGgDz//P/8/8NAAAAAAAaAOb/DQAaAPP/AADm/+b/8/8NABoADQANAAAAGgAaAA0AJwAaAAAADQDm/+b/8//m/+b/5v/z/+b/5v/z//P/AADz//P/GgANAPP/8//z/xoAJwDz//P/GgAaAPP/AADz//P/AADz/ycAGgAAAA0A8//z/wAAGgAaACcAGgAaAA0A8//z//P/5v8NABoA8//z/wAAAADm/+b/5v+//9n/GgAnABoAAAANABoADQAnABoA5v/z/xoAGgDM/8z/zP/M/8z/2f8aAA0AGgAaANn/v//Z/+b/2f8NABoAGgDz//P/DQDz/wAA8/8aABoAGgAaAPP/5v/z/w0AAADm//P/GgANAA0AGgAaAPP/8//z//P/DQAAAA0AAAANAA0A8//z//P/8//z/w0A8//Z//P/8/8nACcA8//z//P/AAAnACcA8/8AABoAGgAnACcADQDz/xoAJwANAAAAAAAAABoADQAnAA0AAADz//P/DQAAAAAAAAAaACcADQAaAAAA8/8NABoAGgANAA0AGgAAAPP/GgAAAPP/8/8NACcAGgAAAPP/AADz/w0AGgAAAPP/GgAnACcAGgANABoADQAaABoAAAAAAPP/8/8AAPP/GgAaAPP/DQAAAAAA8/8NAPP/zP+//8z/zP/M/9n/zP/M/8z/5v/M/8z/2f/M//P/2f+//8z/zP/Z/9n/zP/z//P/8//z/8z/zP/M/+b/8//z//P/5v/z//P/5v/z//P/AADz//P/8//m//P/AAAAAPP/5v/m/+b/AADz//P/8/8aABoA8//z//P/GgAaAAAAGgDz//P/8//z//P/8/8AAPP/AAAAAPP/GgANAOb/8/8NABoAJwAnABoADQAAAPP/5v8NABoADQANAOb/8/8AAOb/DQANAA0AJwDm/xoAGgDm/xoAGgANAA0A8//z/wAAGgDm/+b/DQANABoA8//z//P/5v8aACcAJwDz//P/DQAAABoAAAAaAAAA5v8aAA0ADQAAAA0A8//m//P/AADz/+b/8/8NAA0A8//m//P/8//z//P/AAANACcADQDm/8z/v/+//9n/zP/Z/7//zP/Z/+b/GgDM/7//zP/Z/w0A2f8NACcAzP/M/9n/GgANAMz/AADZ/8z/GgAaAA0A5v8AAA0ADQAaABoADQDm/wAAJwDz/w0AGgDz/+b/5v/z/xoADQDz/wAADQAaABoAAADz//P/DQANABoAAAANABoADQAaAPP/AAAaAA0ADQDz//P/GgAAAA0AAAAAABoADQAaAPP/AADz/xoAGgDz/wAAAAANAAAA8//z//P/DQDz//P/AADz/xoAJwDz/wAAAAAaAA0A8//z/+b/AAAaAA0AAADz/xoAGgAAAAAAAAAAAOb/GgANAOb/NAAaAPP/JwAAAOb/8//z/wAA5v/z/w0AAADz//P/5v/m/wAADQAAABoA8//m//P/8/8aAA0A8//z/+b/8//z/xoAJwDm//P/GgDM/7//sv/m/w0AGgDz/8z/DQDM/9n/zP/M/w0AzP/Z/wAADQAaAA0AJwAaANn/zP/M/8z/v//M/xoAAAAAABoA8/8aABoAAADz//P/8/8aAA0AGgAnAA0AAAAaABoA8/8NAAAADQANAOb/8/8AAPP/8//z/xoAGgAaAPP/8/8NAA0ADQDz/wAA8//m//P/8/8aACcA8//z//P/5v8NAPP/8/8NAOb/DQDz/wAADQDm/9n/2f/m/+b/8/8AAPP/8//m//P/GgDz//P/8//z//P/8//z//P/8/8NAPP/8//z/+b/8//z/+b/8//m//P/8//z//P/8/8NAAAA8//z//P/8//z//P/8//z//P/8//z//P/8//z//P/8//m//P/GgAAAPP/8/8AAAAA5v/z//P/8/8AAAAAGgAaAA0AGgAaAAAA8//M/8z/zP+//8z/v/+//8z/zP/M/8z/2f/m//P/zP+//+b/zP+//8z/zP+//7//zP/M/8z/zP/M/xoADQDm/xoAGgDz//P/8//m//P/DQDm/+b/5v/z/w0A8//m//P/8//z//P/8/8AAPP/8//m/+b/8/8NAPP/5v/m//P/8/8NABoA8//z/xoAGgAaAPP/8//z//P/AADz//P/DQDz//P/8/8aABoA8/8AAAAAAAAAABoAJwAaABoAAADz//P/8//z//P/AADz//P/AADz//P/AAAaABoAGgAaAPP/8/8AAPP/8/8AAA0AGgAaACcAGgAAACcAAADm//P/8//z//P/AADz//P/8//z/+b/8/8NAAAA8//z//P/8//z//P/8//z/+b/8//z//P/8//z/w0AGgAaAAAAAADz/9n/AADm/+b/GgANABoADQAaACcA8/8NACcAJwAnAPP/8//Z/8z/zP/Z//P/8//z/8z/v//M/8z/8/8aAPP/GgAaAAAA8//z/xoAAAANABoAGgAAAOb/GgAnABoAGgAnABoAAAANABoAGgAaABoAGgAAABoAGgAaAAAA8/8AAAAA8//z//P/AAAaACcAJwAAAPP/8//m/xoAJwAaAAAAAADz//P/8/8NABoADQANABoADQDz/wAA8/8AABoADQAaAA0AGgANAPP/8/8AAA0AGgAaAA0AAADz//P/8/8aABoAGgAaAA0AGgAAAPP/8/8NABoAGgAaACcADQAAAPP/8/8AAA0AGgAaAPP/8//z//P/8//m//P/8/8NAA0AGgAAAPP/8//z/w0ADQANAA0ADQDz/wAA8//Z//P/AAAAAA0AJwAaANn/zP8NABoAGgAnABoAGgAaANn/v//Z//P/8/8aABoAGgAaAA0AJwANAPP/DQDM/xoAGgDz/w0A8/8NAPP/5v/z//P/AADz//P/8//m/w0A8//z/w0AGgANAPP/AADz/w0AJwANABoAGgAaABoA8//z/+b/8/8aAAAADQAaABoADQAaAAAA5v/z//P/8/8NABoAGgAaABoAGgDz/wAADQANAA0AGgAaAPP/5v/z/xoA8//z/ycA8//z//P/8//z/wAAGgAaAAAA8/8AAPP/8//z//P/AADz//P/AAANABoAGgAaAAAADQAaABoADQAaABoA8/8aAPP/8/8aAA0AGgDz/+b/5v/m//P/GgAnABoA8/8aAPP/8/8aAPP/GgDz/w0AGgDz/xoAGgDz/xoAAAAaABoAJwAaABoAAADZ/w0A5v/m/ycADQAaAA0AGgANAMz/DQAnABoAGgANABoA8//z/xoAGgAnABoAGgDZ/7//zP/M/9n/zP/M//P/8//z/+b/8//z//P/GgAaABoADQAaAPP/8/8nABoADQANAA0AGgAaAAAAGgAaAPP/8/8NABoA8/8aABoA8/8AAAAA5v/z//P/DQAAAOb/8/8AABoAGgAAAA0AGgANABoAGgDz/xoAGgANABoADQDz/9n/2f/z/+b/5v/z/+b/8//z/+b/8//z//P/8/8AAOb/8/8NAPP/GgAaAA0AAAAAABoADQAaACcA5v/z/xoADQANABoAGgAAAPP/8//z//P/GgAaABoAGgAaAA0A8//z/xoAGgAAAA0AJwAnABoAGgAaAA0AGgAaAA0ADQAaABoAAADz/+b/8//z/+b/AAANAOb/zP+//8z/5v+//8z/GgANAPP/v//M/wAAJwAaAMz/GgAaAA0AGgDZ/8z/5v8aAMz/5v8NANn/v//M/w0AAADm//P/5v/z/+b/5v/z/+b/AAANAAAA5v/z/xoAGgAaABoAGgDz/+b/GgAaABoAGgDz//P/GgAaACcADQDz/w0A8//z//P/8//z//P/AAAaAAAA8/8aAPP/DQAnAA0ADQANACcAGgDz/w0AGgAnABoAGgAaABoADQDz//P/8//z//P/AAAnACcA8/8NAPP/5v8aAPP/AAAaABoAAADz/wAA8/8aAA0A8/8AAPP/8/8aABoA8/8NABoADQDz/w0AAAANACcA8//z/xoAGgDm//P/JwAaAPP/GgAaAA0AAADz/xoAGgAaACcAGgAaAA0AGgAaAA0ADQANAAAA8//z/+b/2f/m/9n/zP/M/8z/zP/M/8z/zP8aACcA8//M/8z/zP/Z/w0AGgAaABoADQAaACcAGgAaACcADQANACcAJwAnABoAJwAnABoA8//z//P/8//m//P/5v/z/xoAAADz/+b/8//m/wAAJwANAA0ADQAaACcADQAaAPP/GgAaAOb/DQDz/wAADQAaAPP/AAAaAPP/JwDz//P/GgDz/w0AAADz/wAAGgAnAA0A8/8NAPP/8/8NAPP/DQANABoADQANABoAAAANAAAA8//z//P/8//z//P/5v/z//P/GgAaAOb/5v/z/w0AGgANAA0AGgAnACcADQAnACcAJwAaABoAGgDz/xoAJwANAAAA5v8AABoA8//z//P/8//z//P/8//z//P/8//z/wAADQDz//P/8//m/wAADQANABoAGgAaACcADQDm/8z/5v/z/w0A8/8aABoA8/8AAAAAGgDz//P/AADz/xoAAAANABoAGgAnABoAzP/Z/w0A8//z//P/5v/M//P/JwDz/wAA5v/z/xoAAAANAPP/DQAaAPP/8/8NABoA8/8aABoAGgAaABoAAADz/xoADQAaABoAGgAnACcAJwAaAPP/5v8NABoADQAAAAAA8/8NACcADQDz//P/8//z//P/DQAAACcAGgANABoAAADm/+b/GgDz//P/AADz/w0A8//z//P/JwAAAPP/JwDz/w0ANAAaABoAGgAaABoAAAANACcAGgAnACcAGgAaAAAADQAaACcA8//z/xoAAAAaAAAADQAnAA0AAADz//P/8/8NABoADQDz/+b/8//z//P/8/8AABoAAADz//P/GgAaAPP/DQDz/wAAJwAaABoADQAaAA0AGgAaAL//2f8aAMz/zP/m/xoAzP8AAA0A2f8aAOb/DQDZ//P/GgDZ/w0A5v+//9n/8//M/8z/2f/M/xoAJwANAPP/8/8nAPP/AAAaAPP/GgANAAAAGgAaABoA8/8NACcADQDz/wAAGgANABoAGgAaABoAAADm/+b/AAANAPP/8//m/wAA8/8NABoADQANABoA8//z/xoA5v8AAA0A8//m/+b/5v8AAA0A5v8aABoAGgAAAOb/AADm/w0AGgANAPP/8//z//P/DQDz/wAADQDz/+b/5v8aABoA2f8aACcA8//z//P/DQAaABoAGgDz/+b/8/8aABoA8/8aAPP/AAANAOb/8/8aABoAAADz/xoADQAAABoAJwANAAAADQDz/w0A8//z/xoADQAaAAAADQAnAA0A8//m/wAAGgAnAAAA2f/z/8z/DQAnAAAAJwDz/9n/sv/z/wAA5v8AAOb/DQDm/9n/8//m/xoAAADZ/8z/v/8aABoAGgANAMz/DQAnANn/AAANAPP/DQAAAOb/AAANAPP/8/8aAA0A5v/z/xoA8//z/w0ADQAAABoAGgDz/w0ADQDz/xoAJwDz/+b/DQDz//P/GgAaAAAA8//z/w0AGgAaABoAAADz/+b/5v/z//P/AADz/+b/5v/z//P/8//z//P/DQAnAPP/AAAaAPP/DQAAAPP/GgAnAA0A8//z/xoADQAaADQAGgAaABoAGgAAABoAGgDz//P/8//m/wAAGgANACcAGgDz/+b/JwAnAAAAGgANAPP/8//z//P/8//z//P/8/8aACcA8/8AABoAGgAAAPP/GgAAAA0AGgAaACcADQAaAAAA5v8AABoAGgDM/7//2f8NABoA2f/z/xoAGgAaAPP/8//M/9n/2f/M//P/JwDm/9n/DQDm/wAADQANAPP/5v/Z/8z/5v8NABoA8//z/w0A5v8AACcADQDz/w0AGgAaABoAGgDz//P/DQAaAOb/5v8aABoAAAAaAPP/5v8nACcA8//z/ycADQDz//P/5v8AABoAGgAaABoAJwAnABoADQANAAAA8/8NABoAGgANAPP/8//z//P/5v/z/xoADQDz/9n/2f8aAA0A8//z//P/JwAaAPP/8//z//P/AADz/+b/5v8NABoA8//m/wAA8//z//P/8/8aABoA8//z/wAADQAaABoA8//m//P/8/8AABoA8//z/wAA5v8AABoADQAnAPP/5v8aAPP/8/8aAPP/8/8NAAAA8//m//P/DQAaABoAAADm/w0AJwAAANn/5v/m/7//2f8AAOb/DQDZ/8z/2f/m/ycADQAnAA0AzP/Z/xoAGgAAAA0AzP/Z/xoADQAaABoAzP+//+b/8/8nACcA8/8NAA0A8/8aAA0A8/8NABoAJwDz/w0AGgDm//P/8/8NAAAA8/8NAPP/GgAaAPP/8//m//P/AAANACcAJwANAPP/8/8AABoAGgANABoAJwANAAAA8//z/w0AAADz/xoAGgAaABoAGgAnABoADQAaAOb/AAAnAAAADQAnAA0ADQAaAAAA8//z//P/8//z/ycAGgAAAAAADQAaAPP/JwA0APP/GgAaAPP/8/8aABoAAAAaAPP/5v8NABoAGgANABoADQDz//P/8/8AABoAGgANABoAGgAaAPP/2f8nABoA8/8aABoAGgAaAAAAAAANAA0ADQDz/w0AGgAAAPP/zP8AABoAzP8NAA0AzP8NANn/AAAaABoADQDM/+b/zP/M/8z/zP/Z/+b/2f/M/8z/zP8NACcA2f/M/xoAJwANAA0AGgDz/w0AJwAaAA0AAADz//P/5v/z/+b/AAAaAPP/AAAnABoA8/8NABoADQANAAAAGgAnABoADQAaAPP/8/8NAPP/AADz/wAAAAAaABoA8//z//P/DQAnAA0AGgAaABoADQAAAPP/8//m/xoADQDm/wAA5v/m/+b/5v8NAA0AJwAAAPP/AAAAAAAA8/8AABoAGgANAAAA5v/z/+b/5v8NAPP/8//z/w0A8//m//P/5v/z//P/JwAnAOb/8/8NABoAGgDm/+b/DQDm//P/8//z/w0ADQANAA0ADQAAAPP/5v/z//P/5v/m//P/8/8aABoA8//z/w0AGgAaABoADQANAA0AzP+//9n/8/8aAL//2f/m/8z/DQAnABoA5v/m/wAAGgANABoADQDz/xoA8/8NABoAGgAaANn/zP+///P/8//z/w0A8/8AAAAADQDz/+b/GgAaABoAGgAaAPP/GgAaABoAGgANABoA5v/z/ycAGgDz/+b/8//z/w0ADQAAAAAA5v/z//P/8/8aACcAGgDz//P/8//m/wAA8//z/wAA5v8AAOb/8//z/wAADQDz/wAA8//z/wAA8//z//P/8//z/xoAAADm/xoAGgAaAPP/8/8NABoAGgAaABoAGgDz//P/GgAAABoAAADm//P/8/8aAPP/GgAnAPP/8/8AABoAJwAaAOb/8/8AAPP/5v/z/w0A8//z/wAAAAAaAA0AGgAaABoAGgAaAA0AGgAaABoAGgAaABoADQAaAOb/8//z/8z/v//M/+b/zP+//8z/2f/z/xoAJwDz/wAA8//z/w0AzP+//9n/zP/M/8z/2f8NAMz/2f8aAMz/zP/M/8z/8/8AAOb/GgAnAPP/8//m/wAADQANABoAAAAaABoAGgAaAA0AGgAaABoAJwANAPP/8//z/wAADQAaABoADQANABoAGgANABoAAADz//P/8/8aABoAGgAAAAAAJwANAAAAGgAaABoADQANABoADQDm/w==\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "index = np.random.randint(len(simu_wave), size=1)\n",
    "print(f\"Index: {index}\")\n",
    "print(simu_label[index])\n",
    "print(simu_phoneme[index])\n",
    "ipd.Audio(simu_wave[index], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC:\n",
    "    '''\n",
    "    This is the Mel-Frequency Cepstral Coefficients, MFCCs Transformation\n",
    "    including\n",
    "        1. Pre-emphasis\n",
    "        2. Framing\n",
    "        3. Hamming window\n",
    "        4. Short-time Fourier Transform\n",
    "        5. Mel triangular bandpass filters\n",
    "        6. Log energy\n",
    "        \n",
    "    not including\n",
    "        7. Discrete cosine transform\n",
    "        8. Delta cepstrum\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Args:\n",
    "            alpha: coefficient applied when applying pre-emphasis, usually between (0.95, 0.98)\n",
    "            frame_size: duration of one frame in second\n",
    "            frame_stride: stride duration of one frame in second\n",
    "            n_fft: decided number while applying Fast Fourier Transformation\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "        self.n_fft = n_fft\n",
    "        self.n_filter = n_filter\n",
    "        self.apply_delta = apply_delta\n",
    "        \n",
    "    def mfcc(self, samples, sample_rate):\n",
    "        samples_emphasized = self.pre_emphasis(samples)\n",
    "        frames, total_samples_in_frame = self.framing(samples_emphasized, sample_rate)\n",
    "        frames = self.hamming_window(frames, total_samples_in_frame)\n",
    "        power_spectrum = self.stft(frames)\n",
    "        fbank = self.filter_bank(power_spectrum, sample_rate)\n",
    "        energy = self.log_energy(fbank)\n",
    "        mfcc_features = np.column_stack((energy, fbank))\n",
    "        \n",
    "        if self.apply_delta:\n",
    "            mfcc_feat = mfcc_features.T\n",
    "            \n",
    "            d_mfcc_feat = self.delta(mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_feat.T, d_mfcc_feat.T), axis=1)\n",
    "\n",
    "            dd_mfcc_feat = self.delta(d_mfcc_feat, 2)\n",
    "            mfcc_features = np.concatenate((mfcc_features, dd_mfcc_feat.T), axis=1)\n",
    "\n",
    "        return mfcc_features\n",
    "    \n",
    "    def pre_emphasis(self, samples):\n",
    "        return np.append(samples[0], samples[1:] - self.alpha*samples[:-1])\n",
    "    \n",
    "    def framing(self, samples, sample_rate):\n",
    "        samples_in_frame = int(np.ceil(self.frame_size*sample_rate))                           # number of samples in one frame\n",
    "        sample_stride = int(np.ceil(self.frame_stride*sample_rate))                            # sample stride in each iteration\n",
    "        frame_num = int(np.ceil(\n",
    "            (len(samples) - samples_in_frame)/sample_stride) + 1)                              # number of iterations\n",
    "\n",
    "        padding_num = (frame_num-1)*sample_stride + samples_in_frame - len(samples)            # length for padding\n",
    "        padding = np.zeros(padding_num)                                                        # prepare the padding array\n",
    "        samples_padded = np.append(samples, padding)                                           # padded sample array\n",
    "\n",
    "        # index to pick all the overlapping samples\n",
    "        index_each_frame = np.arange(samples_in_frame)\n",
    "        index_each_stride = np.linspace(0, len(samples_padded) - samples_in_frame, frame_num).astype(np.int32)\n",
    "        index = np.tile(index_each_frame, reps=(frame_num, 1)) + np.tile(index_each_stride, reps=(samples_in_frame, 1)).T\n",
    "\n",
    "        return np.array([samples_padded[[i]] for i in index]), samples_in_frame                # frames is a 2D array\n",
    "    \n",
    "        \n",
    "    def hamming_window(self, frames, samples_in_frame):\n",
    "        # self.frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (self.total_samples_in_one_frame - 1))\n",
    "        frames *= np.hamming(samples_in_frame)\n",
    "        return frames\n",
    "        \n",
    "    def stft(self, frames):\n",
    "        magnitude = np.abs(np.fft.rfft(frames, n=self.n_fft))                                  # magnitude of the FFT\n",
    "        return (1.0/self.n_fft) * magnitude**2                                                 # power spectrum\n",
    "    \n",
    "    def filter_bank(self, frames, sample_rate):\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = self.hz2mel(sample_rate/2)                                             # highest frequency of the Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.n_filter+2)                 # Equally spaced in Mel scale\n",
    "        bins = np.floor((self.n_fft+1) * self.mel2hz(mel_points) / sample_rate)                # bins for FFT\n",
    "        \n",
    "        fbank = np.zeros((self.n_filter, self.n_fft//2 + 1))\n",
    "        for j in range(self.n_filter):\n",
    "            for i in range(int(bins[j]), int(bins[j+1])):\n",
    "                fbank[j, i] = (i - bins[j]) / (bins[j+1] - bins[j])\n",
    "            for i in range(int(bins[j+1]), int(bins[j+2])):\n",
    "                fbank[j, i] = (bins[j+2] - i) / (bins[j+2] - bins[j+1])\n",
    "        \n",
    "        mel_fbanks = np.dot(frames, fbank.T)\n",
    "        mel_fbanks = np.where(mel_fbanks == 0, np.finfo(float).eps, mel_fbanks)\n",
    "        mel_fbanks = 20 * np.log10(mel_fbanks)                                                 # dB\n",
    "        \n",
    "        return mel_fbanks\n",
    "        \n",
    "    def log_energy(self, mel_fbanks):\n",
    "        return np.log(np.sum(mel_fbanks**2, axis=1))\n",
    "\n",
    "    def hz2mel(self, hz):\n",
    "        return 2595 * np.log10(1 + hz/700)  # Convert Hz to Mel\n",
    "    \n",
    "    def mel2hz(self, mel):\n",
    "        return 700 * (10**(mel/2595.0) - 1) # Convert Mel to Hz\n",
    "    \n",
    "    def delta(self, mfcc_features, neighbor_len=2):\n",
    "        denominator = 2 * sum([i**2 for i in np.arange(1, neighbor_len+1)])\n",
    "        delta_feat = np.empty_like(mfcc_features)\n",
    "        padded = np.pad(mfcc_features, ((neighbor_len, neighbor_len), (0, 0)), mode='edge') # padded version of feat\n",
    "        for t in range(len(mfcc_features)):\n",
    "            delta_feat[t] = np.dot(np.arange(-neighbor_len, neighbor_len+1), \n",
    "                                      padded[t : t+2*neighbor_len+1]) / denominator\n",
    "        return delta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCApplier:\n",
    "    '''\n",
    "    This is the MFCC applier for applying MFCC \n",
    "    and pad zeros for fitting the data into Encoder-Decoder Model with Attention\n",
    "    which we will build later\n",
    "    '''\n",
    "    def __init__(self, alpha, frame_size, frame_stride, n_fft, n_filter, apply_delta=False):\n",
    "        '''\n",
    "        Arg:\n",
    "            mfcc: build by MFCC class for transform inputs\n",
    "            decide_size: a 2^k number which will make the inputs reshape into X*decide_size\n",
    "                         which will help us build the pyramidal RNN encoder\n",
    "        '''\n",
    "        self.mfcc = MFCC(alpha=alpha, \n",
    "                         frame_size=frame_size, \n",
    "                         frame_stride=frame_stride, \n",
    "                         n_fft=n_fft, \n",
    "                         n_filter=n_filter, \n",
    "                         apply_delta=apply_delta)\n",
    "        \n",
    "        \n",
    "    def apply(self, inputs, sample_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: wave data that one would like to process\n",
    "        '''\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        sample = self.mfcc.mfcc(inputs[0, :], sample_rate)\n",
    "        sample_shape = sample.shape\n",
    "        \n",
    "        outputs = np.zeros(((input_shape[0], sample_shape[0], sample_shape[1])))\n",
    "        \n",
    "        for i in np.arange(input_shape[0]):\n",
    "            outputs[i, :, :] = self.mfcc.mfcc(inputs[i, :], sample_rate)\n",
    "            print(f\"Applying MFCC to {i+1}th case\", end=\"\\r\")\n",
    "            \n",
    "        print()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying MFCC to 20000th case\n",
      "Simulated MFCC wave: (input size, time steps, MFCC dimension) (20000, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.95\n",
    "FRAME_SIZE = 0.025\n",
    "FRAME_STRIDE = 0.01\n",
    "N_FFT = 512\n",
    "N_FILTER = 12\n",
    "\n",
    "mfcc_applier = MFCCApplier(alpha=ALPHA, \n",
    "                           frame_size=FRAME_SIZE, \n",
    "                           frame_stride=FRAME_STRIDE, \n",
    "                           n_fft=N_FFT, \n",
    "                           n_filter=N_FILTER, \n",
    "                           apply_delta=True)\n",
    "\n",
    "mfcced_simu_wave = mfcc_applier.apply(simu_wave, sample_rate=SAMPLE_RATE)\n",
    "print(\"Simulated MFCC wave: (input size, time steps, MFCC dimension) {}\".format(mfcced_simu_wave.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (20000, 99, 39)\n",
      "Output Shape: (20000, 7)\n",
      "\n",
      "TOKEN\t--->\tWORDS\n",
      "=======================\n",
      "1\t--->\t<start>\n",
      "5\t--->\ts\n",
      "16\t--->\teh\n",
      "9\t--->\tv\n",
      "10\t--->\tah\n",
      "3\t--->\tn\n",
      "2\t--->\t<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phoneme_tensor, phoneme_tokenizer = preprocesser.tokenize(simu_phoneme)\n",
    "\n",
    "print(\"Input Shape: {}\".format(mfcced_simu_wave.shape))\n",
    "print(\"Output Shape: {}\".format(phoneme_tensor.shape))\n",
    "\n",
    "\n",
    "for tensor in phoneme_tensor[:1]:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)\n",
    "    print()\n",
    "    \n",
    "# Split the data into size (training set, testing set) (18000, 2000)\n",
    "TRAIN_SIZE = 0.95\n",
    "\n",
    "wav_tensor, wav_tensor_val, phoneme_tensor, phoneme_tensor_val = train_test_split(mfcced_simu_wave, \n",
    "                                                                                  phoneme_tensor, \n",
    "                                                                                  train_size=TRAIN_SIZE, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 128\n",
    "WAV_SIZE = len(wav_tensor)\n",
    "VAL_WAV_SIZE = len(wav_tensor_val)\n",
    "PHONEME_SIZE = len(phoneme_tokenizer.word_index) + 1\n",
    "STEP_PER_EPOCH = WAV_SIZE // BATCH_SIZE\n",
    "\n",
    "wav_tensor = tf.convert_to_tensor(wav_tensor, dtype=tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((wav_tensor, phoneme_tensor)).shuffle(WAV_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "wav_tensor_val = tf.convert_to_tensor(wav_tensor_val, dtype=tf.float32)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((wav_tensor_val, phoneme_tensor_val)).shuffle(VAL_WAV_SIZE)\n",
    "\n",
    "# avoid running out of memory\n",
    "simu_wave = None\n",
    "mfcced_simu_wave = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input Shape: (4, 99, 39)\n",
      "Example Output Shape: (4, 7)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(f\"Example Input Shape: {example_input_batch.shape}\")\n",
    "print(f\"Example Output Shape: {example_target_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters, stride=None):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        self.filters1, self.filters2, self.filters3 = filters\n",
    "        \n",
    "        if stride is None:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.filters1, self.stride, padding='valid', activation=\"relu\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.filters2, kernel_size, padding='same', activation=\"relu\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv1D(self.filters3, 1, padding='valid', activation=\"relu\")\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        if self.stride != 1:\n",
    "            short_cut = tf.keras.layers.Conv1D(self.filters3, \n",
    "                                               self.stride, \n",
    "                                               padding='valid', \n",
    "                                               activation=\"relu\")(input_tensor)\n",
    "            x += short_cut\n",
    "        else:\n",
    "            x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 99, 39)\n"
     ]
    }
   ],
   "source": [
    "resnet_block = ResnetIdentityBlock(32, [1, 2, example_input_batch.shape[-1]])\n",
    "resnet_output = resnet_block(example_input_batch)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(resnet_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_identity_block\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              multiple                  40        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  4         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  66        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  8         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  117       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  156       \n",
      "=================================================================\n",
      "Total params: 391\n",
      "Trainable params: 307\n",
      "Non-trainable params: 84\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_block.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder for MFCC transformed wave data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate, \n",
    "                 units,\n",
    "                 squeeze_time):\n",
    "        '''\n",
    "        Args:\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size            \n",
    "            dropout_rate: layer dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_units = lstm_units    \n",
    "        self.squeeze_time = squeeze_time\n",
    "        \n",
    "        # conv1d\n",
    "        self.feat_extract = tf.keras.layers.Dense(units=units, activation=\"relu\")\n",
    "        self.feat_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # ResNet\n",
    "        self.resnet1 = ResnetIdentityBlock(32, filters=[units//4, units//2, units])\n",
    "        \n",
    "        units *= squeeze_time\n",
    "        self.resnet2 = ResnetIdentityBlock(64, [units//4, units//2, units])\n",
    "        \n",
    "        units *= squeeze_time\n",
    "        self.resnet3 = ResnetIdentityBlock(128, [units//4, units//2, units])\n",
    "        \n",
    "        # Encoder lstm\n",
    "        self.enc_lstm = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                             return_sequences=True, \n",
    "                                             return_state=True, \n",
    "                                             kernel_initializer=\"lecun_normal\",\n",
    "                                             activation='tanh', \n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='orthogonal', \n",
    "                                             dropout=dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call pyramidal LSTM neural network encoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: wave input\n",
    "        '''\n",
    "        x = self.feat_extract(inputs)\n",
    "        x = self.feat_dropout(x)\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet1(x)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x = self.reshape_pyramidal(x)\n",
    "        \n",
    "        x = self.resnet3(x)\n",
    "        \n",
    "        # encoder output layer\n",
    "        fw_outputs, fw_state_h, fw_state_c = self.enc_lstm(x)\n",
    "            \n",
    "        return fw_outputs, fw_state_h, fw_state_c\n",
    "    \n",
    "    def reshape_pyramidal(self, outputs):\n",
    "        '''\n",
    "        After concatenating forward and backward outputs\n",
    "        return the reshaped output\n",
    "        \n",
    "        Args:\n",
    "            outputs: outputs from LSTM\n",
    "            squeeze_time: time step one would like to squeeze in pyramidal LSTM\n",
    "        '''\n",
    "        batch_size, time_steps, num_units = outputs.shape\n",
    "\n",
    "        return tf.reshape(outputs, (batch_size, -1, num_units * self.squeeze_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 11, 256)\n",
      "Encoder forward state h shape: (batch size, units) (4, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_DROPOUT_RATE = 0.2\n",
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  squeeze_time=3, \n",
    "                  units=64)\n",
    "\n",
    "# If set the batch size greater than 4, memory of GPU will run out\n",
    "sample_output, fw_sample_state_h, fw_sample_state_c = encoder(example_input_batch)\n",
    "# sample_output, sample_state = encoder(example_input_batch)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder forward state h shape: (batch size, units) {}'.format(fw_sample_state_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  2560      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_1 (Res multiple                  20016     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_2 (Res multiple                  324240    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_3 (Res multiple                  5562288   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  852992    \n",
      "=================================================================\n",
      "Total params: 6,762,096\n",
      "Trainable params: 6,759,184\n",
      "Non-trainable params: 2,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.W2 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.V = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(fw_sample_state_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder for output phonemes\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 target_sz, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_sz, \n",
    "                 dropout_rate):\n",
    "        '''\n",
    "        Args:\n",
    "            target_sz: target size, total phoneme size in this case\n",
    "            embedding_dim: embedding dimension\n",
    "            lstm_units: LSTM units number\n",
    "            batch_sz: batch size\n",
    "            dropout_rate: dropout ratio\n",
    "            rnn_initial_weight: type of weight initialization\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.target_sz = target_sz\n",
    "        self.lstm_units = lstm_units\n",
    "        self.embedding = tf.keras.layers.Embedding(target_sz, embedding_dim)\n",
    "        \n",
    "        # attention model\n",
    "        self.attention = LuongAttention(lstm_units)\n",
    "        \n",
    "        # decoder rnn            \n",
    "        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          kernel_initializer=\"lecun_normal\",\n",
    "                                          activation='tanh',\n",
    "                                          recurrent_activation='sigmoid', \n",
    "                                          recurrent_initializer='orthogonal', \n",
    "                                          dropout=dropout_rate)\n",
    "    \n",
    "        # Fully-connected\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(target_sz, activation=\"softmax\")\n",
    "        \n",
    "        # build layer info dictionary\n",
    "        self.layer_info = dict()\n",
    "\n",
    "\n",
    "    def call(self, inputs, enc_hidden_h, enc_hidden_c, enc_output):\n",
    "        '''\n",
    "        call LSTM decoder\n",
    "        \n",
    "        Args:\n",
    "            inputs: target output, following phoneme for wave data input in this case\n",
    "            enc_hidden_h: encoder hidden state h\n",
    "            enc_hidden_c: encoder hidden state c\n",
    "            enc_output: encoder outputs\n",
    "        '''\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_hidden_h, enc_output)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the 2-layer LSTM (Decoder)\n",
    "        x, state_h, state_c = self.lstm1(x)\n",
    "\n",
    "        # dense layer before final predict output dense layer\n",
    "        x = tf.reshape(x, (-1, x.shape[-1]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        # output shape == (batch_size, phoneme size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, (state_h, state_c), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, phoneme size) (4, 22)\n"
     ]
    }
   ],
   "source": [
    "DECODER_DROPOUT_RATE = 0.2\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)\n",
    "\n",
    "sample_target_size = tf.random.uniform((BATCH_SIZE, 1))\n",
    "sample_decoder_output, sample_decoder_hidden, attention_weights = decoder(\n",
    "    inputs=sample_target_size, \n",
    "    enc_hidden_h=fw_sample_state_h, \n",
    "    enc_hidden_c=fw_sample_state_c, \n",
    "    enc_output=sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, phoneme size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2816      \n",
      "_________________________________________________________________\n",
      "luong_attention_1 (LuongAtte multiple                  131841    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  656384    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  1430      \n",
      "=================================================================\n",
      "Total params: 808,919\n",
      "Trainable params: 808,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "'''\n",
    "Candidate optimizer:\n",
    "    1. Adam\n",
    "    2. Nadam\n",
    "    \n",
    "    the preformence of other optimizers are not good\n",
    "'''\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, \n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.math.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    if tf.is_tensor(real):\n",
    "        real = real.numpy()\n",
    "        \n",
    "    if tf.is_tensor(pred):\n",
    "        pred = pred.numpy()\n",
    "        \n",
    "    if np.isscalar(real):\n",
    "        if real == pred:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    length = len(real)\n",
    "    count_ = 0\n",
    "    for i in range(length):\n",
    "        if real[i] == pred[i]:\n",
    "            count_ += 1\n",
    "\n",
    "    return tf.math.reduce_sum(count_) / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=ENCODER_DROPOUT_RATE, \n",
    "                  squeeze_time=3, \n",
    "                  units=64)\n",
    "\n",
    "decoder = Decoder(target_sz=PHONEME_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  lstm_units=LSTM_UNITS, \n",
    "                  batch_sz=BATCH_SIZE, \n",
    "                  dropout_rate=DECODER_DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/ResNet_LSTM'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward algorithm\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "            \n",
    "            target_id = targ[:, t]\n",
    "            loss += loss_function(target_id, predictions)\n",
    "            \n",
    "            predicted_id = tf.math.argmax(predictions, axis=1)\n",
    "            acc += accuracy_function(target_id, predicted_id)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # backward algorithm\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "#     clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)  # clipping for avoiding gradient explosion\n",
    "#     optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    batch_accuracy = (acc / int(targ.shape[1]))\n",
    "\n",
    "    return batch_loss, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(inp, targ, targ_tokenizer):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    inp = tf.expand_dims(inp, 0)\n",
    "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']], 1)\n",
    "\n",
    "    for t in range(1, targ.shape[0]):\n",
    "        predictions, (dec_hidden_h, dec_hidden_c), _ = decoder(dec_input, dec_hidden_h, dec_hidden_c, enc_output)\n",
    "        \n",
    "        target_id = targ[t]\n",
    "        loss += loss_function(target_id, predictions)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        acc += accuracy_function(target_id, predicted_id)\n",
    "        \n",
    "        if targ_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return loss / int(targ.shape[0]), acc / int(targ.shape[0])\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return loss / int(targ.shape[0]), acc / int(targ.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10  Batch: 1000  Loss: 0.6598  Accuracy: 0.3929  Time: 195s\n",
      "Epoch: 1/10  Batch: 2000  Loss: 0.2377  Accuracy: 0.5714  Time: 385s\n",
      "Epoch: 1/10  Batch: 3000  Loss: 0.1553  Accuracy: 0.5714  Time: 577s\n",
      "Epoch: 1/10  Batch: 4000  Loss: 0.0779  Accuracy: 0.5357  Time: 765s\n",
      "Epoch: 1/10  Batch: 4750  Loss: 0.4477  Accuracy: 0.4643  Time: 909s\n",
      "\n",
      "================================\n",
      "Epoch 1/10\n",
      "Accuracy: 0.4565  Loss: 0.4316  val_acc: 0.4276  val_loss: 0.5285\n",
      "Time taken for epoch 1: 16.42 min\n",
      "Total Time taken: 16.42 min\n",
      "================================\n",
      "\n",
      "Epoch: 2/10  Batch: 1000  Loss: 0.2889  Accuracy: 0.5000  Time: 188s\n",
      "Epoch: 2/10  Batch: 2000  Loss: 0.0317  Accuracy: 0.4643  Time: 384s\n",
      "Epoch: 2/10  Batch: 3000  Loss: 0.3625  Accuracy: 0.5000  Time: 576s\n",
      "Epoch: 2/10  Batch: 4000  Loss: 0.1124  Accuracy: 0.5357  Time: 766s\n",
      "Epoch: 2/10  Batch: 4750  Loss: 0.0991  Accuracy: 0.5714  Time: 910s\n",
      "\n",
      "================================\n",
      "Epoch 2/10\n",
      "Accuracy: 0.5474  Loss: 0.1169  val_acc: 0.4543  val_loss: 0.4437\n",
      "Time taken for epoch 2: 16.49 min\n",
      "Total Time taken: 32.92 min\n",
      "================================\n",
      "\n",
      "Epoch: 3/10  Batch: 1000  Loss: 0.1675  Accuracy: 0.4643  Time: 195s\n",
      "Epoch: 3/10  Batch: 2000  Loss: 0.0587  Accuracy: 0.5357  Time: 387s\n",
      "Epoch: 3/10  Batch: 3000  Loss: 0.0537  Accuracy: 0.6786  Time: 581s\n",
      "Epoch: 3/10  Batch: 4000  Loss: 0.0074  Accuracy: 0.7143  Time: 828s\n",
      "Epoch: 3/10  Batch: 4750  Loss: 0.0672  Accuracy: 0.5000  Time: 1190s\n",
      "\n",
      "================================\n",
      "Epoch 3/10\n",
      "Accuracy: 0.5543  Loss: 0.0926  val_acc: 0.4999  val_loss: 0.3233\n",
      "Time taken for epoch 3: 21.74 min\n",
      "Total Time taken: 54.65 min\n",
      "================================\n",
      "\n",
      "Epoch: 4/10  Batch: 1000  Loss: 0.0034  Accuracy: 0.7143  Time: 477s\n",
      "Epoch: 4/10  Batch: 2000  Loss: 0.0628  Accuracy: 0.7143  Time: 957s\n",
      "Epoch: 4/10  Batch: 3000  Loss: 0.1005  Accuracy: 0.5714  Time: 1442s\n",
      "Epoch: 4/10  Batch: 4000  Loss: 0.0027  Accuracy: 0.5357  Time: 1921s\n",
      "Epoch: 4/10  Batch: 4750  Loss: 0.1693  Accuracy: 0.5714  Time: 2284s\n",
      "\n",
      "================================\n",
      "Epoch 4/10\n",
      "Accuracy: 0.5570  Loss: 0.0826  val_acc: 0.5121  val_loss: 0.2931\n",
      "Time taken for epoch 4: 39.90 min\n",
      "Total Time taken: 94.55 min\n",
      "================================\n",
      "\n",
      "Epoch: 5/10  Batch: 353  Loss: 0.2535  Accuracy: 0.4286  Time: 87s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-32c1467f8c3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoneme_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-d6c654939a9b>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, targ_tokenizer)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m#     clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)  # clipping for avoiding gradient explosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#     optimizer.apply_gradients(zip(clipped_gradients, variables))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    439\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m           kwargs={\"name\": name})\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1915\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   1923\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    483\u001b[0m           update_ops.extend(\n\u001b[0;32m    484\u001b[0m               distribution.extended.update(\n\u001b[1;32m--> 485\u001b[1;33m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   1528\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1530\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1532\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2140\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2142\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   2146\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2148\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2149\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    465\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"apply_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apply_state\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[0;32m    202\u001b[0m           \u001b[0mcoefficients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epsilon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m           use_locking=self._use_locking)\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vhat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[1;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[1;34m\"ResourceApplyAdam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         \u001b[0mbeta1_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1525\u001b[1;33m         \"use_locking\", use_locking, \"use_nesterov\", use_nesterov)\n\u001b[0m\u001b[0;32m   1526\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run eagerly will make tensorflow run step by step or else it will raise\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "# similar to Pytorch, which is a dynamic graph for deep learning\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "EPOCHS = 10\n",
    "TOLERANCE = 0.08\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # train the encoder-decoder model\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for inp, targ in dataset.take(STEP_PER_EPOCH):\n",
    "        batch += 1\n",
    "        \n",
    "        batch_loss, batch_accuracy = train_step(inp, targ, phoneme_tokenizer)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "            \n",
    "        print(\"Epoch: {}/{}  Batch: {}  Loss: {:.4f}  Accuracy: {:.4f}  Time: {:.0f}s\".\n",
    "              format(epoch, EPOCHS, batch, batch_loss.numpy(), batch_accuracy.numpy(), time.time()-epoch_start), \n",
    "              end=\"\\r\")\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            print()\n",
    "    print()\n",
    "    # saving (checkpoint) the model when total loss is less than 0.9\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    # validation process\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    for val_inp, val_targ in dataset_val.take(VAL_WAV_SIZE):\n",
    "        val_loss, val_acc = validate_step(val_inp, val_targ, phoneme_tokenizer)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "\n",
    "    # print out the epoch results\n",
    "    mean_total_acc = total_accuracy / STEP_PER_EPOCH\n",
    "    mean_total_loss = total_loss / STEP_PER_EPOCH\n",
    "    \n",
    "    mean_val_acc = total_val_acc / VAL_WAV_SIZE\n",
    "    mean_val_loss = total_val_loss / VAL_WAV_SIZE\n",
    "    \n",
    "    print(\"\\n================================\")\n",
    "    print(\"Epoch {}/{}\".format(epoch, EPOCHS))\n",
    "    print('Accuracy: {:.4f}  Loss: {:.4f}  val_acc: {:.4f}  val_loss: {:.4f}'.format(\n",
    "        mean_total_acc, \n",
    "        mean_total_loss, \n",
    "        mean_val_acc,\n",
    "        mean_val_loss))\n",
    "    print('Time taken for epoch {}: {:.2f} min'.format(epoch, (time.time() - epoch_start)/60))\n",
    "    print('Total Time taken: {:.2f} min'.format((time.time() - start)/60))\n",
    "    print(\"================================\\n\")\n",
    "    \n",
    "    if mean_total_loss < TOLERANCE and mean_val_acc > 0.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, max_input_len, max_output_len, tokenizer):\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    \n",
    "    \n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    attention_plot = np.zeros((max_output_len, max_input_len))\n",
    "    predicted_ids = np.zeros(max_output_len)\n",
    "    for t in np.arange(max_output_len):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            inputs=dec_input, \n",
    "            enc_hidden_h=dec_hidden_h, \n",
    "            enc_hidden_c=dec_hidden_c, \n",
    "            enc_output=enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        pred_id = tf.argmax(predictions[0]).numpy()\n",
    "        predicted_ids[t] = pred_id\n",
    "\n",
    "        result += tokenizer.index_word[pred_id] + ' '\n",
    "\n",
    "        if tokenizer.index_word[pred_id] == '<end>':\n",
    "            return result, predicted_ids, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "    return result, predicted_ids, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, input_wav, output_phoneme):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_yticklabels([''] + output_phoneme, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(wave, max_in, max_out, tokenizer):\n",
    "    result, _, attention_plot = predict(wave, max_in, max_out, tokenizer)\n",
    "    print(f'Predicted translation: {result}')\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :50]\n",
    "    plot_attention(attention_plot, np.arange(len(wave)), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing with test wave data\n",
    "index = [i for i in np.random.randint(len(wav_tensor_val), size=1)]\n",
    "print(f\"Index: {index}\")\n",
    "test_wave = wav_tensor_val[index]\n",
    "test_phoneme = phoneme_tensor_val[index, :]\n",
    "\n",
    "translate(test_wave, sample_output.shape[1], test_phoneme.shape[1], phoneme_tokenizer)\n",
    "for tensor in test_phoneme:\n",
    "    preprocesser.show_convert(tensor, phoneme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(wav_tensor_val)):\n",
    "    _, IDs, _ = predict(wav_tensor_val[i], sample_output.shape[1], test_phoneme.shape[1], phoneme_tokenizer)\n",
    "    phonemes = phoneme_tensor_val[i, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
